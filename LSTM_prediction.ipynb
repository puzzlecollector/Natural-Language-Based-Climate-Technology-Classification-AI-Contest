{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import * \n",
    "from tensorflow.keras.callbacks import * \n",
    "from tensorflow.keras.models import * \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab, Okt \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174304, 13), (43576, 12), (43576, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv') \n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv') \n",
    "train.shape, test.shape, submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['사업명','사업_부처명','내역사업명','과제명','요약문_연구목표','요약문_연구내용','요약문_기대효과','요약문_한글키워드','label']]\n",
    "\n",
    "test = test[['사업명','사업_부처명','내역사업명','과제명','요약문_연구목표','요약문_연구내용','요약문_기대효과','요약문_한글키워드']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, mecab, remove_stopwords=False, stop_words=[]):\n",
    "    text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ]\",\"\", text)\n",
    "    word_text = mecab.morphs(text)\n",
    "    if remove_stopwords:\n",
    "        word_review = [token for token in word_text if not token in stop_words]\n",
    "    return word_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['은','는','이','가', '하','아','것','들','의','있','되','수','보','주','등','한']\n",
    "mecab = Mecab()\n",
    "clean_train1 = []\n",
    "clean_test1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [00:16<00:00, 10820.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train1 = train['과제명'].values \n",
    "for text in tqdm(train1):\n",
    "    try:\n",
    "        clean_train1.append(preprocessing(text, mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        clean_train1.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43576/43576 [00:04<00:00, 9982.74it/s] \n"
     ]
    }
   ],
   "source": [
    "test1 = test['과제명'].values \n",
    "for text in tqdm(test1):\n",
    "    try:\n",
    "        clean_test1.append(preprocessing(text, mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        clean_test1.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "clean_train1 = np.asarray(clean_train1) \n",
    "clean_test1 = np.asarray(clean_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train1)  \n",
    "train1 = tokenizer.texts_to_sequences(clean_train1) \n",
    "\n",
    "test1 = tokenizer.texts_to_sequences(clean_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "for i in range(len(train1)): \n",
    "    if len(train1[i]) > max_len: \n",
    "        max_len = len(train1[i])\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=pad_sequences(train1, maxlen=max_len+5, padding='post')\n",
    "test1=pad_sequences(test1, maxlen=max_len+5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174304, 57), (43576, 57))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1.shape, test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35816"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = len(tokenizer.word_index)+1\n",
    "vocab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약문_연구목표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train2 = [] \n",
    "clean_test2 = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [01:49<00:00, 1595.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train2 = train['요약문_연구목표'].values \n",
    "for text in tqdm(train2):\n",
    "    try:\n",
    "        clean_train2.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_train2.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43576/43576 [00:26<00:00, 1654.65it/s]\n"
     ]
    }
   ],
   "source": [
    "test2 = test['요약문_연구목표'].values \n",
    "for text in tqdm(test2):\n",
    "    try:\n",
    "        clean_test2.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_test2.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train2)  \n",
    "train2 = tokenizer.texts_to_sequences(clean_train2) \n",
    "\n",
    "test2 = tokenizer.texts_to_sequences(clean_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "for i in range(len(train2)): \n",
    "    if len(train2[i]) > max_len: \n",
    "        max_len = len(train2[i])\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pad_sequences(train2, maxlen=max_len+5, padding='post')\n",
    "test2 = pad_sequences(test2, maxlen=max_len+5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174304, 814)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74479"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2 = len(tokenizer.word_index) + 1\n",
    "vocab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약문_연구내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train3 = [] \n",
    "clean_test3 = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [03:17<00:00, 884.52it/s] \n"
     ]
    }
   ],
   "source": [
    "train3 = train['요약문_연구내용'].values \n",
    "for text in tqdm(train3):\n",
    "    try:\n",
    "        clean_train3.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_train3.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43576/43576 [00:51<00:00, 846.03it/s]\n"
     ]
    }
   ],
   "source": [
    "test3 = test['요약문_연구내용'].values \n",
    "for text in tqdm(test3):\n",
    "    try:\n",
    "        clean_test3.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_test3.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train3)  \n",
    "train3 = tokenizer.texts_to_sequences(clean_train3) \n",
    "\n",
    "test3 = tokenizer.texts_to_sequences(clean_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "for i in range(len(train3)): \n",
    "    if len(train3[i]) > max_len: \n",
    "        max_len = len(train3[i])\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3 = pad_sequences(train3, maxlen=max_len+5, padding='post')\n",
    "test3 = pad_sequences(test3, maxlen=max_len+5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94658"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab3 = len(tokenizer.word_index) + 1 \n",
    "vocab3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약문_기대효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train4 = [] \n",
    "clean_test4 = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [02:23<00:00, 1215.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train4 = train['요약문_기대효과'].values \n",
    "for text in tqdm(train4):\n",
    "    try:\n",
    "        clean_train4.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_train4.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43576/43576 [00:34<00:00, 1255.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test4 = test['요약문_기대효과'].values \n",
    "for text in tqdm(test4):\n",
    "    try:\n",
    "        clean_test4.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_test4.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train4)  \n",
    "train4 = tokenizer.texts_to_sequences(clean_train4) \n",
    "\n",
    "test4 = tokenizer.texts_to_sequences(clean_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "for i in range(len(train4)): \n",
    "    if len(train4[i]) > max_len: \n",
    "        max_len = len(train4[i])\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = pad_sequences(train4, maxlen=max_len+5, padding='post')\n",
    "test4 = pad_sequences(test4, maxlen=max_len+5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71163"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab4 = len(tokenizer.word_index) + 1 \n",
    "vocab4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요약문_한글키워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train5 = [] \n",
    "clean_test5 = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [00:13<00:00, 13127.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train5 = train['요약문_한글키워드'].values \n",
    "for text in tqdm(train5):\n",
    "    try:\n",
    "        clean_train5.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_train5.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43576/43576 [00:03<00:00, 13688.76it/s]\n"
     ]
    }
   ],
   "source": [
    "test5 = test['요약문_한글키워드'].values \n",
    "for text in tqdm(test5):\n",
    "    try:\n",
    "        clean_test5.append(preprocessing(str(text), mecab, remove_stopwords=True, stop_words=stop_words))\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "        clean_test5.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train5)  \n",
    "train5 = tokenizer.texts_to_sequences(clean_train5) \n",
    "\n",
    "test5 = tokenizer.texts_to_sequences(clean_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = -1\n",
    "for i in range(len(train5)): \n",
    "    if len(train5[i]) > max_len: \n",
    "        max_len = len(train5[i])\n",
    "        \n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5 = pad_sequences(train5, maxlen=max_len+5, padding='post')\n",
    "test5 = pad_sequences(test5, maxlen=max_len+5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46067"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab5 = len(tokenizer.word_index) + 1 \n",
    "vocab5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174304, 96)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    inputs1 = Input((57))\n",
    "    embedding1 = Embedding(vocab1, 32, input_length=57)(inputs1)\n",
    "    lstm1 = Bidirectional(LSTM(128, return_sequences=False))(embedding1)  \n",
    "    \n",
    "    inputs2 = Input((96))\n",
    "    embedding2 = Embedding(vocab5, 256, input_length=96)(inputs2)\n",
    "    lstm2 = Bidirectional(LSTM(128, return_sequences=False))(embedding2)\n",
    "\n",
    "    concat = Concatenate()([lstm1,lstm2]) \n",
    "    dense = Dense(64, activation = 'relu')(concat)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dense(46, activation = 'softmax')(dense) \n",
    "    model = Model(inputs=[inputs1,inputs2], outputs=dense) \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 57, 32)       1146112     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 96, 256)      11793152    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 256)          164864      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 256)          394240      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64)           256         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 46)           2990        batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 13,534,446\n",
      "Trainable params: 13,534,318\n",
      "Non-trainable params: 128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4903/4903 [==============================] - 778s 157ms/step - loss: 0.5888 - accuracy: 0.8590 - val_loss: 1.1264 - val_accuracy: 0.7301\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.73008, saving model to Bi_LSTM_epoch_001_val_1.1263858080_accuracy_0.7300785780.h5\n",
      "Epoch 2/200\n",
      "  28/4903 [..............................] - ETA: 11:27 - loss: 0.2389 - accuracy: 0.9342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 745s 152ms/step - loss: 0.2908 - accuracy: 0.9149 - val_loss: 0.4443 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.73008 to 0.88624, saving model to Bi_LSTM_epoch_002_val_0.4442852736_accuracy_0.8862371445.h5\n",
      "Epoch 3/200\n",
      "1751/4903 [=========>....................] - ETA: 7:53 - loss: 0.1717 - accuracy: 0.9472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3406/4903 [===================>..........] - ETA: 3:49 - loss: 0.1353 - accuracy: 0.9582"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2348/4903 [=============>................] - ETA: 6:17 - loss: 0.0976 - accuracy: 0.9706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 739s 151ms/step - loss: 0.1092 - accuracy: 0.9668 - val_loss: 0.5607 - val_accuracy: 0.8811\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.88624\n",
      "Epoch 6/200\n",
      " 247/4903 [>.............................] - ETA: 11:34 - loss: 0.0680 - accuracy: 0.9785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360/4903 [=========================>....] - ETA: 1:18 - loss: 0.0887 - accuracy: 0.9730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2192/4903 [============>.................] - ETA: 6:31 - loss: 0.0646 - accuracy: 0.9802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3187/4903 [==================>...........] - ETA: 4:09 - loss: 0.0616 - accuracy: 0.9818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4751/4903 [============================>.] - ETA: 22s - loss: 0.0614 - accuracy: 0.9818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 747s 152ms/step - loss: 0.0545 - accuracy: 0.9837 - val_loss: 0.4363 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.92353\n",
      "Epoch 11/200\n",
      "1777/4903 [=========>....................] - ETA: 7:38 - loss: 0.0449 - accuracy: 0.9859"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3792/4903 [======================>.......] - ETA: 2:44 - loss: 0.0478 - accuracy: 0.9857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 759s 155ms/step - loss: 0.0445 - accuracy: 0.9858 - val_loss: 0.4711 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.92353 to 0.92387, saving model to Bi_LSTM_epoch_013_val_0.4711167514_accuracy_0.9238712788.h5\n",
      "Epoch 14/200\n",
      " 796/4903 [===>..........................] - ETA: 10:18 - loss: 0.0301 - accuracy: 0.9898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 753s 154ms/step - loss: 0.0432 - accuracy: 0.9863 - val_loss: 0.4549 - val_accuracy: 0.9231\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.92387\n",
      "Epoch 15/200\n",
      " 541/4903 [==>...........................] - ETA: 11:10 - loss: 0.0297 - accuracy: 0.9900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2752/4903 [===============>..............] - ETA: 5:28 - loss: 0.0351 - accuracy: 0.9881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2593/4903 [==============>...............] - ETA: 5:48 - loss: 0.0343 - accuracy: 0.9886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770/4903 [============================>.] - ETA: 20s - loss: 0.0382 - accuracy: 0.9877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4628/4903 [===========================>..] - ETA: 41s - loss: 0.0360 - accuracy: 0.9880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999/4903 [===========>..................] - ETA: 7:15 - loss: 0.0290 - accuracy: 0.9900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 754s 154ms/step - loss: 0.0346 - accuracy: 0.9880 - val_loss: 0.4582 - val_accuracy: 0.9243\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.92485\n",
      "Epoch 19/200\n",
      "1466/4903 [=======>......................] - ETA: 8:46 - loss: 0.0247 - accuracy: 0.9910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3614/4903 [=====================>........] - ETA: 3:16 - loss: 0.0310 - accuracy: 0.9891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2338/4903 [=============>................] - ETA: 6:33 - loss: 0.0276 - accuracy: 0.9903"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903/4903 [==============================] - 768s 157ms/step - loss: 0.0320 - accuracy: 0.9884 - val_loss: 0.4622 - val_accuracy: 0.9223\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.92485\n"
     ]
    }
   ],
   "source": [
    "model_path = 'Bi_LSTM_epoch_{epoch:03d}_val_{val_loss:.10f}_accuracy_{val_accuracy:.10f}.h5'\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.9)\n",
    "checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_accuracy', verbose = 1, save_best_only = True)\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10) \n",
    "\n",
    "history = model.fit(x=[train1,train5],\n",
    "                    y=y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=200,\n",
    "                    validation_split = 0.1, \n",
    "                    callbacks = [learning_rate_reduction,checkpoint,early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
