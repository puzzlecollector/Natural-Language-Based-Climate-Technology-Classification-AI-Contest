{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계획 \n",
    "\n",
    "레이블이 zero 인지 non-zero인지 판단하는 모델이 하나 만들어진다. \n",
    "\n",
    "만약 zero라면 0라고 레이블을 예측한다. \n",
    "\n",
    "만약 non-zero라면 어떤 레이블인지 판단하는 모델이 예측을 한다. \n",
    "\n",
    "\n",
    "KoELECTRA를 사용하고 단순히 모든 레이블을 예측하는 KoELECTRA와 비교해서 얼만큼 성능이 좋은지 살펴본다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.7.0+cu110 available.\n",
      "TensorFlow version 2.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import * \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import re \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "import sklearn \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as f\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from sklearn.model_selection import train_test_split \n",
    "import time \n",
    "import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model for determining zero or non zero labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174304, 13), (43576, 12), 46)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"open/train.csv\") \n",
    "test = pd.read_csv(\"open/test.csv\") \n",
    "\n",
    "train.shape, test.shape, train['label'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_train = train[train['label'] == 0] \n",
    "nonzero_train = train[train['label'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142571, 13), (31733, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_train.shape, nonzero_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# convert all nonzero_train data sample's labels to 1 \n",
    "nonzero_train['label'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([zero_train, nonzero_train], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, overlap = 20, chunk_size = 50): \n",
    "    total = [] \n",
    "    partial = [] \n",
    "    if len(s.split()) // (chunk_size - overlap) > 0:  \n",
    "        n = len(s.split()) // (chunk_size - overlap) \n",
    "    else: \n",
    "        n = 1 \n",
    "    for w in range(n): \n",
    "        if w == 0: \n",
    "            partial = s.split()[:chunk_size] \n",
    "            total.append(\" \".join(partial)) \n",
    "        else:  \n",
    "            partial = s.split()[w*(chunk_size - overlap):w*(chunk_size - overlap) + chunk_size]\n",
    "            total.append(\" \".join(partial)) \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['요약문_내용'] = train_df['요약문_연구목표'] + train_df['요약문_연구내용'] + train_df['요약문_기대효과'] \n",
    "test['요약문_내용'] = test['요약문_연구목표'] + test['요약문_연구내용'] + test['요약문_기대효과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['요약문_내용'].fillna('NAN',inplace=True) \n",
    "test['요약문_내용'].fillna('NAN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['사업명'].fillna('NAN',inplace=True) \n",
    "train_df['사업_부처명'].fillna('NAN',inplace=True) \n",
    "train_df['내역사업명'].fillna('NAN',inplace=True) \n",
    "train_df['과제명'].fillna('NAN',inplace=True) \n",
    "train_df['요약문_한글키워드'].fillna('NAN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [01:41<00:00, 1724.88it/s]\n"
     ]
    }
   ],
   "source": [
    "contents = train_df['요약문_내용'].values \n",
    "feature1 = train_df['사업명'].values \n",
    "feature2 = train_df['사업_부처명'].values \n",
    "feature3 = train_df['내역사업명'].values \n",
    "feature4 = train_df['과제명'].values \n",
    "feature5 = train_df['요약문_한글키워드'].values \n",
    "feature6 = train_df['label'].values \n",
    "\n",
    "train_data = {'사업명':[],'사업_부처명':[],'내역사업명':[],'과제명':[],'한글키워드':[],'요약문':[],'label':[]} \n",
    "\n",
    "for i in tqdm(range(contents.shape[0]), position = 0, leave = True): \n",
    "    sample = str(contents[i]) \n",
    "    splitted_text = split_text(clean_text(sample)) \n",
    "    for t in splitted_text: \n",
    "        train_data['요약문'].append(t) \n",
    "        train_data['사업명'].append(clean_text(str(feature1[i])))\n",
    "        train_data['사업_부처명'].append(clean_text(str(feature2[i]))) \n",
    "        train_data['내역사업명'].append(clean_text(str(feature3[i]))) \n",
    "        train_data['과제명'].append(clean_text(str(feature4[i])))  \n",
    "        train_data['한글키워드'].append(feature5[i]) # no cleaning for this one\n",
    "        train_data['label'].append(feature6[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사업명</th>\n",
       "      <th>사업_부처명</th>\n",
       "      <th>내역사업명</th>\n",
       "      <th>과제명</th>\n",
       "      <th>한글키워드</th>\n",
       "      <th>요약문</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>최종목표 감수성 표적 유전자를 발굴하고 내성제어 기전을 연구 발굴된 유전자를 통한 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>저항성 극복 기전을 규명 환자조직 동물실험 세포실험을 통해 대장암에 특이적으로 조절...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>내성 유발 유전자를 발굴하고 저항성 극복 기전을 규명 추후 기반 항암화학요법 치료효...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>표적 가능성 검증 선천적 내성 예측인자 발굴 차년도 를 통한 후천적 내성 표적 후보...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                사업명 사업_부처명                 내역사업명  \\\n",
       "0  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "1  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "2  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "3  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "4  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "\n",
       "                                                 과제명                  한글키워드  \\\n",
       "0  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "1  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "2  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "3  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "4  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "\n",
       "                                                 요약문  label  \n",
       "0  최종목표 감수성 표적 유전자를 발굴하고 내성제어 기전을 연구 발굴된 유전자를 통한 ...      0  \n",
       "1  저항성 극복 기전을 규명 환자조직 동물실험 세포실험을 통해 대장암에 특이적으로 조절...      0  \n",
       "2  내성 유발 유전자를 발굴하고 저항성 극복 기전을 규명 추후 기반 항암화학요법 치료효...      0  \n",
       "3  대장암 환자조직을 이용하여 후보 유전자의 발현과 발현 양상 분석 후보 유전자 발현 ...      0  \n",
       "4  표적 가능성 검증 선천적 내성 예측인자 발굴 차년도 를 통한 후천적 내성 표적 후보...      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "train_data.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we tokenize each data and make sure they all lie within the 512 tokenization range \n",
    "## if not check how many have token length greater than 512 \n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\") \n",
    "\n",
    "def electra_tokenizer(sent, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens = True, # add [CLS] and [SEP]\n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True # constructing attention_masks \n",
    "    )  \n",
    "    \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] # differentiate padding from non padding \n",
    "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences, not \"really\" necessary for now    \n",
    "    \n",
    "    if len(input_id) > 512: # head + tail methodology \n",
    "        input_id = input_id[:512] \n",
    "        attention_mask = attention_mask[:512] \n",
    "        token_type_id = token_type_id[:512]  \n",
    "        print(\"Long Text!! Truncating to the first 512 tokens!\")\n",
    "    elif len(input_id) <= 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512 - len(attention_mask))\n",
    "        token_type_id = token_type_id + [0]*(512 - len(token_type_id))  \n",
    "        \n",
    "    return input_id, attention_mask, token_type_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define important hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "VALID_SPLIT = 0.1 \n",
    "MAX_LEN = 512 # max token size for BERT, ELECTRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사업명</th>\n",
       "      <th>사업_부처명</th>\n",
       "      <th>내역사업명</th>\n",
       "      <th>과제명</th>\n",
       "      <th>한글키워드</th>\n",
       "      <th>요약문</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>최종목표 감수성 표적 유전자를 발굴하고 내성제어 기전을 연구 발굴된 유전자를 통한 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>이공학학술연구기반구축      교육부 지역대학우수과학자지원사업  년  년  대장암의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이공학학술연구기반구축</td>\n",
       "      <td>교육부</td>\n",
       "      <td>지역대학우수과학자지원사업  년  년</td>\n",
       "      <td>대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...</td>\n",
       "      <td>대장암,항암제 내성,세포사멸,유전자발굴</td>\n",
       "      <td>저항성 극복 기전을 규명 환자조직 동물실험 세포실험을 통해 대장암에 특이적으로 조절...</td>\n",
       "      <td>0</td>\n",
       "      <td>이공학학술연구기반구축      교육부 지역대학우수과학자지원사업  년  년  대장암의...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                사업명 사업_부처명                 내역사업명  \\\n",
       "0  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "1  이공학학술연구기반구축         교육부  지역대학우수과학자지원사업  년  년    \n",
       "\n",
       "                                                 과제명                  한글키워드  \\\n",
       "0  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "1  대장암의       내성 표적 인자 발굴 및       반응 예측 유전자 지도 구축...  대장암,항암제 내성,세포사멸,유전자발굴   \n",
       "\n",
       "                                                 요약문  label  \\\n",
       "0  최종목표 감수성 표적 유전자를 발굴하고 내성제어 기전을 연구 발굴된 유전자를 통한 ...      0   \n",
       "1  저항성 극복 기전을 규명 환자조직 동물실험 세포실험을 통해 대장암에 특이적으로 조절...      0   \n",
       "\n",
       "                                                data  \n",
       "0  이공학학술연구기반구축      교육부 지역대학우수과학자지원사업  년  년  대장암의...  \n",
       "1  이공학학술연구기반구축      교육부 지역대학우수과학자지원사업  년  년  대장암의...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['data'] = train_data['사업명'] + \" \" + train_data['사업_부처명'] + \" \" + train_data['내역사업명'] + \" \" + train_data['과제명'] + \" \" + train_data['한글키워드'] + \" \" + train_data['요약문'] \n",
    "\n",
    "train_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data['data'].values \n",
    "train_labels = train_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 65413/1638867 [01:54<43:07, 607.99it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "  4%|▍         | 65541/1638867 [01:54<43:34, 601.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 100868/1638867 [02:55<40:33, 632.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 113267/1638867 [03:16<40:46, 623.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 141721/1638867 [04:05<41:26, 602.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 146011/1638867 [04:12<39:20, 632.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 191878/1638867 [05:31<57:13, 421.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 205819/1638867 [05:56<40:49, 585.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 211854/1638867 [06:06<39:13, 606.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 376012/1638867 [10:50<32:50, 640.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 390028/1638867 [11:14<32:44, 635.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 400880/1638867 [11:33<32:51, 627.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 446477/1638867 [12:52<31:29, 631.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 465702/1638867 [13:27<31:24, 622.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 471453/1638867 [13:37<44:09, 440.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 476419/1638867 [13:46<56:39, 342.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 481139/1638867 [13:55<29:17, 658.72it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 47%|████▋     | 773839/1638867 [22:27<25:58, 554.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 776874/1638867 [22:33<22:16, 644.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 876297/1638867 [25:24<20:05, 632.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 891554/1638867 [25:51<29:56, 415.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n",
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 924688/1638867 [26:51<20:48, 571.85it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 73%|███████▎  | 1192080/1638867 [34:46<11:49, 629.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1273483/1638867 [37:07<09:47, 622.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 1288719/1638867 [37:33<09:25, 619.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1351974/1638867 [39:23<07:36, 628.82it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 95%|█████████▌| 1557696/1638867 [45:22<02:04, 649.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1558224/1638867 [45:23<02:55, 458.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1569307/1638867 [45:42<01:47, 645.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Truncating to the first 512 tokens!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638867/1638867 [47:46<00:00, 571.73it/s]\n"
     ]
    }
   ],
   "source": [
    "N = train_data.shape[0] \n",
    "\n",
    "input_ids = np.zeros((N, MAX_LEN),dtype=int)\n",
    "attention_masks = np.zeros((N, MAX_LEN),dtype=int)\n",
    "token_type_ids = np.zeros((N, MAX_LEN),dtype=int) \n",
    "labels = np.zeros((N),dtype=int)\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True): \n",
    "    try:\n",
    "        cur_str = train_text[i]\n",
    "        cur_label = train_labels[i]\n",
    "        input_id, attention_mask, token_type_id = electra_tokenizer(cur_str, MAX_LEN=MAX_LEN) \n",
    "        input_ids[i,] = input_id \n",
    "        attention_masks[i,] = attention_mask \n",
    "        token_type_ids[i,] = token_type_id\n",
    "        labels[i] = cur_label \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(cur_str)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int)\n",
    "token_type_ids = torch.tensor(token_type_ids, dtype=int) \n",
    "labels = torch.tensor(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1474980, 512]),\n",
       " torch.Size([1474980, 512]),\n",
       " torch.Size([1474980, 512]),\n",
       " torch.Size([1474980]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(attention_masks, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "train_token_ids, val_token_ids, _, _ = train_test_split(token_type_ids, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "\n",
    "train_inputs.shape, train_attention_mask.shape, train_token_ids.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([163887, 512]),\n",
       " torch.Size([163887, 512]),\n",
       " torch.Size([163887, 512]),\n",
       " torch.Size([163887]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_attention_mask.shape, val_token_ids.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_attention_mask, train_token_ids, train_labels) \n",
    "train_sampler = RandomSampler(train_data) \n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE) \n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_attention_mask, val_token_ids, val_labels) \n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if label is zero\n",
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=2)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch    20  of  46,094.    Elapsed: 0:00:19.\n",
      "  current average loss = 0.5253074169158936\n",
      "  Batch    40  of  46,094.    Elapsed: 0:00:39.\n",
      "  current average loss = 0.5173340976238251\n",
      "  Batch    60  of  46,094.    Elapsed: 0:00:58.\n",
      "  current average loss = 0.4975190247098605\n",
      "  Batch    80  of  46,094.    Elapsed: 0:01:18.\n",
      "  current average loss = 0.48271819911897185\n",
      "  Batch   100  of  46,094.    Elapsed: 0:01:37.\n",
      "  current average loss = 0.46367808103561403\n",
      "  Batch   120  of  46,094.    Elapsed: 0:01:57.\n",
      "  current average loss = 0.4480092347910007\n",
      "  Batch   140  of  46,094.    Elapsed: 0:02:16.\n",
      "  current average loss = 0.4409704953432083\n",
      "  Batch   160  of  46,094.    Elapsed: 0:02:35.\n",
      "  current average loss = 0.42653598710894586\n",
      "  Batch   180  of  46,094.    Elapsed: 0:02:55.\n",
      "  current average loss = 0.4130048093696435\n",
      "  Batch   200  of  46,094.    Elapsed: 0:03:14.\n",
      "  current average loss = 0.40458577204495666\n",
      "  Batch   220  of  46,094.    Elapsed: 0:03:34.\n",
      "  current average loss = 0.3946866458451206\n",
      "  Batch   240  of  46,094.    Elapsed: 0:03:53.\n",
      "  current average loss = 0.38485393409306806\n",
      "  Batch   260  of  46,094.    Elapsed: 0:04:13.\n",
      "  current average loss = 0.3786178089391727\n",
      "  Batch   280  of  46,094.    Elapsed: 0:04:32.\n",
      "  current average loss = 0.37347652433173995\n",
      "  Batch   300  of  46,094.    Elapsed: 0:04:52.\n",
      "  current average loss = 0.3685248460372289\n",
      "  Batch   320  of  46,094.    Elapsed: 0:05:11.\n",
      "  current average loss = 0.3619285580003634\n",
      "  Batch   340  of  46,094.    Elapsed: 0:05:31.\n",
      "  current average loss = 0.3577012698220856\n",
      "  Batch   360  of  46,094.    Elapsed: 0:05:50.\n",
      "  current average loss = 0.3538862101940645\n",
      "  Batch   380  of  46,094.    Elapsed: 0:06:10.\n",
      "  current average loss = 0.3498774963578111\n",
      "  Batch   400  of  46,094.    Elapsed: 0:06:30.\n",
      "  current average loss = 0.34408407993614676\n",
      "  Batch   420  of  46,094.    Elapsed: 0:06:49.\n",
      "  current average loss = 0.33886679517371315\n",
      "  Batch   440  of  46,094.    Elapsed: 0:07:09.\n",
      "  current average loss = 0.3373284217986194\n",
      "  Batch   460  of  46,094.    Elapsed: 0:07:29.\n",
      "  current average loss = 0.33248444372869057\n",
      "  Batch   480  of  46,094.    Elapsed: 0:07:48.\n",
      "  current average loss = 0.3295493349122504\n",
      "  Batch   500  of  46,094.    Elapsed: 0:08:08.\n",
      "  current average loss = 0.3277275440990925\n",
      "  Batch   520  of  46,094.    Elapsed: 0:08:27.\n",
      "  current average loss = 0.3246827313819757\n",
      "  Batch   540  of  46,094.    Elapsed: 0:08:47.\n",
      "  current average loss = 0.3213765999233281\n",
      "  Batch   560  of  46,094.    Elapsed: 0:09:06.\n",
      "  current average loss = 0.3188826065377465\n",
      "  Batch   580  of  46,094.    Elapsed: 0:09:26.\n",
      "  current average loss = 0.31770253547563637\n",
      "  Batch   600  of  46,094.    Elapsed: 0:09:45.\n",
      "  current average loss = 0.31538718679298955\n",
      "  Batch   620  of  46,094.    Elapsed: 0:10:05.\n",
      "  current average loss = 0.3136546318930003\n",
      "  Batch   640  of  46,094.    Elapsed: 0:10:24.\n",
      "  current average loss = 0.3110856442595832\n",
      "  Batch   660  of  46,094.    Elapsed: 0:10:44.\n",
      "  current average loss = 0.30890728535525724\n",
      "  Batch   680  of  46,094.    Elapsed: 0:11:03.\n",
      "  current average loss = 0.3079441923867254\n",
      "  Batch   700  of  46,094.    Elapsed: 0:11:23.\n",
      "  current average loss = 0.3062089436075517\n",
      "  Batch   720  of  46,094.    Elapsed: 0:11:43.\n",
      "  current average loss = 0.3055099375442498\n",
      "  Batch   740  of  46,094.    Elapsed: 0:12:03.\n",
      "  current average loss = 0.303585525451077\n",
      "  Batch   760  of  46,094.    Elapsed: 0:12:22.\n",
      "  current average loss = 0.30249722629883574\n",
      "  Batch   780  of  46,094.    Elapsed: 0:12:42.\n",
      "  current average loss = 0.30091058438500534\n",
      "  Batch   800  of  46,094.    Elapsed: 0:13:01.\n",
      "  current average loss = 0.2991370694106445\n",
      "  Batch   820  of  46,094.    Elapsed: 0:13:21.\n",
      "  current average loss = 0.29762814717776165\n",
      "  Batch   840  of  46,094.    Elapsed: 0:13:40.\n",
      "  current average loss = 0.2957997207175053\n",
      "  Batch   860  of  46,094.    Elapsed: 0:14:00.\n",
      "  current average loss = 0.2954771989523325\n",
      "  Batch   880  of  46,094.    Elapsed: 0:14:19.\n",
      "  current average loss = 0.2942308171186596\n",
      "  Batch   900  of  46,094.    Elapsed: 0:14:39.\n",
      "  current average loss = 0.2931044792756438\n",
      "  Batch   920  of  46,094.    Elapsed: 0:14:58.\n",
      "  current average loss = 0.2918737696929146\n",
      "  Batch   940  of  46,094.    Elapsed: 0:15:18.\n",
      "  current average loss = 0.2902702948237036\n",
      "  Batch   960  of  46,094.    Elapsed: 0:15:37.\n",
      "  current average loss = 0.2884044929135901\n",
      "  Batch   980  of  46,094.    Elapsed: 0:15:57.\n",
      "  current average loss = 0.2879121296982072\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:16:16.\n",
      "  current average loss = 0.28726433719322086\n",
      "  Batch 1,020  of  46,094.    Elapsed: 0:16:36.\n",
      "  current average loss = 0.2865756702591099\n",
      "  Batch 1,040  of  46,094.    Elapsed: 0:16:55.\n",
      "  current average loss = 0.2855372548210793\n",
      "  Batch 1,060  of  46,094.    Elapsed: 0:17:15.\n",
      "  current average loss = 0.28456115893105854\n",
      "  Batch 1,080  of  46,094.    Elapsed: 0:17:34.\n",
      "  current average loss = 0.28391272946157387\n",
      "  Batch 1,100  of  46,094.    Elapsed: 0:17:54.\n",
      "  current average loss = 0.2827739958566698\n",
      "  Batch 1,120  of  46,094.    Elapsed: 0:18:13.\n",
      "  current average loss = 0.28271405069224004\n",
      "  Batch 1,140  of  46,094.    Elapsed: 0:18:33.\n",
      "  current average loss = 0.28153108370474034\n",
      "  Batch 1,160  of  46,094.    Elapsed: 0:18:52.\n",
      "  current average loss = 0.2807979382542444\n",
      "  Batch 1,180  of  46,094.    Elapsed: 0:19:12.\n",
      "  current average loss = 0.280180833883331\n",
      "  Batch 1,200  of  46,094.    Elapsed: 0:19:31.\n",
      "  current average loss = 0.2793695776009311\n",
      "  Batch 1,220  of  46,094.    Elapsed: 0:19:51.\n",
      "  current average loss = 0.27840583717419964\n",
      "  Batch 1,240  of  46,094.    Elapsed: 0:20:11.\n",
      "  current average loss = 0.2775288546007247\n",
      "  Batch 1,260  of  46,094.    Elapsed: 0:20:30.\n",
      "  current average loss = 0.2768316126206801\n",
      "  Batch 1,280  of  46,094.    Elapsed: 0:20:50.\n",
      "  current average loss = 0.2756086868030252\n",
      "  Batch 1,300  of  46,094.    Elapsed: 0:21:09.\n",
      "  current average loss = 0.27482493538982594\n",
      "  Batch 1,320  of  46,094.    Elapsed: 0:21:29.\n",
      "  current average loss = 0.27367391676895997\n",
      "  Batch 1,340  of  46,094.    Elapsed: 0:21:48.\n",
      "  current average loss = 0.2729710357374887\n",
      "  Batch 1,360  of  46,094.    Elapsed: 0:22:08.\n",
      "  current average loss = 0.27271305765025317\n",
      "  Batch 1,380  of  46,094.    Elapsed: 0:22:27.\n",
      "  current average loss = 0.2719790313149924\n",
      "  Batch 1,400  of  46,094.    Elapsed: 0:22:47.\n",
      "  current average loss = 0.2709983192038323\n",
      "  Batch 1,420  of  46,094.    Elapsed: 0:23:07.\n",
      "  current average loss = 0.27104996036423346\n",
      "  Batch 1,440  of  46,094.    Elapsed: 0:23:27.\n",
      "  current average loss = 0.27023079711297116\n",
      "  Batch 1,460  of  46,094.    Elapsed: 0:23:47.\n",
      "  current average loss = 0.26979081246312964\n",
      "  Batch 1,480  of  46,094.    Elapsed: 0:24:06.\n",
      "  current average loss = 0.2691819957621093\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:24:26.\n",
      "  current average loss = 0.26859967965756854\n",
      "  Batch 1,520  of  46,094.    Elapsed: 0:24:45.\n",
      "  current average loss = 0.26845117402875696\n",
      "  Batch 1,540  of  46,094.    Elapsed: 0:25:05.\n",
      "  current average loss = 0.2674799008040943\n",
      "  Batch 1,560  of  46,094.    Elapsed: 0:25:25.\n",
      "  current average loss = 0.2666884103252624\n",
      "  Batch 1,580  of  46,094.    Elapsed: 0:25:45.\n",
      "  current average loss = 0.2661830114330389\n",
      "  Batch 1,600  of  46,094.    Elapsed: 0:26:05.\n",
      "  current average loss = 0.2657196748920251\n",
      "  Batch 1,620  of  46,094.    Elapsed: 0:26:25.\n",
      "  current average loss = 0.2654814739067705\n",
      "  Batch 1,640  of  46,094.    Elapsed: 0:26:44.\n",
      "  current average loss = 0.2650514173673511\n",
      "  Batch 1,660  of  46,094.    Elapsed: 0:27:04.\n",
      "  current average loss = 0.2645891315393509\n",
      "  Batch 1,680  of  46,094.    Elapsed: 0:27:24.\n",
      "  current average loss = 0.26431366946393003\n",
      "  Batch 1,700  of  46,094.    Elapsed: 0:27:44.\n",
      "  current average loss = 0.2638502590938964\n",
      "  Batch 1,720  of  46,094.    Elapsed: 0:28:03.\n",
      "  current average loss = 0.2636779617312429\n",
      "  Batch 1,740  of  46,094.    Elapsed: 0:28:23.\n",
      "  current average loss = 0.26306722818113776\n",
      "  Batch 1,760  of  46,094.    Elapsed: 0:28:42.\n",
      "  current average loss = 0.2623049521109682\n",
      "  Batch 1,780  of  46,094.    Elapsed: 0:29:02.\n",
      "  current average loss = 0.2618369023378394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,800  of  46,094.    Elapsed: 0:29:21.\n",
      "  current average loss = 0.260937618802612\n",
      "  Batch 1,820  of  46,094.    Elapsed: 0:29:41.\n",
      "  current average loss = 0.26082038300439386\n",
      "  Batch 1,840  of  46,094.    Elapsed: 0:30:00.\n",
      "  current average loss = 0.2601363322444503\n",
      "  Batch 1,860  of  46,094.    Elapsed: 0:30:20.\n",
      "  current average loss = 0.2594171258440662\n",
      "  Batch 1,880  of  46,094.    Elapsed: 0:30:39.\n",
      "  current average loss = 0.25852836796399603\n",
      "  Batch 1,900  of  46,094.    Elapsed: 0:30:59.\n",
      "  current average loss = 0.25769283980719354\n",
      "  Batch 1,920  of  46,094.    Elapsed: 0:31:19.\n",
      "  current average loss = 0.2571763135473399\n",
      "  Batch 1,940  of  46,094.    Elapsed: 0:31:38.\n",
      "  current average loss = 0.2565373414666536\n",
      "  Batch 1,960  of  46,094.    Elapsed: 0:31:59.\n",
      "  current average loss = 0.2560250788565953\n",
      "  Batch 1,980  of  46,094.    Elapsed: 0:32:18.\n",
      "  current average loss = 0.25570937190204857\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:32:38.\n",
      "  current average loss = 0.2549235575739294\n",
      "  Batch 2,020  of  46,094.    Elapsed: 0:32:57.\n",
      "  current average loss = 0.25468325051156304\n",
      "  Batch 2,040  of  46,094.    Elapsed: 0:33:17.\n",
      "  current average loss = 0.25430083231196976\n",
      "  Batch 2,060  of  46,094.    Elapsed: 0:33:36.\n",
      "  current average loss = 0.25382600123768\n",
      "  Batch 2,080  of  46,094.    Elapsed: 0:33:56.\n",
      "  current average loss = 0.25359412477566645\n",
      "  Batch 2,100  of  46,094.    Elapsed: 0:34:15.\n",
      "  current average loss = 0.25354411363956475\n",
      "  Batch 2,120  of  46,094.    Elapsed: 0:34:35.\n",
      "  current average loss = 0.2534816275842769\n",
      "  Batch 2,140  of  46,094.    Elapsed: 0:34:54.\n",
      "  current average loss = 0.2529489719578735\n",
      "  Batch 2,160  of  46,094.    Elapsed: 0:35:14.\n",
      "  current average loss = 0.25236991724102864\n",
      "  Batch 2,180  of  46,094.    Elapsed: 0:35:33.\n",
      "  current average loss = 0.25239623108827464\n",
      "  Batch 2,200  of  46,094.    Elapsed: 0:35:53.\n",
      "  current average loss = 0.251787765385075\n",
      "  Batch 2,220  of  46,094.    Elapsed: 0:36:12.\n",
      "  current average loss = 0.25132832439565983\n",
      "  Batch 2,240  of  46,094.    Elapsed: 0:36:32.\n",
      "  current average loss = 0.251233428946164\n",
      "  Batch 2,260  of  46,094.    Elapsed: 0:36:51.\n",
      "  current average loss = 0.2507856943290186\n",
      "  Batch 2,280  of  46,094.    Elapsed: 0:37:11.\n",
      "  current average loss = 0.25047663131140563\n",
      "  Batch 2,300  of  46,094.    Elapsed: 0:37:30.\n",
      "  current average loss = 0.25040090973772433\n",
      "  Batch 2,320  of  46,094.    Elapsed: 0:37:50.\n",
      "  current average loss = 0.24950935725636522\n",
      "  Batch 2,340  of  46,094.    Elapsed: 0:38:10.\n",
      "  current average loss = 0.24920890739935841\n",
      "  Batch 2,360  of  46,094.    Elapsed: 0:38:29.\n",
      "  current average loss = 0.2486515835795741\n",
      "  Batch 2,380  of  46,094.    Elapsed: 0:38:49.\n",
      "  current average loss = 0.24838938190750465\n",
      "  Batch 2,400  of  46,094.    Elapsed: 0:39:08.\n",
      "  current average loss = 0.24813444163495055\n",
      "  Batch 2,420  of  46,094.    Elapsed: 0:39:28.\n",
      "  current average loss = 0.24783375102965052\n",
      "  Batch 2,440  of  46,094.    Elapsed: 0:39:47.\n",
      "  current average loss = 0.24770546954766404\n",
      "  Batch 2,460  of  46,094.    Elapsed: 0:40:07.\n",
      "  current average loss = 0.2475261660201884\n",
      "  Batch 2,480  of  46,094.    Elapsed: 0:40:26.\n",
      "  current average loss = 0.24698446510329602\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:40:46.\n",
      "  current average loss = 0.24683093954473734\n",
      "  Batch 2,520  of  46,094.    Elapsed: 0:41:05.\n",
      "  current average loss = 0.24655119710646214\n",
      "  Batch 2,540  of  46,094.    Elapsed: 0:41:25.\n",
      "  current average loss = 0.24627099182368734\n",
      "  Batch 2,560  of  46,094.    Elapsed: 0:41:44.\n",
      "  current average loss = 0.24567733162330113\n",
      "  Batch 2,580  of  46,094.    Elapsed: 0:42:04.\n",
      "  current average loss = 0.2453940984330956\n",
      "  Batch 2,600  of  46,094.    Elapsed: 0:42:23.\n",
      "  current average loss = 0.24502835489881153\n",
      "  Batch 2,620  of  46,094.    Elapsed: 0:42:43.\n",
      "  current average loss = 0.24472408275411442\n",
      "  Batch 2,640  of  46,094.    Elapsed: 0:43:02.\n",
      "  current average loss = 0.24460511106616734\n",
      "  Batch 2,660  of  46,094.    Elapsed: 0:43:22.\n",
      "  current average loss = 0.24431127370930367\n",
      "  Batch 2,680  of  46,094.    Elapsed: 0:43:42.\n",
      "  current average loss = 0.2439658681354694\n",
      "  Batch 2,700  of  46,094.    Elapsed: 0:44:01.\n",
      "  current average loss = 0.24361529480183014\n",
      "  Batch 2,720  of  46,094.    Elapsed: 0:44:21.\n",
      "  current average loss = 0.2431634986643022\n",
      "  Batch 2,740  of  46,094.    Elapsed: 0:44:40.\n",
      "  current average loss = 0.24281241370936055\n",
      "  Batch 2,760  of  46,094.    Elapsed: 0:45:00.\n",
      "  current average loss = 0.2421951110103586\n",
      "  Batch 2,780  of  46,094.    Elapsed: 0:45:20.\n",
      "  current average loss = 0.24230787218782862\n",
      "  Batch 2,800  of  46,094.    Elapsed: 0:45:39.\n",
      "  current average loss = 0.24230325427904192\n",
      "  Batch 2,820  of  46,094.    Elapsed: 0:45:59.\n",
      "  current average loss = 0.24203047107257847\n",
      "  Batch 2,840  of  46,094.    Elapsed: 0:46:19.\n",
      "  current average loss = 0.24159328773630862\n",
      "  Batch 2,860  of  46,094.    Elapsed: 0:46:38.\n",
      "  current average loss = 0.24098486696415536\n",
      "  Batch 2,880  of  46,094.    Elapsed: 0:46:58.\n",
      "  current average loss = 0.24071108188751775\n",
      "  Batch 2,900  of  46,094.    Elapsed: 0:47:18.\n",
      "  current average loss = 0.2406150057687071\n",
      "  Batch 2,920  of  46,094.    Elapsed: 0:47:37.\n",
      "  current average loss = 0.240236874014675\n",
      "  Batch 2,940  of  46,094.    Elapsed: 0:47:57.\n",
      "  current average loss = 0.23994744092821568\n",
      "  Batch 2,960  of  46,094.    Elapsed: 0:48:16.\n",
      "  current average loss = 0.239830639417837\n",
      "  Batch 2,980  of  46,094.    Elapsed: 0:48:36.\n",
      "  current average loss = 0.23937601456668653\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:48:55.\n",
      "  current average loss = 0.23911472960251073\n",
      "  Batch 3,020  of  46,094.    Elapsed: 0:49:15.\n",
      "  current average loss = 0.23871827376883056\n",
      "  Batch 3,040  of  46,094.    Elapsed: 0:49:34.\n",
      "  current average loss = 0.23836238471925944\n",
      "  Batch 3,060  of  46,094.    Elapsed: 0:49:54.\n",
      "  current average loss = 0.23816567307910302\n",
      "  Batch 3,080  of  46,094.    Elapsed: 0:50:13.\n",
      "  current average loss = 0.2377635250194603\n",
      "  Batch 3,100  of  46,094.    Elapsed: 0:50:33.\n",
      "  current average loss = 0.23762231207783183\n",
      "  Batch 3,120  of  46,094.    Elapsed: 0:50:53.\n",
      "  current average loss = 0.23710689783239594\n",
      "  Batch 3,140  of  46,094.    Elapsed: 0:51:12.\n",
      "  current average loss = 0.2369026446999733\n",
      "  Batch 3,160  of  46,094.    Elapsed: 0:51:32.\n",
      "  current average loss = 0.23666140530140528\n",
      "  Batch 3,180  of  46,094.    Elapsed: 0:51:51.\n",
      "  current average loss = 0.236217732008242\n",
      "  Batch 3,200  of  46,094.    Elapsed: 0:52:11.\n",
      "  current average loss = 0.23612320523010566\n",
      "  Batch 3,220  of  46,094.    Elapsed: 0:52:30.\n",
      "  current average loss = 0.2359017956307771\n",
      "  Batch 3,240  of  46,094.    Elapsed: 0:52:50.\n",
      "  current average loss = 0.23532633535379982\n",
      "  Batch 3,260  of  46,094.    Elapsed: 0:53:09.\n",
      "  current average loss = 0.23531174968151045\n",
      "  Batch 3,280  of  46,094.    Elapsed: 0:53:29.\n",
      "  current average loss = 0.23514888512265936\n",
      "  Batch 3,300  of  46,094.    Elapsed: 0:53:48.\n",
      "  current average loss = 0.23463784578171643\n",
      "  Batch 3,320  of  46,094.    Elapsed: 0:54:08.\n",
      "  current average loss = 0.23420370226075132\n",
      "  Batch 3,340  of  46,094.    Elapsed: 0:54:27.\n",
      "  current average loss = 0.2340634395351667\n",
      "  Batch 3,360  of  46,094.    Elapsed: 0:54:47.\n",
      "  current average loss = 0.23386473037923375\n",
      "  Batch 3,380  of  46,094.    Elapsed: 0:55:06.\n",
      "  current average loss = 0.2337111293850742\n",
      "  Batch 3,400  of  46,094.    Elapsed: 0:55:26.\n",
      "  current average loss = 0.23336345413371043\n",
      "  Batch 3,420  of  46,094.    Elapsed: 0:55:45.\n",
      "  current average loss = 0.23321805864746809\n",
      "  Batch 3,440  of  46,094.    Elapsed: 0:56:05.\n",
      "  current average loss = 0.23286843361592915\n",
      "  Batch 3,460  of  46,094.    Elapsed: 0:56:24.\n",
      "  current average loss = 0.2326488874788057\n",
      "  Batch 3,480  of  46,094.    Elapsed: 0:56:44.\n",
      "  current average loss = 0.23242943969437446\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:57:04.\n",
      "  current average loss = 0.232108724527061\n",
      "  Batch 3,520  of  46,094.    Elapsed: 0:57:23.\n",
      "  current average loss = 0.23188231664138254\n",
      "  Batch 3,540  of  46,094.    Elapsed: 0:57:43.\n",
      "  current average loss = 0.2318005491837354\n",
      "  Batch 3,560  of  46,094.    Elapsed: 0:58:02.\n",
      "  current average loss = 0.23170014426970248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,580  of  46,094.    Elapsed: 0:58:22.\n",
      "  current average loss = 0.23172058185572897\n",
      "  Batch 3,600  of  46,094.    Elapsed: 0:58:41.\n",
      "  current average loss = 0.2314276478356785\n",
      "  Batch 3,620  of  46,094.    Elapsed: 0:59:01.\n",
      "  current average loss = 0.23119373669981627\n",
      "  Batch 3,640  of  46,094.    Elapsed: 0:59:20.\n",
      "  current average loss = 0.2310831024413826\n",
      "  Batch 3,660  of  46,094.    Elapsed: 0:59:40.\n",
      "  current average loss = 0.23108487712229536\n",
      "  Batch 3,680  of  46,094.    Elapsed: 0:59:59.\n",
      "  current average loss = 0.2307760171511251\n",
      "  Batch 3,700  of  46,094.    Elapsed: 1:00:19.\n",
      "  current average loss = 0.2307320638997732\n",
      "  Batch 3,720  of  46,094.    Elapsed: 1:00:38.\n",
      "  current average loss = 0.23056110459409895\n",
      "  Batch 3,740  of  46,094.    Elapsed: 1:00:58.\n",
      "  current average loss = 0.230335403672036\n",
      "  Batch 3,760  of  46,094.    Elapsed: 1:01:18.\n",
      "  current average loss = 0.2300824071290566\n",
      "  Batch 3,780  of  46,094.    Elapsed: 1:01:37.\n",
      "  current average loss = 0.2298507022154\n",
      "  Batch 3,800  of  46,094.    Elapsed: 1:01:57.\n",
      "  current average loss = 0.229688596061774\n",
      "  Batch 3,820  of  46,094.    Elapsed: 1:02:16.\n",
      "  current average loss = 0.2295965907276067\n",
      "  Batch 3,840  of  46,094.    Elapsed: 1:02:36.\n",
      "  current average loss = 0.2291183817142155\n",
      "  Batch 3,860  of  46,094.    Elapsed: 1:02:55.\n",
      "  current average loss = 0.2288515250579758\n",
      "  Batch 3,880  of  46,094.    Elapsed: 1:03:15.\n",
      "  current average loss = 0.22848344771935575\n",
      "  Batch 3,900  of  46,094.    Elapsed: 1:03:35.\n",
      "  current average loss = 0.2284407869284638\n",
      "  Batch 3,920  of  46,094.    Elapsed: 1:03:54.\n",
      "  current average loss = 0.22817949722592282\n",
      "  Batch 3,940  of  46,094.    Elapsed: 1:04:14.\n",
      "  current average loss = 0.2279312040994823\n",
      "  Batch 3,960  of  46,094.    Elapsed: 1:04:33.\n",
      "  current average loss = 0.22762323487808722\n",
      "  Batch 3,980  of  46,094.    Elapsed: 1:04:53.\n",
      "  current average loss = 0.22759428431897719\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:05:12.\n",
      "  current average loss = 0.22729053750704042\n",
      "  Batch 4,020  of  46,094.    Elapsed: 1:05:32.\n",
      "  current average loss = 0.2271145871166251\n",
      "  Batch 4,040  of  46,094.    Elapsed: 1:05:51.\n",
      "  current average loss = 0.22687188750294285\n",
      "  Batch 4,060  of  46,094.    Elapsed: 1:06:11.\n",
      "  current average loss = 0.22659492705893597\n",
      "  Batch 4,080  of  46,094.    Elapsed: 1:06:30.\n",
      "  current average loss = 0.22625865583860444\n",
      "  Batch 4,100  of  46,094.    Elapsed: 1:06:50.\n",
      "  current average loss = 0.22622969200539333\n",
      "  Batch 4,120  of  46,094.    Elapsed: 1:07:09.\n",
      "  current average loss = 0.22606857170925015\n",
      "  Batch 4,140  of  46,094.    Elapsed: 1:07:29.\n",
      "  current average loss = 0.22599090082584408\n",
      "  Batch 4,160  of  46,094.    Elapsed: 1:07:48.\n",
      "  current average loss = 0.2259670629584714\n",
      "  Batch 4,180  of  46,094.    Elapsed: 1:08:08.\n",
      "  current average loss = 0.2256890564370437\n",
      "  Batch 4,200  of  46,094.    Elapsed: 1:08:27.\n",
      "  current average loss = 0.22561785035434045\n",
      "  Batch 4,220  of  46,094.    Elapsed: 1:08:47.\n",
      "  current average loss = 0.22540917739656122\n",
      "  Batch 4,240  of  46,094.    Elapsed: 1:09:06.\n",
      "  current average loss = 0.2252283651871554\n",
      "  Batch 4,260  of  46,094.    Elapsed: 1:09:26.\n",
      "  current average loss = 0.22493220507123557\n",
      "  Batch 4,280  of  46,094.    Elapsed: 1:09:45.\n",
      "  current average loss = 0.22476470431108375\n",
      "  Batch 4,300  of  46,094.    Elapsed: 1:10:05.\n",
      "  current average loss = 0.2244748096967246\n",
      "  Batch 4,320  of  46,094.    Elapsed: 1:10:25.\n",
      "  current average loss = 0.2242268077341443\n",
      "  Batch 4,340  of  46,094.    Elapsed: 1:10:44.\n",
      "  current average loss = 0.2240053570895855\n",
      "  Batch 4,360  of  46,094.    Elapsed: 1:11:04.\n",
      "  current average loss = 0.22366208042799915\n",
      "  Batch 4,380  of  46,094.    Elapsed: 1:11:23.\n",
      "  current average loss = 0.22346165087860775\n",
      "  Batch 4,400  of  46,094.    Elapsed: 1:11:43.\n",
      "  current average loss = 0.22324634536190635\n",
      "  Batch 4,420  of  46,094.    Elapsed: 1:12:02.\n",
      "  current average loss = 0.22318495846041625\n",
      "  Batch 4,440  of  46,094.    Elapsed: 1:12:22.\n",
      "  current average loss = 0.223050608231921\n",
      "  Batch 4,460  of  46,094.    Elapsed: 1:12:41.\n",
      "  current average loss = 0.2227490741352346\n",
      "  Batch 4,480  of  46,094.    Elapsed: 1:13:01.\n",
      "  current average loss = 0.2226188266976221\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:13:21.\n",
      "  current average loss = 0.2223556528615041\n",
      "  Batch 4,520  of  46,094.    Elapsed: 1:13:40.\n",
      "  current average loss = 0.2222614258175419\n",
      "  Batch 4,540  of  46,094.    Elapsed: 1:13:59.\n",
      "  current average loss = 0.2220141368062699\n",
      "  Batch 4,560  of  46,094.    Elapsed: 1:14:19.\n",
      "  current average loss = 0.2218031889622165\n",
      "  Batch 4,580  of  46,094.    Elapsed: 1:14:38.\n",
      "  current average loss = 0.22182150649447727\n",
      "  Batch 4,600  of  46,094.    Elapsed: 1:14:58.\n",
      "  current average loss = 0.22164284823685074\n",
      "  Batch 4,620  of  46,094.    Elapsed: 1:15:17.\n",
      "  current average loss = 0.22151325146175793\n",
      "  Batch 4,640  of  46,094.    Elapsed: 1:15:37.\n",
      "  current average loss = 0.2213673088569114\n",
      "  Batch 4,660  of  46,094.    Elapsed: 1:15:56.\n",
      "  current average loss = 0.22132255274446705\n",
      "  Batch 4,680  of  46,094.    Elapsed: 1:16:16.\n",
      "  current average loss = 0.22124068673489153\n",
      "  Batch 4,700  of  46,094.    Elapsed: 1:16:36.\n",
      "  current average loss = 0.22100404602079157\n",
      "  Batch 4,720  of  46,094.    Elapsed: 1:16:55.\n",
      "  current average loss = 0.22089976038195794\n",
      "  Batch 4,740  of  46,094.    Elapsed: 1:17:15.\n",
      "  current average loss = 0.22061035427312026\n",
      "  Batch 4,760  of  46,094.    Elapsed: 1:17:34.\n",
      "  current average loss = 0.22047290651249002\n",
      "  Batch 4,780  of  46,094.    Elapsed: 1:17:54.\n",
      "  current average loss = 0.22026792752564298\n",
      "  Batch 4,800  of  46,094.    Elapsed: 1:18:13.\n",
      "  current average loss = 0.22017999527878904\n",
      "  Batch 4,820  of  46,094.    Elapsed: 1:18:33.\n",
      "  current average loss = 0.219859045142447\n",
      "  Batch 4,840  of  46,094.    Elapsed: 1:18:52.\n",
      "  current average loss = 0.21985151193601132\n",
      "  Batch 4,860  of  46,094.    Elapsed: 1:19:12.\n",
      "  current average loss = 0.21963809689740676\n",
      "  Batch 4,880  of  46,094.    Elapsed: 1:19:32.\n",
      "  current average loss = 0.21953903217017498\n",
      "  Batch 4,900  of  46,094.    Elapsed: 1:19:51.\n",
      "  current average loss = 0.21942771739706549\n",
      "  Batch 4,920  of  46,094.    Elapsed: 1:20:11.\n",
      "  current average loss = 0.21920526379518726\n",
      "  Batch 4,940  of  46,094.    Elapsed: 1:20:30.\n",
      "  current average loss = 0.21899928087613693\n",
      "  Batch 4,960  of  46,094.    Elapsed: 1:20:50.\n",
      "  current average loss = 0.21886219831705545\n",
      "  Batch 4,980  of  46,094.    Elapsed: 1:21:09.\n",
      "  current average loss = 0.21867914151093626\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:21:28.\n",
      "  current average loss = 0.21855453835316002\n",
      "  Batch 5,020  of  46,094.    Elapsed: 1:21:48.\n",
      "  current average loss = 0.21842328585810455\n",
      "  Batch 5,040  of  46,094.    Elapsed: 1:22:07.\n",
      "  current average loss = 0.21833942672644283\n",
      "  Batch 5,060  of  46,094.    Elapsed: 1:22:27.\n",
      "  current average loss = 0.21824301756484057\n",
      "  Batch 5,080  of  46,094.    Elapsed: 1:22:46.\n",
      "  current average loss = 0.2181013155996242\n",
      "  Batch 5,100  of  46,094.    Elapsed: 1:23:06.\n",
      "  current average loss = 0.2178708188568114\n",
      "  Batch 5,120  of  46,094.    Elapsed: 1:23:26.\n",
      "  current average loss = 0.21768576555077743\n",
      "  Batch 5,140  of  46,094.    Elapsed: 1:23:45.\n",
      "  current average loss = 0.21757345585775806\n",
      "  Batch 5,160  of  46,094.    Elapsed: 1:24:05.\n",
      "  current average loss = 0.21741920221026612\n",
      "  Batch 5,180  of  46,094.    Elapsed: 1:24:24.\n",
      "  current average loss = 0.2173171350412416\n",
      "  Batch 5,200  of  46,094.    Elapsed: 1:24:44.\n",
      "  current average loss = 0.21705564443487674\n",
      "  Batch 5,220  of  46,094.    Elapsed: 1:25:03.\n",
      "  current average loss = 0.21703960221881669\n",
      "  Batch 5,240  of  46,094.    Elapsed: 1:25:23.\n",
      "  current average loss = 0.216777736748718\n",
      "  Batch 5,260  of  46,094.    Elapsed: 1:25:42.\n",
      "  current average loss = 0.21655566434698187\n",
      "  Batch 5,280  of  46,094.    Elapsed: 1:26:01.\n",
      "  current average loss = 0.21651659999254413\n",
      "  Batch 5,300  of  46,094.    Elapsed: 1:26:21.\n",
      "  current average loss = 0.2164873595591986\n",
      "  Batch 5,320  of  46,094.    Elapsed: 1:26:41.\n",
      "  current average loss = 0.21628387498396232\n",
      "  Batch 5,340  of  46,094.    Elapsed: 1:27:00.\n",
      "  current average loss = 0.21613036856850545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,360  of  46,094.    Elapsed: 1:27:20.\n",
      "  current average loss = 0.21603943056621547\n",
      "  Batch 5,380  of  46,094.    Elapsed: 1:27:39.\n",
      "  current average loss = 0.21584740003508596\n",
      "  Batch 5,400  of  46,094.    Elapsed: 1:27:59.\n",
      "  current average loss = 0.21578729584675144\n",
      "  Batch 5,420  of  46,094.    Elapsed: 1:28:18.\n",
      "  current average loss = 0.21566373441863215\n",
      "  Batch 5,440  of  46,094.    Elapsed: 1:28:38.\n",
      "  current average loss = 0.21541275619494948\n",
      "  Batch 5,460  of  46,094.    Elapsed: 1:28:57.\n",
      "  current average loss = 0.21522417515993883\n",
      "  Batch 5,480  of  46,094.    Elapsed: 1:29:17.\n",
      "  current average loss = 0.21509580162360611\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:29:36.\n",
      "  current average loss = 0.21509487054226073\n",
      "  Batch 5,520  of  46,094.    Elapsed: 1:29:56.\n",
      "  current average loss = 0.21500188480131327\n",
      "  Batch 5,540  of  46,094.    Elapsed: 1:30:16.\n",
      "  current average loss = 0.21490083777342356\n",
      "  Batch 5,560  of  46,094.    Elapsed: 1:30:35.\n",
      "  current average loss = 0.21484143007077652\n",
      "  Batch 5,580  of  46,094.    Elapsed: 1:30:55.\n",
      "  current average loss = 0.21473829734830124\n",
      "  Batch 5,600  of  46,094.    Elapsed: 1:31:15.\n",
      "  current average loss = 0.21455156414237406\n",
      "  Batch 5,620  of  46,094.    Elapsed: 1:31:34.\n",
      "  current average loss = 0.21431280853478668\n",
      "  Batch 5,640  of  46,094.    Elapsed: 1:31:54.\n",
      "  current average loss = 0.2141827698060516\n",
      "  Batch 5,660  of  46,094.    Elapsed: 1:32:14.\n",
      "  current average loss = 0.21401984279501407\n",
      "  Batch 5,680  of  46,094.    Elapsed: 1:32:33.\n",
      "  current average loss = 0.21401266516678194\n",
      "  Batch 5,700  of  46,094.    Elapsed: 1:32:53.\n",
      "  current average loss = 0.21389358656873045\n",
      "  Batch 5,720  of  46,094.    Elapsed: 1:33:13.\n",
      "  current average loss = 0.21377374935615157\n",
      "  Batch 5,740  of  46,094.    Elapsed: 1:33:32.\n",
      "  current average loss = 0.2135255546438855\n",
      "  Batch 5,760  of  46,094.    Elapsed: 1:33:52.\n",
      "  current average loss = 0.2132962527840088\n",
      "  Batch 5,780  of  46,094.    Elapsed: 1:34:11.\n",
      "  current average loss = 0.21315050068501526\n",
      "  Batch 5,800  of  46,094.    Elapsed: 1:34:31.\n",
      "  current average loss = 0.21297755211752292\n",
      "  Batch 5,820  of  46,094.    Elapsed: 1:34:50.\n",
      "  current average loss = 0.21296902276300328\n",
      "  Batch 5,840  of  46,094.    Elapsed: 1:35:10.\n",
      "  current average loss = 0.21285282319111826\n",
      "  Batch 5,860  of  46,094.    Elapsed: 1:35:29.\n",
      "  current average loss = 0.2126570174537294\n",
      "  Batch 5,880  of  46,094.    Elapsed: 1:35:49.\n",
      "  current average loss = 0.2125120503140525\n",
      "  Batch 5,900  of  46,094.    Elapsed: 1:36:08.\n",
      "  current average loss = 0.21244373967176525\n",
      "  Batch 5,920  of  46,094.    Elapsed: 1:36:28.\n",
      "  current average loss = 0.21223090780565415\n",
      "  Batch 5,940  of  46,094.    Elapsed: 1:36:47.\n",
      "  current average loss = 0.2120691634397891\n",
      "  Batch 5,960  of  46,094.    Elapsed: 1:37:07.\n",
      "  current average loss = 0.21183290777777966\n",
      "  Batch 5,980  of  46,094.    Elapsed: 1:37:27.\n",
      "  current average loss = 0.2117650380731875\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:37:46.\n",
      "  current average loss = 0.21171501649854083\n",
      "  Batch 6,020  of  46,094.    Elapsed: 1:38:06.\n",
      "  current average loss = 0.2115332105379564\n",
      "  Batch 6,040  of  46,094.    Elapsed: 1:38:25.\n",
      "  current average loss = 0.2113946881268593\n",
      "  Batch 6,060  of  46,094.    Elapsed: 1:38:45.\n",
      "  current average loss = 0.21121570139250445\n",
      "  Batch 6,080  of  46,094.    Elapsed: 1:39:04.\n",
      "  current average loss = 0.21097560467559992\n",
      "  Batch 6,100  of  46,094.    Elapsed: 1:39:24.\n",
      "  current average loss = 0.21074856659886046\n",
      "  Batch 6,120  of  46,094.    Elapsed: 1:39:43.\n",
      "  current average loss = 0.2106427903192763\n",
      "  Batch 6,140  of  46,094.    Elapsed: 1:40:03.\n",
      "  current average loss = 0.21063360457766667\n",
      "  Batch 6,160  of  46,094.    Elapsed: 1:40:22.\n",
      "  current average loss = 0.21056829051072287\n",
      "  Batch 6,180  of  46,094.    Elapsed: 1:40:42.\n",
      "  current average loss = 0.21037635278848865\n",
      "  Batch 6,200  of  46,094.    Elapsed: 1:41:01.\n",
      "  current average loss = 0.21024453760725595\n",
      "  Batch 6,220  of  46,094.    Elapsed: 1:41:21.\n",
      "  current average loss = 0.21020036590932434\n",
      "  Batch 6,240  of  46,094.    Elapsed: 1:41:40.\n",
      "  current average loss = 0.21012548716508975\n",
      "  Batch 6,260  of  46,094.    Elapsed: 1:42:00.\n",
      "  current average loss = 0.2099742446372958\n",
      "  Batch 6,280  of  46,094.    Elapsed: 1:42:20.\n",
      "  current average loss = 0.2098156978184903\n",
      "  Batch 6,300  of  46,094.    Elapsed: 1:42:40.\n",
      "  current average loss = 0.20971185566399186\n",
      "  Batch 6,320  of  46,094.    Elapsed: 1:42:59.\n",
      "  current average loss = 0.20961013628166947\n",
      "  Batch 6,340  of  46,094.    Elapsed: 1:43:19.\n",
      "  current average loss = 0.2094308615123048\n",
      "  Batch 6,360  of  46,094.    Elapsed: 1:43:38.\n",
      "  current average loss = 0.20926711674144127\n",
      "  Batch 6,380  of  46,094.    Elapsed: 1:43:58.\n",
      "  current average loss = 0.20923159788858997\n",
      "  Batch 6,400  of  46,094.    Elapsed: 1:44:18.\n",
      "  current average loss = 0.20904807698170771\n",
      "  Batch 6,420  of  46,094.    Elapsed: 1:44:37.\n",
      "  current average loss = 0.208965392412641\n",
      "  Batch 6,440  of  46,094.    Elapsed: 1:44:57.\n",
      "  current average loss = 0.20874385322510663\n",
      "  Batch 6,460  of  46,094.    Elapsed: 1:45:16.\n",
      "  current average loss = 0.2085329805222562\n",
      "  Batch 6,480  of  46,094.    Elapsed: 1:45:36.\n",
      "  current average loss = 0.20847662965563\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:45:55.\n",
      "  current average loss = 0.20829615993410922\n",
      "  Batch 6,520  of  46,094.    Elapsed: 1:46:15.\n",
      "  current average loss = 0.20812199201186307\n",
      "  Batch 6,540  of  46,094.    Elapsed: 1:46:34.\n",
      "  current average loss = 0.2080149275329055\n",
      "  Batch 6,560  of  46,094.    Elapsed: 1:46:54.\n",
      "  current average loss = 0.20803699664751682\n",
      "  Batch 6,580  of  46,094.    Elapsed: 1:47:13.\n",
      "  current average loss = 0.20782051477482186\n",
      "  Batch 6,600  of  46,094.    Elapsed: 1:47:33.\n",
      "  current average loss = 0.20773140544029461\n",
      "  Batch 6,620  of  46,094.    Elapsed: 1:47:52.\n",
      "  current average loss = 0.20758116643726712\n",
      "  Batch 6,640  of  46,094.    Elapsed: 1:48:12.\n",
      "  current average loss = 0.2074445995854225\n",
      "  Batch 6,660  of  46,094.    Elapsed: 1:48:32.\n",
      "  current average loss = 0.20721666258060606\n",
      "  Batch 6,680  of  46,094.    Elapsed: 1:48:52.\n",
      "  current average loss = 0.2070974750281172\n",
      "  Batch 6,700  of  46,094.    Elapsed: 1:49:11.\n",
      "  current average loss = 0.2069259802820578\n",
      "  Batch 6,720  of  46,094.    Elapsed: 1:49:31.\n",
      "  current average loss = 0.20680742460383392\n",
      "  Batch 6,740  of  46,094.    Elapsed: 1:49:51.\n",
      "  current average loss = 0.20673805419949798\n",
      "  Batch 6,760  of  46,094.    Elapsed: 1:50:10.\n",
      "  current average loss = 0.2064846232369509\n",
      "  Batch 6,780  of  46,094.    Elapsed: 1:50:30.\n",
      "  current average loss = 0.20645242435404354\n",
      "  Batch 6,800  of  46,094.    Elapsed: 1:50:49.\n",
      "  current average loss = 0.20645076238344806\n",
      "  Batch 6,820  of  46,094.    Elapsed: 1:51:09.\n",
      "  current average loss = 0.20631456175379323\n",
      "  Batch 6,840  of  46,094.    Elapsed: 1:51:28.\n",
      "  current average loss = 0.2061878391839727\n",
      "  Batch 6,860  of  46,094.    Elapsed: 1:51:48.\n",
      "  current average loss = 0.20602994840089828\n",
      "  Batch 6,880  of  46,094.    Elapsed: 1:52:07.\n",
      "  current average loss = 0.20589856452993552\n",
      "  Batch 6,900  of  46,094.    Elapsed: 1:52:27.\n",
      "  current average loss = 0.20579108856766876\n",
      "  Batch 6,920  of  46,094.    Elapsed: 1:52:46.\n",
      "  current average loss = 0.20574270587325677\n",
      "  Batch 6,940  of  46,094.    Elapsed: 1:53:05.\n",
      "  current average loss = 0.20556864529050972\n",
      "  Batch 6,960  of  46,094.    Elapsed: 1:53:25.\n",
      "  current average loss = 0.2055569374373172\n",
      "  Batch 6,980  of  46,094.    Elapsed: 1:53:45.\n",
      "  current average loss = 0.20542397167717766\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:54:04.\n",
      "  current average loss = 0.20535534744989126\n",
      "  Batch 7,020  of  46,094.    Elapsed: 1:54:24.\n",
      "  current average loss = 0.2053547055459186\n",
      "  Batch 7,040  of  46,094.    Elapsed: 1:54:43.\n",
      "  current average loss = 0.20513783697560112\n",
      "  Batch 7,060  of  46,094.    Elapsed: 1:55:03.\n",
      "  current average loss = 0.20496644761384314\n",
      "  Batch 7,080  of  46,094.    Elapsed: 1:55:23.\n",
      "  current average loss = 0.204825860038426\n",
      "  Batch 7,100  of  46,094.    Elapsed: 1:55:42.\n",
      "  current average loss = 0.20485889158998682\n",
      "  Batch 7,120  of  46,094.    Elapsed: 1:56:02.\n",
      "  current average loss = 0.2048114074683183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7,140  of  46,094.    Elapsed: 1:56:21.\n",
      "  current average loss = 0.20458111937643036\n",
      "  Batch 7,160  of  46,094.    Elapsed: 1:56:41.\n",
      "  current average loss = 0.20449233052344193\n",
      "  Batch 7,180  of  46,094.    Elapsed: 1:57:01.\n",
      "  current average loss = 0.20431595980098943\n",
      "  Batch 7,200  of  46,094.    Elapsed: 1:57:20.\n",
      "  current average loss = 0.2042221578083829\n",
      "  Batch 7,220  of  46,094.    Elapsed: 1:57:40.\n",
      "  current average loss = 0.2042076225805295\n",
      "  Batch 7,240  of  46,094.    Elapsed: 1:58:00.\n",
      "  current average loss = 0.20416844377515436\n",
      "  Batch 7,260  of  46,094.    Elapsed: 1:58:19.\n",
      "  current average loss = 0.20401002054767217\n",
      "  Batch 7,280  of  46,094.    Elapsed: 1:58:39.\n",
      "  current average loss = 0.2039089649212086\n",
      "  Batch 7,300  of  46,094.    Elapsed: 1:58:58.\n",
      "  current average loss = 0.20377173934820783\n",
      "  Batch 7,320  of  46,094.    Elapsed: 1:59:18.\n",
      "  current average loss = 0.2036259635176021\n",
      "  Batch 7,340  of  46,094.    Elapsed: 1:59:37.\n",
      "  current average loss = 0.20348432043222178\n",
      "  Batch 7,360  of  46,094.    Elapsed: 1:59:57.\n",
      "  current average loss = 0.20335604221959921\n",
      "  Batch 7,380  of  46,094.    Elapsed: 2:00:16.\n",
      "  current average loss = 0.2033020639310018\n",
      "  Batch 7,400  of  46,094.    Elapsed: 2:00:36.\n",
      "  current average loss = 0.20319113441893982\n",
      "  Batch 7,420  of  46,094.    Elapsed: 2:00:55.\n",
      "  current average loss = 0.20311636779773629\n",
      "  Batch 7,440  of  46,094.    Elapsed: 2:01:15.\n",
      "  current average loss = 0.2030000385322598\n",
      "  Batch 7,460  of  46,094.    Elapsed: 2:01:34.\n",
      "  current average loss = 0.20288889353505607\n",
      "  Batch 7,480  of  46,094.    Elapsed: 2:01:54.\n",
      "  current average loss = 0.2027194046674366\n",
      "  Batch 7,500  of  46,094.    Elapsed: 2:02:14.\n",
      "  current average loss = 0.20276944691886503\n",
      "  Batch 7,520  of  46,094.    Elapsed: 2:02:34.\n",
      "  current average loss = 0.2025859193267063\n",
      "  Batch 7,540  of  46,094.    Elapsed: 2:02:54.\n",
      "  current average loss = 0.202431717509557\n",
      "  Batch 7,560  of  46,094.    Elapsed: 2:03:13.\n",
      "  current average loss = 0.20231439742818969\n",
      "  Batch 7,580  of  46,094.    Elapsed: 2:03:33.\n",
      "  current average loss = 0.2021703340785239\n",
      "  Batch 7,600  of  46,094.    Elapsed: 2:03:52.\n",
      "  current average loss = 0.20204120936644215\n",
      "  Batch 7,620  of  46,094.    Elapsed: 2:04:12.\n",
      "  current average loss = 0.20187695786941143\n",
      "  Batch 7,640  of  46,094.    Elapsed: 2:04:31.\n",
      "  current average loss = 0.20176778656628022\n",
      "  Batch 7,660  of  46,094.    Elapsed: 2:04:51.\n",
      "  current average loss = 0.20167315911134467\n",
      "  Batch 7,680  of  46,094.    Elapsed: 2:05:10.\n",
      "  current average loss = 0.20155532408352883\n",
      "  Batch 7,700  of  46,094.    Elapsed: 2:05:30.\n",
      "  current average loss = 0.20142838318251655\n",
      "  Batch 7,720  of  46,094.    Elapsed: 2:05:49.\n",
      "  current average loss = 0.20130154077009504\n",
      "  Batch 7,740  of  46,094.    Elapsed: 2:06:09.\n",
      "  current average loss = 0.20120693449910432\n",
      "  Batch 7,760  of  46,094.    Elapsed: 2:06:28.\n",
      "  current average loss = 0.20117627009017316\n",
      "  Batch 7,780  of  46,094.    Elapsed: 2:06:48.\n",
      "  current average loss = 0.2010638923746962\n",
      "  Batch 7,800  of  46,094.    Elapsed: 2:07:07.\n",
      "  current average loss = 0.20088930676106173\n",
      "  Batch 7,820  of  46,094.    Elapsed: 2:07:27.\n",
      "  current average loss = 0.20087791781865366\n",
      "  Batch 7,840  of  46,094.    Elapsed: 2:07:46.\n",
      "  current average loss = 0.20075959622496453\n",
      "  Batch 7,860  of  46,094.    Elapsed: 2:08:06.\n",
      "  current average loss = 0.20058145251980586\n",
      "  Batch 7,880  of  46,094.    Elapsed: 2:08:25.\n",
      "  current average loss = 0.2005092881930765\n",
      "  Batch 7,900  of  46,094.    Elapsed: 2:08:45.\n",
      "  current average loss = 0.20038814118756806\n",
      "  Batch 7,920  of  46,094.    Elapsed: 2:09:04.\n",
      "  current average loss = 0.2002638116087341\n",
      "  Batch 7,940  of  46,094.    Elapsed: 2:09:24.\n",
      "  current average loss = 0.20018482428379544\n",
      "  Batch 7,960  of  46,094.    Elapsed: 2:09:44.\n",
      "  current average loss = 0.2000698897188492\n",
      "  Batch 7,980  of  46,094.    Elapsed: 2:10:03.\n",
      "  current average loss = 0.20005082308109337\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:10:23.\n",
      "  current average loss = 0.1999432064491557\n",
      "  Batch 8,020  of  46,094.    Elapsed: 2:10:42.\n",
      "  current average loss = 0.19976865822261203\n",
      "  Batch 8,040  of  46,094.    Elapsed: 2:11:02.\n",
      "  current average loss = 0.1997389118422277\n",
      "  Batch 8,060  of  46,094.    Elapsed: 2:11:21.\n",
      "  current average loss = 0.19959552085302104\n",
      "  Batch 8,080  of  46,094.    Elapsed: 2:11:40.\n",
      "  current average loss = 0.19946882484566034\n",
      "  Batch 8,100  of  46,094.    Elapsed: 2:12:00.\n",
      "  current average loss = 0.19930738521084465\n",
      "  Batch 8,120  of  46,094.    Elapsed: 2:12:19.\n",
      "  current average loss = 0.19924592349616344\n",
      "  Batch 8,140  of  46,094.    Elapsed: 2:12:39.\n",
      "  current average loss = 0.19909906771082056\n",
      "  Batch 8,160  of  46,094.    Elapsed: 2:12:59.\n",
      "  current average loss = 0.19890565667563923\n",
      "  Batch 8,180  of  46,094.    Elapsed: 2:13:18.\n",
      "  current average loss = 0.19875774699226842\n",
      "  Batch 8,200  of  46,094.    Elapsed: 2:13:38.\n",
      "  current average loss = 0.19855845438441397\n",
      "  Batch 8,220  of  46,094.    Elapsed: 2:13:57.\n",
      "  current average loss = 0.1984502373230383\n",
      "  Batch 8,240  of  46,094.    Elapsed: 2:14:17.\n",
      "  current average loss = 0.19834243821746497\n",
      "  Batch 8,260  of  46,094.    Elapsed: 2:14:36.\n",
      "  current average loss = 0.19824406465269698\n",
      "  Batch 8,280  of  46,094.    Elapsed: 2:14:56.\n",
      "  current average loss = 0.19809369044659597\n",
      "  Batch 8,300  of  46,094.    Elapsed: 2:15:15.\n",
      "  current average loss = 0.19800548944386642\n",
      "  Batch 8,320  of  46,094.    Elapsed: 2:15:35.\n",
      "  current average loss = 0.1979111466878049\n",
      "  Batch 8,340  of  46,094.    Elapsed: 2:15:55.\n",
      "  current average loss = 0.19771922250251495\n",
      "  Batch 8,360  of  46,094.    Elapsed: 2:16:14.\n",
      "  current average loss = 0.19758701397420206\n",
      "  Batch 8,380  of  46,094.    Elapsed: 2:16:34.\n",
      "  current average loss = 0.19750453955449412\n",
      "  Batch 8,400  of  46,094.    Elapsed: 2:16:53.\n",
      "  current average loss = 0.19733294739443366\n",
      "  Batch 8,420  of  46,094.    Elapsed: 2:17:13.\n",
      "  current average loss = 0.19728327903115675\n",
      "  Batch 8,440  of  46,094.    Elapsed: 2:17:32.\n",
      "  current average loss = 0.19718339148880162\n",
      "  Batch 8,460  of  46,094.    Elapsed: 2:17:52.\n",
      "  current average loss = 0.1970194166055033\n",
      "  Batch 8,480  of  46,094.    Elapsed: 2:18:11.\n",
      "  current average loss = 0.19689719231715777\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:18:31.\n",
      "  current average loss = 0.1968991511792821\n",
      "  Batch 8,520  of  46,094.    Elapsed: 2:18:50.\n",
      "  current average loss = 0.19677308517630637\n",
      "  Batch 8,540  of  46,094.    Elapsed: 2:19:10.\n",
      "  current average loss = 0.19666650494063312\n",
      "  Batch 8,560  of  46,094.    Elapsed: 2:19:29.\n",
      "  current average loss = 0.1966151571530135\n",
      "  Batch 8,580  of  46,094.    Elapsed: 2:19:49.\n",
      "  current average loss = 0.19658576059578495\n",
      "  Batch 8,600  of  46,094.    Elapsed: 2:20:08.\n",
      "  current average loss = 0.19648652693884838\n",
      "  Batch 8,620  of  46,094.    Elapsed: 2:20:28.\n",
      "  current average loss = 0.19640898518501987\n",
      "  Batch 8,640  of  46,094.    Elapsed: 2:20:47.\n",
      "  current average loss = 0.19624785326585104\n",
      "  Batch 8,660  of  46,094.    Elapsed: 2:21:07.\n",
      "  current average loss = 0.1961338576541323\n",
      "  Batch 8,680  of  46,094.    Elapsed: 2:21:27.\n",
      "  current average loss = 0.19605285202953449\n",
      "  Batch 8,700  of  46,094.    Elapsed: 2:21:49.\n",
      "  current average loss = 0.19599435566838874\n",
      "  Batch 8,720  of  46,094.    Elapsed: 2:22:08.\n",
      "  current average loss = 0.19584954092930576\n",
      "  Batch 8,740  of  46,094.    Elapsed: 2:22:28.\n",
      "  current average loss = 0.19581598674407488\n",
      "  Batch 8,760  of  46,094.    Elapsed: 2:22:47.\n",
      "  current average loss = 0.19576183635854166\n",
      "  Batch 8,780  of  46,094.    Elapsed: 2:23:07.\n",
      "  current average loss = 0.19563344301632551\n",
      "  Batch 8,800  of  46,094.    Elapsed: 2:23:26.\n",
      "  current average loss = 0.19554739569176244\n",
      "  Batch 8,820  of  46,094.    Elapsed: 2:23:46.\n",
      "  current average loss = 0.19541824998581137\n",
      "  Batch 8,840  of  46,094.    Elapsed: 2:24:05.\n",
      "  current average loss = 0.19535250758122213\n",
      "  Batch 8,860  of  46,094.    Elapsed: 2:24:25.\n",
      "  current average loss = 0.19528079462073675\n",
      "  Batch 8,880  of  46,094.    Elapsed: 2:24:44.\n",
      "  current average loss = 0.19509452413180256\n",
      "  Batch 8,900  of  46,094.    Elapsed: 2:25:04.\n",
      "  current average loss = 0.19498974789687423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8,920  of  46,094.    Elapsed: 2:25:23.\n",
      "  current average loss = 0.19475046516053646\n",
      "  Batch 8,940  of  46,094.    Elapsed: 2:25:43.\n",
      "  current average loss = 0.1946692486033177\n",
      "  Batch 8,960  of  46,094.    Elapsed: 2:26:02.\n",
      "  current average loss = 0.19458929059432453\n",
      "  Batch 8,980  of  46,094.    Elapsed: 2:26:22.\n",
      "  current average loss = 0.19445918303632276\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:26:41.\n",
      "  current average loss = 0.19430433496843197\n",
      "  Batch 9,020  of  46,094.    Elapsed: 2:27:00.\n",
      "  current average loss = 0.19432204397335368\n",
      "  Batch 9,040  of  46,094.    Elapsed: 2:27:20.\n",
      "  current average loss = 0.19425391191956629\n",
      "  Batch 9,060  of  46,094.    Elapsed: 2:27:39.\n",
      "  current average loss = 0.19412459730145995\n",
      "  Batch 9,080  of  46,094.    Elapsed: 2:27:59.\n",
      "  current average loss = 0.19407804644008123\n",
      "  Batch 9,100  of  46,094.    Elapsed: 2:28:18.\n",
      "  current average loss = 0.19404721949771456\n",
      "  Batch 9,120  of  46,094.    Elapsed: 2:28:38.\n",
      "  current average loss = 0.19399569020412552\n",
      "  Batch 9,140  of  46,094.    Elapsed: 2:28:58.\n",
      "  current average loss = 0.19388349810951436\n",
      "  Batch 9,160  of  46,094.    Elapsed: 2:29:17.\n",
      "  current average loss = 0.1937455535952742\n",
      "  Batch 9,180  of  46,094.    Elapsed: 2:29:37.\n",
      "  current average loss = 0.19364480701952863\n",
      "  Batch 9,200  of  46,094.    Elapsed: 2:29:56.\n",
      "  current average loss = 0.19354452030173927\n",
      "  Batch 9,220  of  46,094.    Elapsed: 2:30:16.\n",
      "  current average loss = 0.19344705438780258\n",
      "  Batch 9,240  of  46,094.    Elapsed: 2:30:35.\n",
      "  current average loss = 0.1933967301127898\n",
      "  Batch 9,260  of  46,094.    Elapsed: 2:30:55.\n",
      "  current average loss = 0.1932291129901432\n",
      "  Batch 9,280  of  46,094.    Elapsed: 2:31:14.\n",
      "  current average loss = 0.19311718917409146\n",
      "  Batch 9,300  of  46,094.    Elapsed: 2:31:34.\n",
      "  current average loss = 0.19298039488952548\n",
      "  Batch 9,320  of  46,094.    Elapsed: 2:31:53.\n",
      "  current average loss = 0.1928490741669786\n",
      "  Batch 9,340  of  46,094.    Elapsed: 2:32:13.\n",
      "  current average loss = 0.19270999590032165\n",
      "  Batch 9,360  of  46,094.    Elapsed: 2:32:32.\n",
      "  current average loss = 0.19260300830029983\n",
      "  Batch 9,380  of  46,094.    Elapsed: 2:32:52.\n",
      "  current average loss = 0.1925031238911437\n",
      "  Batch 9,400  of  46,094.    Elapsed: 2:33:11.\n",
      "  current average loss = 0.192473803580159\n",
      "  Batch 9,420  of  46,094.    Elapsed: 2:33:31.\n",
      "  current average loss = 0.19238286045666156\n",
      "  Batch 9,440  of  46,094.    Elapsed: 2:33:50.\n",
      "  current average loss = 0.19230750727798232\n",
      "  Batch 9,460  of  46,094.    Elapsed: 2:34:10.\n",
      "  current average loss = 0.1922539448063086\n",
      "  Batch 9,480  of  46,094.    Elapsed: 2:34:29.\n",
      "  current average loss = 0.19215017286026634\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:34:49.\n",
      "  current average loss = 0.19205296449007858\n",
      "  Batch 9,520  of  46,094.    Elapsed: 2:35:08.\n",
      "  current average loss = 0.1919192302836831\n",
      "  Batch 9,540  of  46,094.    Elapsed: 2:35:28.\n",
      "  current average loss = 0.19189002347201697\n",
      "  Batch 9,560  of  46,094.    Elapsed: 2:35:48.\n",
      "  current average loss = 0.19177788127012546\n",
      "  Batch 9,580  of  46,094.    Elapsed: 2:36:07.\n",
      "  current average loss = 0.19170472653121406\n",
      "  Batch 9,600  of  46,094.    Elapsed: 2:36:27.\n",
      "  current average loss = 0.19161033902567093\n",
      "  Batch 9,620  of  46,094.    Elapsed: 2:36:46.\n",
      "  current average loss = 0.19162552673473718\n",
      "  Batch 9,640  of  46,094.    Elapsed: 2:37:06.\n",
      "  current average loss = 0.1915581092639117\n",
      "  Batch 9,660  of  46,094.    Elapsed: 2:37:25.\n",
      "  current average loss = 0.19142252209605495\n",
      "  Batch 9,680  of  46,094.    Elapsed: 2:37:45.\n",
      "  current average loss = 0.19130883523528097\n",
      "  Batch 9,700  of  46,094.    Elapsed: 2:38:04.\n",
      "  current average loss = 0.19120527037781826\n",
      "  Batch 9,720  of  46,094.    Elapsed: 2:38:24.\n",
      "  current average loss = 0.19108236023258904\n",
      "  Batch 9,740  of  46,094.    Elapsed: 2:38:43.\n",
      "  current average loss = 0.1910526936427017\n",
      "  Batch 9,760  of  46,094.    Elapsed: 2:39:03.\n",
      "  current average loss = 0.19107413261784834\n",
      "  Batch 9,780  of  46,094.    Elapsed: 2:39:22.\n",
      "  current average loss = 0.19101512273152194\n",
      "  Batch 9,800  of  46,094.    Elapsed: 2:39:42.\n",
      "  current average loss = 0.19092018533277097\n",
      "  Batch 9,820  of  46,094.    Elapsed: 2:40:01.\n",
      "  current average loss = 0.19078531576035537\n",
      "  Batch 9,840  of  46,094.    Elapsed: 2:40:21.\n",
      "  current average loss = 0.1907227920164587\n",
      "  Batch 9,860  of  46,094.    Elapsed: 2:40:40.\n",
      "  current average loss = 0.19061242171190584\n",
      "  Batch 9,880  of  46,094.    Elapsed: 2:41:00.\n",
      "  current average loss = 0.19049986816312542\n",
      "  Batch 9,900  of  46,094.    Elapsed: 2:41:19.\n",
      "  current average loss = 0.19043036341796526\n",
      "  Batch 9,920  of  46,094.    Elapsed: 2:41:39.\n",
      "  current average loss = 0.1904087086393819\n",
      "  Batch 9,940  of  46,094.    Elapsed: 2:41:58.\n",
      "  current average loss = 0.19030818294945043\n",
      "  Batch 9,960  of  46,094.    Elapsed: 2:42:18.\n",
      "  current average loss = 0.19021992923869063\n",
      "  Batch 9,980  of  46,094.    Elapsed: 2:42:37.\n",
      "  current average loss = 0.19005111682008044\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:42:57.\n",
      "  current average loss = 0.1899920058630174\n",
      "  Batch 10,020  of  46,094.    Elapsed: 2:43:17.\n",
      "  current average loss = 0.18987895387804374\n",
      "  Batch 10,040  of  46,094.    Elapsed: 2:43:36.\n",
      "  current average loss = 0.1898125115792922\n",
      "  Batch 10,060  of  46,094.    Elapsed: 2:43:56.\n",
      "  current average loss = 0.18974797533078408\n",
      "  Batch 10,080  of  46,094.    Elapsed: 2:44:15.\n",
      "  current average loss = 0.18970997386077504\n",
      "  Batch 10,100  of  46,094.    Elapsed: 2:44:35.\n",
      "  current average loss = 0.18960969330980396\n",
      "  Batch 10,120  of  46,094.    Elapsed: 2:44:54.\n",
      "  current average loss = 0.18948204559897508\n",
      "  Batch 10,140  of  46,094.    Elapsed: 2:45:14.\n",
      "  current average loss = 0.18937967002069894\n",
      "  Batch 10,160  of  46,094.    Elapsed: 2:45:33.\n",
      "  current average loss = 0.1892913856688203\n",
      "  Batch 10,180  of  46,094.    Elapsed: 2:45:53.\n",
      "  current average loss = 0.18921106793002404\n",
      "  Batch 10,200  of  46,094.    Elapsed: 2:46:12.\n",
      "  current average loss = 0.1890830010118867\n",
      "  Batch 10,220  of  46,094.    Elapsed: 2:46:32.\n",
      "  current average loss = 0.18902332613592304\n",
      "  Batch 10,240  of  46,094.    Elapsed: 2:46:51.\n",
      "  current average loss = 0.18889077380333674\n",
      "  Batch 10,260  of  46,094.    Elapsed: 2:47:11.\n",
      "  current average loss = 0.18874844404236554\n",
      "  Batch 10,280  of  46,094.    Elapsed: 2:47:30.\n",
      "  current average loss = 0.18870955421034416\n",
      "  Batch 10,300  of  46,094.    Elapsed: 2:47:50.\n",
      "  current average loss = 0.18864683994125955\n",
      "  Batch 10,320  of  46,094.    Elapsed: 2:48:09.\n",
      "  current average loss = 0.18862036857995396\n",
      "  Batch 10,340  of  46,094.    Elapsed: 2:48:29.\n",
      "  current average loss = 0.188526533247531\n",
      "  Batch 10,360  of  46,094.    Elapsed: 2:48:48.\n",
      "  current average loss = 0.1884664423204374\n",
      "  Batch 10,380  of  46,094.    Elapsed: 2:49:08.\n",
      "  current average loss = 0.1883574316042802\n",
      "  Batch 10,400  of  46,094.    Elapsed: 2:49:27.\n",
      "  current average loss = 0.18828522108752693\n",
      "  Batch 10,420  of  46,094.    Elapsed: 2:49:47.\n",
      "  current average loss = 0.18820914391706645\n",
      "  Batch 10,440  of  46,094.    Elapsed: 2:50:06.\n",
      "  current average loss = 0.18808883559875372\n",
      "  Batch 10,460  of  46,094.    Elapsed: 2:50:26.\n",
      "  current average loss = 0.18802123925803108\n",
      "  Batch 10,480  of  46,094.    Elapsed: 2:50:45.\n",
      "  current average loss = 0.18793545439739387\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:51:05.\n",
      "  current average loss = 0.18786220649565527\n",
      "  Batch 10,520  of  46,094.    Elapsed: 2:51:24.\n",
      "  current average loss = 0.1877688451916884\n",
      "  Batch 10,540  of  46,094.    Elapsed: 2:51:44.\n",
      "  current average loss = 0.18772989556631178\n",
      "  Batch 10,560  of  46,094.    Elapsed: 2:52:04.\n",
      "  current average loss = 0.1876544082121682\n",
      "  Batch 10,580  of  46,094.    Elapsed: 2:52:23.\n",
      "  current average loss = 0.18752950118202805\n",
      "  Batch 10,600  of  46,094.    Elapsed: 2:52:43.\n",
      "  current average loss = 0.18745976257689279\n",
      "  Batch 10,620  of  46,094.    Elapsed: 2:53:02.\n",
      "  current average loss = 0.18739411111843463\n",
      "  Batch 10,640  of  46,094.    Elapsed: 2:53:22.\n",
      "  current average loss = 0.18724908762901613\n",
      "  Batch 10,660  of  46,094.    Elapsed: 2:53:41.\n",
      "  current average loss = 0.1872125840998136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,680  of  46,094.    Elapsed: 2:54:01.\n",
      "  current average loss = 0.18712652213302483\n",
      "  Batch 10,700  of  46,094.    Elapsed: 2:54:21.\n",
      "  current average loss = 0.18705909194466092\n",
      "  Batch 10,720  of  46,094.    Elapsed: 2:54:40.\n",
      "  current average loss = 0.18696996770030733\n",
      "  Batch 10,740  of  46,094.    Elapsed: 2:55:00.\n",
      "  current average loss = 0.18691085102147156\n",
      "  Batch 10,760  of  46,094.    Elapsed: 2:55:19.\n",
      "  current average loss = 0.18684613099899472\n",
      "  Batch 10,780  of  46,094.    Elapsed: 2:55:39.\n",
      "  current average loss = 0.1867756642714611\n",
      "  Batch 10,800  of  46,094.    Elapsed: 2:55:58.\n",
      "  current average loss = 0.18664498800125093\n",
      "  Batch 10,820  of  46,094.    Elapsed: 2:56:18.\n",
      "  current average loss = 0.18651371871002015\n",
      "  Batch 10,840  of  46,094.    Elapsed: 2:56:37.\n",
      "  current average loss = 0.18645867569158558\n",
      "  Batch 10,860  of  46,094.    Elapsed: 2:56:56.\n",
      "  current average loss = 0.18634135332067872\n",
      "  Batch 10,880  of  46,094.    Elapsed: 2:57:16.\n",
      "  current average loss = 0.18626565062224792\n",
      "  Batch 10,900  of  46,094.    Elapsed: 2:57:35.\n",
      "  current average loss = 0.18619459885947937\n",
      "  Batch 10,920  of  46,094.    Elapsed: 2:57:55.\n",
      "  current average loss = 0.18607155016498494\n",
      "  Batch 10,940  of  46,094.    Elapsed: 2:58:14.\n",
      "  current average loss = 0.18599878413229415\n",
      "  Batch 10,960  of  46,094.    Elapsed: 2:58:34.\n",
      "  current average loss = 0.18590977710982307\n",
      "  Batch 10,980  of  46,094.    Elapsed: 2:58:53.\n",
      "  current average loss = 0.18583903825095005\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:59:13.\n",
      "  current average loss = 0.1857786469977476\n",
      "  Batch 11,020  of  46,094.    Elapsed: 2:59:32.\n",
      "  current average loss = 0.18570123562712254\n",
      "  Batch 11,040  of  46,094.    Elapsed: 2:59:51.\n",
      "  current average loss = 0.18562821586001782\n",
      "  Batch 11,060  of  46,094.    Elapsed: 3:00:11.\n",
      "  current average loss = 0.18553788502813043\n",
      "  Batch 11,080  of  46,094.    Elapsed: 3:00:31.\n",
      "  current average loss = 0.18541887979918759\n",
      "  Batch 11,100  of  46,094.    Elapsed: 3:00:51.\n",
      "  current average loss = 0.1853634106862644\n",
      "  Batch 11,120  of  46,094.    Elapsed: 3:01:10.\n",
      "  current average loss = 0.18526217299126166\n",
      "  Batch 11,140  of  46,094.    Elapsed: 3:01:30.\n",
      "  current average loss = 0.18513400343042352\n",
      "  Batch 11,160  of  46,094.    Elapsed: 3:01:49.\n",
      "  current average loss = 0.18510681041364063\n",
      "  Batch 11,180  of  46,094.    Elapsed: 3:02:09.\n",
      "  current average loss = 0.18504372108525685\n",
      "  Batch 11,200  of  46,094.    Elapsed: 3:02:28.\n",
      "  current average loss = 0.18495739385092747\n",
      "  Batch 11,220  of  46,094.    Elapsed: 3:02:48.\n",
      "  current average loss = 0.18492146042611785\n",
      "  Batch 11,240  of  46,094.    Elapsed: 3:03:07.\n",
      "  current average loss = 0.18483028359173836\n",
      "  Batch 11,260  of  46,094.    Elapsed: 3:03:27.\n",
      "  current average loss = 0.18474796580365357\n",
      "  Batch 11,280  of  46,094.    Elapsed: 3:03:46.\n",
      "  current average loss = 0.18463908237087662\n",
      "  Batch 11,300  of  46,094.    Elapsed: 3:04:06.\n",
      "  current average loss = 0.18450743081351018\n",
      "  Batch 11,320  of  46,094.    Elapsed: 3:04:25.\n",
      "  current average loss = 0.1844099321749593\n",
      "  Batch 11,340  of  46,094.    Elapsed: 3:04:45.\n",
      "  current average loss = 0.18428755573206868\n",
      "  Batch 11,360  of  46,094.    Elapsed: 3:05:04.\n",
      "  current average loss = 0.18420689644955646\n",
      "  Batch 11,380  of  46,094.    Elapsed: 3:05:24.\n",
      "  current average loss = 0.18412788474393407\n",
      "  Batch 11,400  of  46,094.    Elapsed: 3:05:44.\n",
      "  current average loss = 0.18405674122353574\n",
      "  Batch 11,420  of  46,094.    Elapsed: 3:06:03.\n",
      "  current average loss = 0.1839144908560272\n",
      "  Batch 11,440  of  46,094.    Elapsed: 3:06:23.\n",
      "  current average loss = 0.18388577031777192\n",
      "  Batch 11,460  of  46,094.    Elapsed: 3:06:42.\n",
      "  current average loss = 0.18382833371356205\n",
      "  Batch 11,480  of  46,094.    Elapsed: 3:07:02.\n",
      "  current average loss = 0.18370740467206678\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:07:21.\n",
      "  current average loss = 0.18359917246969418\n",
      "  Batch 11,520  of  46,094.    Elapsed: 3:07:41.\n",
      "  current average loss = 0.18352598542840295\n",
      "  Batch 11,540  of  46,094.    Elapsed: 3:08:01.\n",
      "  current average loss = 0.1834492248219706\n",
      "  Batch 11,560  of  46,094.    Elapsed: 3:08:20.\n",
      "  current average loss = 0.18338883502252165\n",
      "  Batch 11,580  of  46,094.    Elapsed: 3:08:40.\n",
      "  current average loss = 0.1832867405523501\n",
      "  Batch 11,600  of  46,094.    Elapsed: 3:08:59.\n",
      "  current average loss = 0.18315489775418897\n",
      "  Batch 11,620  of  46,094.    Elapsed: 3:09:18.\n",
      "  current average loss = 0.18303516359373417\n",
      "  Batch 11,640  of  46,094.    Elapsed: 3:09:38.\n",
      "  current average loss = 0.18296420836928812\n",
      "  Batch 11,660  of  46,094.    Elapsed: 3:09:58.\n",
      "  current average loss = 0.18290754405617488\n",
      "  Batch 11,680  of  46,094.    Elapsed: 3:10:17.\n",
      "  current average loss = 0.1828451913468299\n",
      "  Batch 11,700  of  46,094.    Elapsed: 3:10:36.\n",
      "  current average loss = 0.1827522198134102\n",
      "  Batch 11,720  of  46,094.    Elapsed: 3:10:56.\n",
      "  current average loss = 0.1826937663618995\n",
      "  Batch 11,740  of  46,094.    Elapsed: 3:11:15.\n",
      "  current average loss = 0.18260026347815111\n",
      "  Batch 11,760  of  46,094.    Elapsed: 3:11:35.\n",
      "  current average loss = 0.18249370432844889\n",
      "  Batch 11,780  of  46,094.    Elapsed: 3:11:54.\n",
      "  current average loss = 0.18238716583130946\n",
      "  Batch 11,800  of  46,094.    Elapsed: 3:12:14.\n",
      "  current average loss = 0.18231525810007243\n",
      "  Batch 11,820  of  46,094.    Elapsed: 3:12:33.\n",
      "  current average loss = 0.18224886372579457\n",
      "  Batch 11,840  of  46,094.    Elapsed: 3:12:53.\n",
      "  current average loss = 0.18219776520333678\n",
      "  Batch 11,860  of  46,094.    Elapsed: 3:13:12.\n",
      "  current average loss = 0.18209634584327217\n",
      "  Batch 11,880  of  46,094.    Elapsed: 3:13:32.\n",
      "  current average loss = 0.1820128513142421\n",
      "  Batch 11,900  of  46,094.    Elapsed: 3:13:52.\n",
      "  current average loss = 0.18194506141801264\n",
      "  Batch 11,920  of  46,094.    Elapsed: 3:14:11.\n",
      "  current average loss = 0.18184120895904754\n",
      "  Batch 11,940  of  46,094.    Elapsed: 3:14:31.\n",
      "  current average loss = 0.18174881790193279\n",
      "  Batch 11,960  of  46,094.    Elapsed: 3:14:50.\n",
      "  current average loss = 0.18167258243021606\n",
      "  Batch 11,980  of  46,094.    Elapsed: 3:15:10.\n",
      "  current average loss = 0.18155631865139582\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:15:29.\n",
      "  current average loss = 0.18154678022727602\n",
      "  Batch 12,020  of  46,094.    Elapsed: 3:15:49.\n",
      "  current average loss = 0.18148070350062712\n",
      "  Batch 12,040  of  46,094.    Elapsed: 3:16:08.\n",
      "  current average loss = 0.18143652287484735\n",
      "  Batch 12,060  of  46,094.    Elapsed: 3:16:28.\n",
      "  current average loss = 0.18140302633063152\n",
      "  Batch 12,080  of  46,094.    Elapsed: 3:16:47.\n",
      "  current average loss = 0.18128496278345274\n",
      "  Batch 12,100  of  46,094.    Elapsed: 3:17:07.\n",
      "  current average loss = 0.18125363079682838\n",
      "  Batch 12,120  of  46,094.    Elapsed: 3:17:26.\n",
      "  current average loss = 0.18112907815244733\n",
      "  Batch 12,140  of  46,094.    Elapsed: 3:17:46.\n",
      "  current average loss = 0.18099792385878521\n",
      "  Batch 12,160  of  46,094.    Elapsed: 3:18:05.\n",
      "  current average loss = 0.1808725677712669\n",
      "  Batch 12,180  of  46,094.    Elapsed: 3:18:25.\n",
      "  current average loss = 0.1808409352042278\n",
      "  Batch 12,200  of  46,094.    Elapsed: 3:18:44.\n",
      "  current average loss = 0.18074670399953305\n",
      "  Batch 12,220  of  46,094.    Elapsed: 3:19:04.\n",
      "  current average loss = 0.18063285531216636\n",
      "  Batch 12,240  of  46,094.    Elapsed: 3:19:23.\n",
      "  current average loss = 0.18057232972276716\n",
      "  Batch 12,260  of  46,094.    Elapsed: 3:19:43.\n",
      "  current average loss = 0.18052984652848958\n",
      "  Batch 12,280  of  46,094.    Elapsed: 3:20:03.\n",
      "  current average loss = 0.1804794387105232\n",
      "  Batch 12,300  of  46,094.    Elapsed: 3:20:22.\n",
      "  current average loss = 0.18038732097936583\n",
      "  Batch 12,320  of  46,094.    Elapsed: 3:20:42.\n",
      "  current average loss = 0.18027463195693919\n",
      "  Batch 12,340  of  46,094.    Elapsed: 3:21:01.\n",
      "  current average loss = 0.18020417583022172\n",
      "  Batch 12,360  of  46,094.    Elapsed: 3:21:21.\n",
      "  current average loss = 0.18014936572892823\n",
      "  Batch 12,380  of  46,094.    Elapsed: 3:21:40.\n",
      "  current average loss = 0.18012799506919364\n",
      "  Batch 12,400  of  46,094.    Elapsed: 3:22:00.\n",
      "  current average loss = 0.18001169894391955\n",
      "  Batch 12,420  of  46,094.    Elapsed: 3:22:19.\n",
      "  current average loss = 0.17996774272108002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 12,440  of  46,094.    Elapsed: 3:22:39.\n",
      "  current average loss = 0.17990088473346272\n",
      "  Batch 12,460  of  46,094.    Elapsed: 3:22:58.\n",
      "  current average loss = 0.1798222375457193\n",
      "  Batch 12,480  of  46,094.    Elapsed: 3:23:18.\n",
      "  current average loss = 0.17971026721842906\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:23:37.\n",
      "  current average loss = 0.17964564169248565\n",
      "  Batch 12,520  of  46,094.    Elapsed: 3:23:57.\n",
      "  current average loss = 0.17959061860344552\n",
      "  Batch 12,540  of  46,094.    Elapsed: 3:24:16.\n",
      "  current average loss = 0.17953686180449888\n",
      "  Batch 12,560  of  46,094.    Elapsed: 3:24:36.\n",
      "  current average loss = 0.1794662518763231\n",
      "  Batch 12,580  of  46,094.    Elapsed: 3:24:55.\n",
      "  current average loss = 0.1793711286930061\n",
      "  Batch 12,600  of  46,094.    Elapsed: 3:25:15.\n",
      "  current average loss = 0.17933393071972883\n",
      "  Batch 12,620  of  46,094.    Elapsed: 3:25:34.\n",
      "  current average loss = 0.1792657421303141\n",
      "  Batch 12,640  of  46,094.    Elapsed: 3:25:54.\n",
      "  current average loss = 0.1791637985140956\n",
      "  Batch 12,660  of  46,094.    Elapsed: 3:26:14.\n",
      "  current average loss = 0.1790299930416105\n",
      "  Batch 12,680  of  46,094.    Elapsed: 3:26:33.\n",
      "  current average loss = 0.17892166307360913\n",
      "  Batch 12,700  of  46,094.    Elapsed: 3:26:53.\n",
      "  current average loss = 0.17881678205564633\n",
      "  Batch 12,720  of  46,094.    Elapsed: 3:27:12.\n",
      "  current average loss = 0.17875072781647094\n",
      "  Batch 12,740  of  46,094.    Elapsed: 3:27:32.\n",
      "  current average loss = 0.1786470464652432\n",
      "  Batch 12,760  of  46,094.    Elapsed: 3:27:51.\n",
      "  current average loss = 0.1786321309861708\n",
      "  Batch 12,780  of  46,094.    Elapsed: 3:28:11.\n",
      "  current average loss = 0.1785507530297724\n",
      "  Batch 12,800  of  46,094.    Elapsed: 3:28:30.\n",
      "  current average loss = 0.17849912271196444\n",
      "  Batch 12,820  of  46,094.    Elapsed: 3:28:50.\n",
      "  current average loss = 0.17840830785211093\n",
      "  Batch 12,840  of  46,094.    Elapsed: 3:29:09.\n",
      "  current average loss = 0.17830691226154952\n",
      "  Batch 12,860  of  46,094.    Elapsed: 3:29:29.\n",
      "  current average loss = 0.1782011517058677\n",
      "  Batch 12,880  of  46,094.    Elapsed: 3:29:48.\n",
      "  current average loss = 0.17819585950673855\n",
      "  Batch 12,900  of  46,094.    Elapsed: 3:30:08.\n",
      "  current average loss = 0.17809936861617692\n",
      "  Batch 12,920  of  46,094.    Elapsed: 3:30:27.\n",
      "  current average loss = 0.17801948685443025\n",
      "  Batch 12,940  of  46,094.    Elapsed: 3:30:46.\n",
      "  current average loss = 0.17795244579995484\n",
      "  Batch 12,960  of  46,094.    Elapsed: 3:31:06.\n",
      "  current average loss = 0.17790093854341704\n",
      "  Batch 12,980  of  46,094.    Elapsed: 3:31:25.\n",
      "  current average loss = 0.17784528895519827\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:31:45.\n",
      "  current average loss = 0.177778131646337\n",
      "  Batch 13,020  of  46,094.    Elapsed: 3:32:04.\n",
      "  current average loss = 0.17764094072365819\n",
      "  Batch 13,040  of  46,094.    Elapsed: 3:32:24.\n",
      "  current average loss = 0.17750781883642383\n",
      "  Batch 13,060  of  46,094.    Elapsed: 3:32:44.\n",
      "  current average loss = 0.17748237447076867\n",
      "  Batch 13,080  of  46,094.    Elapsed: 3:33:03.\n",
      "  current average loss = 0.17738576879052356\n",
      "  Batch 13,100  of  46,094.    Elapsed: 3:33:23.\n",
      "  current average loss = 0.17730719107997653\n",
      "  Batch 13,120  of  46,094.    Elapsed: 3:33:42.\n",
      "  current average loss = 0.17718222028057293\n",
      "  Batch 13,140  of  46,094.    Elapsed: 3:34:02.\n",
      "  current average loss = 0.17711316684069242\n",
      "  Batch 13,160  of  46,094.    Elapsed: 3:34:21.\n",
      "  current average loss = 0.17704485651885166\n",
      "  Batch 13,180  of  46,094.    Elapsed: 3:34:41.\n",
      "  current average loss = 0.17699268014692773\n",
      "  Batch 13,200  of  46,094.    Elapsed: 3:35:00.\n",
      "  current average loss = 0.17690503251468975\n",
      "  Batch 13,220  of  46,094.    Elapsed: 3:35:20.\n",
      "  current average loss = 0.17684909938135412\n",
      "  Batch 13,240  of  46,094.    Elapsed: 3:35:39.\n",
      "  current average loss = 0.17680539398007813\n",
      "  Batch 13,260  of  46,094.    Elapsed: 3:35:59.\n",
      "  current average loss = 0.17670507681052808\n",
      "  Batch 13,280  of  46,094.    Elapsed: 3:36:18.\n",
      "  current average loss = 0.1765984920851279\n",
      "  Batch 13,300  of  46,094.    Elapsed: 3:36:38.\n",
      "  current average loss = 0.1765358560279369\n",
      "  Batch 13,320  of  46,094.    Elapsed: 3:36:57.\n",
      "  current average loss = 0.176534388399548\n",
      "  Batch 13,340  of  46,094.    Elapsed: 3:37:17.\n",
      "  current average loss = 0.17651639590477933\n",
      "  Batch 13,360  of  46,094.    Elapsed: 3:37:36.\n",
      "  current average loss = 0.17640526765352046\n",
      "  Batch 13,380  of  46,094.    Elapsed: 3:37:56.\n",
      "  current average loss = 0.1763499327917703\n",
      "  Batch 13,400  of  46,094.    Elapsed: 3:38:15.\n",
      "  current average loss = 0.1762914779713315\n",
      "  Batch 13,420  of  46,094.    Elapsed: 3:38:35.\n",
      "  current average loss = 0.1762245315104\n",
      "  Batch 13,440  of  46,094.    Elapsed: 3:38:55.\n",
      "  current average loss = 0.17615387949833045\n",
      "  Batch 13,460  of  46,094.    Elapsed: 3:39:14.\n",
      "  current average loss = 0.17607706756928498\n",
      "  Batch 13,480  of  46,094.    Elapsed: 3:39:34.\n",
      "  current average loss = 0.17596606165058398\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:39:54.\n",
      "  current average loss = 0.17590265571945174\n",
      "  Batch 13,520  of  46,094.    Elapsed: 3:40:13.\n",
      "  current average loss = 0.17583322866718884\n",
      "  Batch 13,540  of  46,094.    Elapsed: 3:40:33.\n",
      "  current average loss = 0.17575468312213996\n",
      "  Batch 13,560  of  46,094.    Elapsed: 3:40:52.\n",
      "  current average loss = 0.1757007442413902\n",
      "  Batch 13,580  of  46,094.    Elapsed: 3:41:12.\n",
      "  current average loss = 0.17566188389361\n",
      "  Batch 13,600  of  46,094.    Elapsed: 3:41:31.\n",
      "  current average loss = 0.17556406056212248\n",
      "  Batch 13,620  of  46,094.    Elapsed: 3:41:50.\n",
      "  current average loss = 0.17547416637517257\n",
      "  Batch 13,640  of  46,094.    Elapsed: 3:42:10.\n",
      "  current average loss = 0.17536804422671645\n",
      "  Batch 13,660  of  46,094.    Elapsed: 3:42:30.\n",
      "  current average loss = 0.1752756690739935\n",
      "  Batch 13,680  of  46,094.    Elapsed: 3:42:49.\n",
      "  current average loss = 0.17521753359968265\n",
      "  Batch 13,700  of  46,094.    Elapsed: 3:43:09.\n",
      "  current average loss = 0.175130497392957\n",
      "  Batch 13,720  of  46,094.    Elapsed: 3:43:28.\n",
      "  current average loss = 0.1750667159562169\n",
      "  Batch 13,740  of  46,094.    Elapsed: 3:43:48.\n",
      "  current average loss = 0.17502883691772794\n",
      "  Batch 13,760  of  46,094.    Elapsed: 3:44:07.\n",
      "  current average loss = 0.1749649565576049\n",
      "  Batch 13,780  of  46,094.    Elapsed: 3:44:26.\n",
      "  current average loss = 0.17487491681989004\n",
      "  Batch 13,800  of  46,094.    Elapsed: 3:44:46.\n",
      "  current average loss = 0.17487791518773546\n",
      "  Batch 13,820  of  46,094.    Elapsed: 3:45:06.\n",
      "  current average loss = 0.1748409010622045\n",
      "  Batch 13,840  of  46,094.    Elapsed: 3:45:25.\n",
      "  current average loss = 0.1747580160838895\n",
      "  Batch 13,860  of  46,094.    Elapsed: 3:45:44.\n",
      "  current average loss = 0.17468307348165438\n",
      "  Batch 13,880  of  46,094.    Elapsed: 3:46:04.\n",
      "  current average loss = 0.17458770898462983\n",
      "  Batch 13,900  of  46,094.    Elapsed: 3:46:23.\n",
      "  current average loss = 0.17451237796777325\n",
      "  Batch 13,920  of  46,094.    Elapsed: 3:46:43.\n",
      "  current average loss = 0.17440858314978963\n",
      "  Batch 13,940  of  46,094.    Elapsed: 3:47:02.\n",
      "  current average loss = 0.17433595014280082\n",
      "  Batch 13,960  of  46,094.    Elapsed: 3:47:22.\n",
      "  current average loss = 0.17429728437857875\n",
      "  Batch 13,980  of  46,094.    Elapsed: 3:47:41.\n",
      "  current average loss = 0.17426213154051978\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:48:01.\n",
      "  current average loss = 0.17423501260124613\n",
      "  Batch 14,020  of  46,094.    Elapsed: 3:48:20.\n",
      "  current average loss = 0.17417309036112713\n",
      "  Batch 14,040  of  46,094.    Elapsed: 3:48:40.\n",
      "  current average loss = 0.17409153692024637\n",
      "  Batch 14,060  of  46,094.    Elapsed: 3:48:59.\n",
      "  current average loss = 0.1740285217026573\n",
      "  Batch 14,080  of  46,094.    Elapsed: 3:49:19.\n",
      "  current average loss = 0.17398307988728035\n",
      "  Batch 14,100  of  46,094.    Elapsed: 3:49:38.\n",
      "  current average loss = 0.1739215679185181\n",
      "  Batch 14,120  of  46,094.    Elapsed: 3:49:58.\n",
      "  current average loss = 0.1738486183206874\n",
      "  Batch 14,140  of  46,094.    Elapsed: 3:50:17.\n",
      "  current average loss = 0.17373642503431125\n",
      "  Batch 14,160  of  46,094.    Elapsed: 3:50:37.\n",
      "  current average loss = 0.17366349113094529\n",
      "  Batch 14,180  of  46,094.    Elapsed: 3:50:56.\n",
      "  current average loss = 0.17356663263061023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 14,200  of  46,094.    Elapsed: 3:51:16.\n",
      "  current average loss = 0.1735750619965953\n",
      "  Batch 14,220  of  46,094.    Elapsed: 3:51:35.\n",
      "  current average loss = 0.17349256354273768\n",
      "  Batch 14,240  of  46,094.    Elapsed: 3:51:55.\n",
      "  current average loss = 0.1734742240103719\n",
      "  Batch 14,260  of  46,094.    Elapsed: 3:52:15.\n",
      "  current average loss = 0.1733761899151544\n",
      "  Batch 14,280  of  46,094.    Elapsed: 3:52:34.\n",
      "  current average loss = 0.17328571609920218\n",
      "  Batch 14,300  of  46,094.    Elapsed: 3:52:54.\n",
      "  current average loss = 0.17320332151510381\n",
      "  Batch 14,320  of  46,094.    Elapsed: 3:53:13.\n",
      "  current average loss = 0.17312850039463695\n",
      "  Batch 14,340  of  46,094.    Elapsed: 3:53:33.\n",
      "  current average loss = 0.17305635099350883\n",
      "  Batch 14,360  of  46,094.    Elapsed: 3:53:52.\n",
      "  current average loss = 0.1729721783437088\n",
      "  Batch 14,380  of  46,094.    Elapsed: 3:54:12.\n",
      "  current average loss = 0.17288797794864222\n",
      "  Batch 14,400  of  46,094.    Elapsed: 3:54:31.\n",
      "  current average loss = 0.1727738382983403\n",
      "  Batch 14,420  of  46,094.    Elapsed: 3:54:51.\n",
      "  current average loss = 0.17274552992777253\n",
      "  Batch 14,440  of  46,094.    Elapsed: 3:55:10.\n",
      "  current average loss = 0.17266499330210655\n",
      "  Batch 14,460  of  46,094.    Elapsed: 3:55:30.\n",
      "  current average loss = 0.17260073446299956\n",
      "  Batch 14,480  of  46,094.    Elapsed: 3:55:50.\n",
      "  current average loss = 0.1725137104463438\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:56:09.\n",
      "  current average loss = 0.1724776722622939\n",
      "  Batch 14,520  of  46,094.    Elapsed: 3:56:29.\n",
      "  current average loss = 0.1723992158446978\n",
      "  Batch 14,540  of  46,094.    Elapsed: 3:56:48.\n",
      "  current average loss = 0.1722950384003351\n",
      "  Batch 14,560  of  46,094.    Elapsed: 3:57:08.\n",
      "  current average loss = 0.17225015862014306\n",
      "  Batch 14,580  of  46,094.    Elapsed: 3:57:27.\n",
      "  current average loss = 0.17214781888752642\n",
      "  Batch 14,600  of  46,094.    Elapsed: 3:57:47.\n",
      "  current average loss = 0.17209745671662297\n",
      "  Batch 14,620  of  46,094.    Elapsed: 3:58:06.\n",
      "  current average loss = 0.17199966421562007\n",
      "  Batch 14,640  of  46,094.    Elapsed: 3:58:26.\n",
      "  current average loss = 0.17194326167124327\n",
      "  Batch 14,660  of  46,094.    Elapsed: 3:58:46.\n",
      "  current average loss = 0.17191543001673626\n",
      "  Batch 14,680  of  46,094.    Elapsed: 3:59:05.\n",
      "  current average loss = 0.1717950647224054\n",
      "  Batch 14,700  of  46,094.    Elapsed: 3:59:25.\n",
      "  current average loss = 0.17171984438060586\n",
      "  Batch 14,720  of  46,094.    Elapsed: 3:59:44.\n",
      "  current average loss = 0.17159438284881776\n",
      "  Batch 14,740  of  46,094.    Elapsed: 4:00:03.\n",
      "  current average loss = 0.1714759548113358\n",
      "  Batch 14,760  of  46,094.    Elapsed: 4:00:23.\n",
      "  current average loss = 0.1714113904417052\n",
      "  Batch 14,780  of  46,094.    Elapsed: 4:00:42.\n",
      "  current average loss = 0.171378924930799\n",
      "  Batch 14,800  of  46,094.    Elapsed: 4:01:02.\n",
      "  current average loss = 0.17133172197772403\n",
      "  Batch 14,820  of  46,094.    Elapsed: 4:01:21.\n",
      "  current average loss = 0.17121180525125293\n",
      "  Batch 14,840  of  46,094.    Elapsed: 4:01:41.\n",
      "  current average loss = 0.17119910944138045\n",
      "  Batch 14,860  of  46,094.    Elapsed: 4:02:00.\n",
      "  current average loss = 0.1711578756812891\n",
      "  Batch 14,880  of  46,094.    Elapsed: 4:02:20.\n",
      "  current average loss = 0.17106171609892984\n",
      "  Batch 14,900  of  46,094.    Elapsed: 4:02:39.\n",
      "  current average loss = 0.17104102680617445\n",
      "  Batch 14,920  of  46,094.    Elapsed: 4:02:59.\n",
      "  current average loss = 0.1709570740065731\n",
      "  Batch 14,940  of  46,094.    Elapsed: 4:03:18.\n",
      "  current average loss = 0.1708684023209895\n",
      "  Batch 14,960  of  46,094.    Elapsed: 4:03:38.\n",
      "  current average loss = 0.17074352560172987\n",
      "  Batch 14,980  of  46,094.    Elapsed: 4:03:57.\n",
      "  current average loss = 0.1706929103671685\n",
      "  Batch 15,000  of  46,094.    Elapsed: 4:04:17.\n",
      "  current average loss = 0.1706285552770365\n",
      "  Batch 15,020  of  46,094.    Elapsed: 4:04:37.\n",
      "  current average loss = 0.17054553235142855\n",
      "  Batch 15,040  of  46,094.    Elapsed: 4:04:56.\n",
      "  current average loss = 0.17047140821422874\n",
      "  Batch 15,060  of  46,094.    Elapsed: 4:05:16.\n",
      "  current average loss = 0.17038187062938898\n",
      "  Batch 15,080  of  46,094.    Elapsed: 4:05:35.\n",
      "  current average loss = 0.1703057389245807\n",
      "  Batch 15,100  of  46,094.    Elapsed: 4:05:55.\n",
      "  current average loss = 0.17026161247865906\n",
      "  Batch 15,120  of  46,094.    Elapsed: 4:06:14.\n",
      "  current average loss = 0.17019222057684727\n",
      "  Batch 15,140  of  46,094.    Elapsed: 4:06:34.\n",
      "  current average loss = 0.17010979250941088\n",
      "  Batch 15,160  of  46,094.    Elapsed: 4:06:53.\n",
      "  current average loss = 0.1700512130115836\n",
      "  Batch 15,180  of  46,094.    Elapsed: 4:07:13.\n",
      "  current average loss = 0.1699490223526102\n",
      "  Batch 15,200  of  46,094.    Elapsed: 4:07:32.\n",
      "  current average loss = 0.16990150662428566\n",
      "  Batch 15,220  of  46,094.    Elapsed: 4:07:52.\n",
      "  current average loss = 0.16983768577363437\n",
      "  Batch 15,240  of  46,094.    Elapsed: 4:08:11.\n",
      "  current average loss = 0.16980300074636637\n",
      "  Batch 15,260  of  46,094.    Elapsed: 4:08:31.\n",
      "  current average loss = 0.16972393278067024\n",
      "  Batch 15,280  of  46,094.    Elapsed: 4:08:50.\n",
      "  current average loss = 0.1696564375873471\n",
      "  Batch 15,300  of  46,094.    Elapsed: 4:09:10.\n",
      "  current average loss = 0.16961927913684777\n",
      "  Batch 15,320  of  46,094.    Elapsed: 4:09:29.\n",
      "  current average loss = 0.1695452536429083\n",
      "  Batch 15,340  of  46,094.    Elapsed: 4:09:49.\n",
      "  current average loss = 0.16944338967405914\n",
      "  Batch 15,360  of  46,094.    Elapsed: 4:10:08.\n",
      "  current average loss = 0.1694226440583255\n",
      "  Batch 15,380  of  46,094.    Elapsed: 4:10:28.\n",
      "  current average loss = 0.169377456534359\n",
      "  Batch 15,400  of  46,094.    Elapsed: 4:10:47.\n",
      "  current average loss = 0.16925215141849242\n",
      "  Batch 15,420  of  46,094.    Elapsed: 4:11:07.\n",
      "  current average loss = 0.16915503956122932\n",
      "  Batch 15,440  of  46,094.    Elapsed: 4:11:26.\n",
      "  current average loss = 0.16912462408197476\n",
      "  Batch 15,460  of  46,094.    Elapsed: 4:11:46.\n",
      "  current average loss = 0.1690184420222425\n",
      "  Batch 15,480  of  46,094.    Elapsed: 4:12:06.\n",
      "  current average loss = 0.16902158465857609\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:12:25.\n",
      "  current average loss = 0.1689633634830224\n",
      "  Batch 15,520  of  46,094.    Elapsed: 4:12:45.\n",
      "  current average loss = 0.16887330799779107\n",
      "  Batch 15,540  of  46,094.    Elapsed: 4:13:04.\n",
      "  current average loss = 0.16879604029976505\n",
      "  Batch 15,560  of  46,094.    Elapsed: 4:13:24.\n",
      "  current average loss = 0.1686845963388223\n",
      "  Batch 15,580  of  46,094.    Elapsed: 4:13:43.\n",
      "  current average loss = 0.168583814685328\n",
      "  Batch 15,600  of  46,094.    Elapsed: 4:14:02.\n",
      "  current average loss = 0.1685392439632396\n",
      "  Batch 15,620  of  46,094.    Elapsed: 4:14:22.\n",
      "  current average loss = 0.16846215668482892\n",
      "  Batch 15,640  of  46,094.    Elapsed: 4:14:41.\n",
      "  current average loss = 0.168392523117688\n",
      "  Batch 15,660  of  46,094.    Elapsed: 4:15:01.\n",
      "  current average loss = 0.16833186282017737\n",
      "  Batch 15,680  of  46,094.    Elapsed: 4:15:20.\n",
      "  current average loss = 0.16829253157978813\n",
      "  Batch 15,700  of  46,094.    Elapsed: 4:15:40.\n",
      "  current average loss = 0.16821263073037443\n",
      "  Batch 15,720  of  46,094.    Elapsed: 4:15:59.\n",
      "  current average loss = 0.16814125975827643\n",
      "  Batch 15,740  of  46,094.    Elapsed: 4:16:19.\n",
      "  current average loss = 0.16805905049764094\n",
      "  Batch 15,760  of  46,094.    Elapsed: 4:16:38.\n",
      "  current average loss = 0.16797085438559817\n",
      "  Batch 15,780  of  46,094.    Elapsed: 4:16:58.\n",
      "  current average loss = 0.16789529645393209\n",
      "  Batch 15,800  of  46,094.    Elapsed: 4:17:17.\n",
      "  current average loss = 0.16781883271335907\n",
      "  Batch 15,820  of  46,094.    Elapsed: 4:17:37.\n",
      "  current average loss = 0.1677127443503161\n",
      "  Batch 15,840  of  46,094.    Elapsed: 4:17:56.\n",
      "  current average loss = 0.167644068142214\n",
      "  Batch 15,860  of  46,094.    Elapsed: 4:18:16.\n",
      "  current average loss = 0.1675530932596321\n",
      "  Batch 15,880  of  46,094.    Elapsed: 4:18:35.\n",
      "  current average loss = 0.1674998698416383\n",
      "  Batch 15,900  of  46,094.    Elapsed: 4:18:55.\n",
      "  current average loss = 0.16742111402163984\n",
      "  Batch 15,920  of  46,094.    Elapsed: 4:19:14.\n",
      "  current average loss = 0.1673822918266085\n",
      "  Batch 15,940  of  46,094.    Elapsed: 4:19:34.\n",
      "  current average loss = 0.16730310532205692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15,960  of  46,094.    Elapsed: 4:19:53.\n",
      "  current average loss = 0.16719606567644235\n",
      "  Batch 15,980  of  46,094.    Elapsed: 4:20:13.\n",
      "  current average loss = 0.16714494032912056\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:20:32.\n",
      "  current average loss = 0.16710462231854034\n",
      "  Batch 16,020  of  46,094.    Elapsed: 4:20:52.\n",
      "  current average loss = 0.16701279897686064\n",
      "  Batch 16,040  of  46,094.    Elapsed: 4:21:11.\n",
      "  current average loss = 0.16690570968349855\n",
      "  Batch 16,060  of  46,094.    Elapsed: 4:21:31.\n",
      "  current average loss = 0.16686714445195852\n",
      "  Batch 16,080  of  46,094.    Elapsed: 4:21:50.\n",
      "  current average loss = 0.1667689269878444\n",
      "  Batch 16,100  of  46,094.    Elapsed: 4:22:10.\n",
      "  current average loss = 0.1666951384520453\n",
      "  Batch 16,120  of  46,094.    Elapsed: 4:22:29.\n",
      "  current average loss = 0.16666663381279442\n",
      "  Batch 16,140  of  46,094.    Elapsed: 4:22:49.\n",
      "  current average loss = 0.1665924847831279\n",
      "  Batch 16,160  of  46,094.    Elapsed: 4:23:08.\n",
      "  current average loss = 0.16650656162172447\n",
      "  Batch 16,180  of  46,094.    Elapsed: 4:23:28.\n",
      "  current average loss = 0.1664919086252063\n",
      "  Batch 16,200  of  46,094.    Elapsed: 4:23:48.\n",
      "  current average loss = 0.1664154085445787\n",
      "  Batch 16,220  of  46,094.    Elapsed: 4:24:07.\n",
      "  current average loss = 0.16633815653910677\n",
      "  Batch 16,240  of  46,094.    Elapsed: 4:24:27.\n",
      "  current average loss = 0.16627981523695584\n",
      "  Batch 16,260  of  46,094.    Elapsed: 4:24:46.\n",
      "  current average loss = 0.16625579072571753\n",
      "  Batch 16,280  of  46,094.    Elapsed: 4:25:06.\n",
      "  current average loss = 0.1661827634848541\n",
      "  Batch 16,300  of  46,094.    Elapsed: 4:25:25.\n",
      "  current average loss = 0.16610890167915301\n",
      "  Batch 16,320  of  46,094.    Elapsed: 4:25:45.\n",
      "  current average loss = 0.16602289281665933\n",
      "  Batch 16,340  of  46,094.    Elapsed: 4:26:04.\n",
      "  current average loss = 0.16597489595883144\n",
      "  Batch 16,360  of  46,094.    Elapsed: 4:26:24.\n",
      "  current average loss = 0.1659333662767661\n",
      "  Batch 16,380  of  46,094.    Elapsed: 4:26:43.\n",
      "  current average loss = 0.16586581025585606\n",
      "  Batch 16,400  of  46,094.    Elapsed: 4:27:03.\n",
      "  current average loss = 0.16580357324878583\n",
      "  Batch 16,420  of  46,094.    Elapsed: 4:27:22.\n",
      "  current average loss = 0.1657174139419297\n",
      "  Batch 16,440  of  46,094.    Elapsed: 4:27:42.\n",
      "  current average loss = 0.16569974252393577\n",
      "  Batch 16,460  of  46,094.    Elapsed: 4:28:01.\n",
      "  current average loss = 0.16563480922089152\n",
      "  Batch 16,480  of  46,094.    Elapsed: 4:28:21.\n",
      "  current average loss = 0.16559063450104694\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:28:40.\n",
      "  current average loss = 0.16553593422835128\n",
      "  Batch 16,520  of  46,094.    Elapsed: 4:29:00.\n",
      "  current average loss = 0.1654592260933841\n",
      "  Batch 16,540  of  46,094.    Elapsed: 4:29:19.\n",
      "  current average loss = 0.16538005447784607\n",
      "  Batch 16,560  of  46,094.    Elapsed: 4:29:39.\n",
      "  current average loss = 0.1653099133122565\n",
      "  Batch 16,580  of  46,094.    Elapsed: 4:29:59.\n",
      "  current average loss = 0.16525756569077352\n",
      "  Batch 16,600  of  46,094.    Elapsed: 4:30:18.\n",
      "  current average loss = 0.16520863427237764\n",
      "  Batch 16,620  of  46,094.    Elapsed: 4:30:38.\n",
      "  current average loss = 0.16512380261367823\n",
      "  Batch 16,640  of  46,094.    Elapsed: 4:30:58.\n",
      "  current average loss = 0.16509269111871758\n",
      "  Batch 16,660  of  46,094.    Elapsed: 4:31:17.\n",
      "  current average loss = 0.16504078312473158\n",
      "  Batch 16,680  of  46,094.    Elapsed: 4:31:37.\n",
      "  current average loss = 0.16498913654603334\n",
      "  Batch 16,700  of  46,094.    Elapsed: 4:31:56.\n",
      "  current average loss = 0.16492695052579637\n",
      "  Batch 16,720  of  46,094.    Elapsed: 4:32:16.\n",
      "  current average loss = 0.16485282316076755\n",
      "  Batch 16,740  of  46,094.    Elapsed: 4:32:35.\n",
      "  current average loss = 0.16478415632014315\n",
      "  Batch 16,760  of  46,094.    Elapsed: 4:32:55.\n",
      "  current average loss = 0.16470592885867466\n",
      "  Batch 16,780  of  46,094.    Elapsed: 4:33:14.\n",
      "  current average loss = 0.16464578422671405\n",
      "  Batch 16,800  of  46,094.    Elapsed: 4:33:34.\n",
      "  current average loss = 0.16454050431556036\n",
      "  Batch 16,820  of  46,094.    Elapsed: 4:33:53.\n",
      "  current average loss = 0.16450313709934566\n",
      "  Batch 16,840  of  46,094.    Elapsed: 4:34:13.\n",
      "  current average loss = 0.1644168124693051\n",
      "  Batch 16,860  of  46,094.    Elapsed: 4:34:32.\n",
      "  current average loss = 0.16432219758136243\n",
      "  Batch 16,880  of  46,094.    Elapsed: 4:34:52.\n",
      "  current average loss = 0.16427308827412693\n",
      "  Batch 16,900  of  46,094.    Elapsed: 4:35:11.\n",
      "  current average loss = 0.16421861689980005\n",
      "  Batch 16,920  of  46,094.    Elapsed: 4:35:31.\n",
      "  current average loss = 0.1641452639096336\n",
      "  Batch 16,940  of  46,094.    Elapsed: 4:35:50.\n",
      "  current average loss = 0.1640904706726461\n",
      "  Batch 16,960  of  46,094.    Elapsed: 4:36:10.\n",
      "  current average loss = 0.16402018993318945\n",
      "  Batch 16,980  of  46,094.    Elapsed: 4:36:30.\n",
      "  current average loss = 0.16393723347016145\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:36:49.\n",
      "  current average loss = 0.1638878960401943\n",
      "  Batch 17,020  of  46,094.    Elapsed: 4:37:09.\n",
      "  current average loss = 0.16381571534146927\n",
      "  Batch 17,040  of  46,094.    Elapsed: 4:37:28.\n",
      "  current average loss = 0.16372249898367655\n",
      "  Batch 17,060  of  46,094.    Elapsed: 4:37:48.\n",
      "  current average loss = 0.16363979703961007\n",
      "  Batch 17,080  of  46,094.    Elapsed: 4:38:07.\n",
      "  current average loss = 0.16354362932667715\n",
      "  Batch 17,100  of  46,094.    Elapsed: 4:38:27.\n",
      "  current average loss = 0.163476970176286\n",
      "  Batch 17,120  of  46,094.    Elapsed: 4:38:46.\n",
      "  current average loss = 0.1634370784729063\n",
      "  Batch 17,140  of  46,094.    Elapsed: 4:39:06.\n",
      "  current average loss = 0.16337037170535418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1cc710c0fe8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Backward 수행으로 그래디언트 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# 그래디언트 클리핑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 10 \n",
    "\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels): \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) \n",
    "\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 20 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=b_token_type_ids, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=b_token_type_ids, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels = b_labels)\n",
    "    \n",
    "        \n",
    "        loss = outputs[0] \n",
    "        logits = outputs[1] \n",
    "        \n",
    "        # 로스 구함 \n",
    "        eval_loss += loss.item() \n",
    "        \n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)            \n",
    "    print(\"  Average validation loss: {}\".format(avg_val_loss))\n",
    "    print(\"  Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    torch.save(model.state_dict(), \"ELECTRA_ZERO_MODEL_\" + str(epoch_i + 1)) \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
