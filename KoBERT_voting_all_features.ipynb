{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.functional as f\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import re \n",
    "import math \n",
    "from tqdm import tqdm \n",
    "import sklearn\n",
    "from transformers import *\n",
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174304, 13), (43576, 12), 46)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\") \n",
    "test = pd.read_csv(\"test.csv\") \n",
    "\n",
    "train.shape, test.shape, train['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
    "    return sent_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(s, overlap = 20, chunk_size = 50): \n",
    "    total = [] \n",
    "    partial = [] \n",
    "    if len(s.split()) // (chunk_size - overlap) > 0:  \n",
    "        n = len(s.split()) // (chunk_size - overlap) \n",
    "    else: \n",
    "        n = 1 \n",
    "    for w in range(n): \n",
    "        if w == 0: \n",
    "            partial = s.split()[:chunk_size] \n",
    "            total.append(\" \".join(partial)) \n",
    "        else:  \n",
    "            partial = s.split()[w*(chunk_size - overlap):w*(chunk_size - overlap) + chunk_size]\n",
    "            total.append(\" \".join(partial)) \n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['요약문_내용'] = train['요약문_연구목표'] + train['요약문_연구내용'] + train['요약문_기대효과'] \n",
    "test['요약문_내용'] = test['요약문_연구목표'] + test['요약문_연구내용'] + test['요약문_기대효과']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['요약문_내용'].fillna('NAN',inplace=True) \n",
    "test['요약문_내용'].fillna('NAN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['사업명'].fillna('NAN',inplace=True) \n",
    "train['사업_부처명'].fillna('NAN',inplace=True) \n",
    "train['내역사업명'].fillna('NAN',inplace=True) \n",
    "train['과제명'].fillna('NAN',inplace=True) \n",
    "train['요약문_한글키워드'].fillna('NAN',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [02:46<00:00, 1047.21it/s]\n"
     ]
    }
   ],
   "source": [
    "contents = train['요약문_내용'].values \n",
    "feature1 = train['사업명'].values \n",
    "feature2 = train['사업_부처명'].values \n",
    "feature3 = train['내역사업명'].values \n",
    "feature4 = train['과제명'].values \n",
    "feature5 = train['요약문_한글키워드'].values \n",
    "feature6 = train['label'].values \n",
    "\n",
    "train_data = {'사업명':[],'사업_부처명':[],'내역사업명':[],'과제명':[],'한글키워드':[],'요약문':[],'label':[]} \n",
    "\n",
    "for i in tqdm(range(contents.shape[0]), position = 0, leave = True): \n",
    "    sample = str(contents[i]) \n",
    "    splitted_text = split_text(clean_text(sample)) \n",
    "    for t in splitted_text: \n",
    "        train_data['요약문'].append(t) \n",
    "        train_data['사업명'].append(clean_text(str(feature1[i])))\n",
    "        train_data['사업_부처명'].append(clean_text(str(feature2[i]))) \n",
    "        train_data['내역사업명'].append(clean_text(str(feature3[i]))) \n",
    "        train_data['과제명'].append(clean_text(str(feature4[i])))  \n",
    "        train_data['한글키워드'].append(feature5[i]) # no cleaning for this one\n",
    "        train_data['label'].append(feature6[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc621df08b74cd2b4aa414113a05d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=371427, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Now we tokenize each data and make sure they all lie within the 512 tokenization range \n",
    "## if not check how many have token length greater than 512 \n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "\n",
    "\n",
    "def bert_tokenizer(sent, MAX_LEN):  \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text = sent, \n",
    "        add_special_tokens = True, # add [CLS] and [SEP]\n",
    "        pad_to_max_length = False, \n",
    "        return_attention_mask = True # constructing attention_masks \n",
    "    )  \n",
    "    \n",
    "    input_id = encoded_dict['input_ids'] \n",
    "    attention_mask = encoded_dict['attention_mask'] # differentiate padding from non padding \n",
    "    token_type_id = encoded_dict['token_type_ids'] # differentiate two sentences, not \"really\" necessary for now    \n",
    "    \n",
    "    if len(input_id) > 512: # head + tail methodology \n",
    "        input_id = input_id[:129] + input_id[-383:] \n",
    "        attention_mask = attention_mask[:129] + attention_mask[-383:]  \n",
    "        token_type_id = token_type_id[:129] + token_type_id[-383:]    \n",
    "        print(\"Long Text!! Using Head+Tail Truncation\")\n",
    "    elif len(input_id) <= 512: \n",
    "        input_id = input_id + [0]*(512 - len(input_id)) \n",
    "        attention_mask = attention_mask + [0]*(512 - len(attention_mask))\n",
    "        token_type_id = token_type_id + [0]*(512 - len(token_type_id))  \n",
    "        \n",
    "    return input_id, attention_mask, token_type_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>사업명</th>\n",
       "      <th>사업_부처명</th>\n",
       "      <th>내역사업명</th>\n",
       "      <th>과제명</th>\n",
       "      <th>한글키워드</th>\n",
       "      <th>요약문</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>농업기초기반연구</td>\n",
       "      <td>농촌진흥청</td>\n",
       "      <td>농산물안전성연구</td>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
       "      <td>뉴클레오티드 염기서열, 분자마커, 종 동정, 침샘, 전사체</td>\n",
       "      <td>새로운 해충분류군의 동정기술 개발 및 유입확산 추적 가 외래 및 돌발해충의 발생조사...</td>\n",
       "      <td>24</td>\n",
       "      <td>농업기초기반연구 농촌진흥청 농산물안전성연구 유전정보를 활용한 새로운 해충 분류군 동...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>농업기초기반연구</td>\n",
       "      <td>농촌진흥청</td>\n",
       "      <td>농산물안전성연구</td>\n",
       "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
       "      <td>뉴클레오티드 염기서열, 분자마커, 종 동정, 침샘, 전사체</td>\n",
       "      <td>의 돌발 및 외래해충 다 외래 및 돌발해충의 유전적 다양성 조사 시험곤충 나 의 해...</td>\n",
       "      <td>24</td>\n",
       "      <td>농업기초기반연구 농촌진흥청 농산물안전성연구 유전정보를 활용한 새로운 해충 분류군 동...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        사업명 사업_부처명     내역사업명                           과제명  \\\n",
       "0  농업기초기반연구  농촌진흥청  농산물안전성연구  유전정보를 활용한 새로운 해충 분류군 동정기술 개발   \n",
       "1  농업기초기반연구  농촌진흥청  농산물안전성연구  유전정보를 활용한 새로운 해충 분류군 동정기술 개발   \n",
       "\n",
       "                              한글키워드  \\\n",
       "0  뉴클레오티드 염기서열, 분자마커, 종 동정, 침샘, 전사체   \n",
       "1  뉴클레오티드 염기서열, 분자마커, 종 동정, 침샘, 전사체   \n",
       "\n",
       "                                                 요약문  label  \\\n",
       "0  새로운 해충분류군의 동정기술 개발 및 유입확산 추적 가 외래 및 돌발해충의 발생조사...     24   \n",
       "1  의 돌발 및 외래해충 다 외래 및 돌발해충의 유전적 다양성 조사 시험곤충 나 의 해...     24   \n",
       "\n",
       "                                                data  \n",
       "0  농업기초기반연구 농촌진흥청 농산물안전성연구 유전정보를 활용한 새로운 해충 분류군 동...  \n",
       "1  농업기초기반연구 농촌진흥청 농산물안전성연구 유전정보를 활용한 새로운 해충 분류군 동...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['data'] = train_data['사업명'] + \" \" + train_data['사업_부처명'] + \" \" + train_data['내역사업명'] + \" \" + train_data['과제명'] + \" \" + train_data['한글키워드'] + \" \" + train_data['요약문'] \n",
    "\n",
    "train_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_data['data'].values \n",
    "train_labels = train_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "VALID_SPLIT = 0.1 \n",
    "MAX_LEN = 512 # max token size for BERT, ELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 80049/1638867 [01:52<22:59, 1130.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 122947/1638867 [02:29<21:42, 1163.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 138519/1638867 [02:42<21:37, 1156.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 173164/1638867 [03:13<21:30, 1135.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 178516/1638867 [03:17<20:53, 1165.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 233938/1638867 [04:07<20:50, 1123.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 251108/1638867 [04:23<34:49, 664.00it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 258672/1638867 [04:30<21:53, 1051.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 475668/1638867 [07:45<18:33, 1044.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 489135/1638867 [07:58<16:34, 1156.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 545212/1638867 [08:48<16:07, 1130.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 569162/1638867 [09:10<16:07, 1106.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 576159/1638867 [09:17<15:55, 1112.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 582171/1638867 [09:23<16:44, 1052.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 624326/1638867 [10:02<14:27, 1169.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 657227/1638867 [10:32<18:01, 907.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 713308/1638867 [11:22<14:37, 1054.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 733749/1638867 [11:41<12:52, 1171.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 743962/1638867 [11:50<13:09, 1133.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 748408/1638867 [11:54<12:55, 1148.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 796909/1638867 [12:39<11:31, 1218.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 805647/1638867 [12:47<15:17, 908.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 854255/1638867 [13:29<10:44, 1217.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 895946/1638867 [14:07<10:31, 1176.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 947992/1638867 [14:54<10:12, 1127.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1068835/1638867 [17:18<10:01, 948.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 1087517/1638867 [17:44<15:02, 610.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1185175/1638867 [20:00<17:22, 435.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1186971/1638867 [20:03<08:53, 846.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1199393/1638867 [20:22<11:09, 656.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1222443/1638867 [20:56<12:19, 562.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1225591/1638867 [21:02<11:05, 620.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1233850/1638867 [21:14<07:52, 857.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1353132/1638867 [24:18<06:57, 684.62it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1373834/1638867 [24:52<07:13, 611.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1410443/1638867 [25:54<06:09, 618.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1422020/1638867 [26:12<04:16, 844.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 1454962/1638867 [27:05<03:55, 779.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n",
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1554766/1638867 [29:46<02:24, 581.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Text!! Using Head+Tail Truncation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1638867/1638867 [32:06<00:00, 850.76it/s] \n"
     ]
    }
   ],
   "source": [
    "N = train_data.shape[0] \n",
    "\n",
    "input_ids = np.zeros((N, MAX_LEN),dtype=int)\n",
    "attention_masks = np.zeros((N, MAX_LEN),dtype=int)\n",
    "token_type_ids = np.zeros((N, MAX_LEN),dtype=int) \n",
    "labels = np.zeros((N),dtype=int)\n",
    "\n",
    "for i in tqdm(range(N), position=0, leave=True): \n",
    "    try:\n",
    "        cur_str = train_text[i]\n",
    "        cur_label = train_labels[i]\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(cur_str, MAX_LEN=MAX_LEN) \n",
    "        input_ids[i,] = input_id \n",
    "        attention_masks[i,] = attention_mask \n",
    "        token_type_ids[i,] = token_type_id\n",
    "        labels[i] = cur_label \n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print(cur_str)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids, dtype=int)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=int)\n",
    "token_type_ids = torch.tensor(token_type_ids, dtype=int) \n",
    "labels = torch.tensor(labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1474980, 512]),\n",
       " torch.Size([1474980, 512]),\n",
       " torch.Size([1474980, 512]),\n",
       " torch.Size([1474980]),\n",
       " torch.Size([163887, 512]),\n",
       " torch.Size([163887, 512]),\n",
       " torch.Size([163887, 512]),\n",
       " torch.Size([163887]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "train_attention_mask, val_attention_mask, _, _ = train_test_split(attention_masks, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "train_token_ids, val_token_ids, _, _ = train_test_split(token_type_ids, labels, random_state = 42, test_size = VALID_SPLIT) \n",
    "\n",
    "\n",
    "train_inputs.shape, train_attention_mask.shape, train_token_ids.shape, train_labels.shape, val_inputs.shape, val_attention_mask.shape, val_token_ids.shape, val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "train_data = TensorDataset(train_inputs, train_attention_mask, train_token_ids, train_labels) \n",
    "train_sampler = RandomSampler(train_data) \n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) \n",
    "\n",
    "validation_data = TensorDataset(val_inputs, val_attention_mask, val_token_ids, val_labels) \n",
    "validation_sampler = SequentialSampler(validation_data) \n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03034f78a8cb444baf577512c4897542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=535, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f4a758fd08450fae74e0a9c04a78f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=368792544, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=46)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:08:03.\n",
      "  current average loss = 1.012136717170477\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:16:06.\n",
      "  current average loss = 0.8487846918702125\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:24:01.\n",
      "  current average loss = 0.7619319985459249\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:31:58.\n",
      "  current average loss = 0.6991074663512409\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:39:59.\n",
      "  current average loss = 0.6520356897130608\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:48:00.\n",
      "  current average loss = 0.6144530848897993\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:55:59.\n",
      "  current average loss = 0.5845445481411048\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:03:58.\n",
      "  current average loss = 0.5583495307192207\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:11:56.\n",
      "  current average loss = 0.536209449806975\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:19:59.\n",
      "  current average loss = 0.5163870525781066\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:28:02.\n",
      "  current average loss = 0.5004110670059242\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:36:03.\n",
      "  current average loss = 0.4858116545348118\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:44:07.\n",
      "  current average loss = 0.4731813156355459\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:52:08.\n",
      "  current average loss = 0.45997604002031894\n",
      "  Batch 7,500  of  46,094.    Elapsed: 2:00:10.\n",
      "  current average loss = 0.44831582299868267\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:08:12.\n",
      "  current average loss = 0.43794616643409245\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:16:13.\n",
      "  current average loss = 0.42878992600421256\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:24:16.\n",
      "  current average loss = 0.4190143201790957\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:32:18.\n",
      "  current average loss = 0.41163197860033496\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:40:21.\n",
      "  current average loss = 0.40381482698377225\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:48:24.\n",
      "  current average loss = 0.3959562867145453\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:56:24.\n",
      "  current average loss = 0.38867932169562713\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:04:27.\n",
      "  current average loss = 0.382135802131956\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:12:29.\n",
      "  current average loss = 0.3757978702803763\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:20:30.\n",
      "  current average loss = 0.369706632527709\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:28:33.\n",
      "  current average loss = 0.36366881534793916\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:36:35.\n",
      "  current average loss = 0.3581848743281293\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:44:37.\n",
      "  current average loss = 0.3527412977157593\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:52:38.\n",
      "  current average loss = 0.3479046945214946\n",
      "  Batch 15,000  of  46,094.    Elapsed: 4:00:37.\n",
      "  current average loss = 0.34299465809093477\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:08:36.\n",
      "  current average loss = 0.3381225415065194\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:16:35.\n",
      "  current average loss = 0.3333228201643069\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:24:34.\n",
      "  current average loss = 0.3288346972889549\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:32:32.\n",
      "  current average loss = 0.3245906847088242\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:40:31.\n",
      "  current average loss = 0.3202030667611398\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:48:25.\n",
      "  current average loss = 0.31620757573857233\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:56:17.\n",
      "  current average loss = 0.31249001020064726\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:04:10.\n",
      "  current average loss = 0.308881274848\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:12:00.\n",
      "  current average loss = 0.3052635778016411\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:19:51.\n",
      "  current average loss = 0.3019341386146378\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:27:42.\n",
      "  current average loss = 0.2983452647543944\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:35:34.\n",
      "  current average loss = 0.29485959945727763\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:43:27.\n",
      "  current average loss = 0.2917162385417668\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:51:32.\n",
      "  current average loss = 0.28839112239534204\n",
      "  Batch 22,500  of  46,094.    Elapsed: 5:59:37.\n",
      "  current average loss = 0.28546595369486344\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:07:41.\n",
      "  current average loss = 0.2823952177537544\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:15:47.\n",
      "  current average loss = 0.27941949966711865\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:23:52.\n",
      "  current average loss = 0.27662400013564914\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:31:57.\n",
      "  current average loss = 0.2738732089604375\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:40:02.\n",
      "  current average loss = 0.27101322859410665\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:48:07.\n",
      "  current average loss = 0.26829286945381603\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:56:11.\n",
      "  current average loss = 0.26576955283473946\n",
      "  Batch 26,500  of  46,094.    Elapsed: 7:04:16.\n",
      "  current average loss = 0.26312582688240493\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:12:20.\n",
      "  current average loss = 0.2605340751719885\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:20:26.\n",
      "  current average loss = 0.2581586246134036\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:28:31.\n",
      "  current average loss = 0.255735528070093\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:36:37.\n",
      "  current average loss = 0.25334238238754814\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:44:42.\n",
      "  current average loss = 0.25103254721111395\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:52:47.\n",
      "  current average loss = 0.24887517951820362\n",
      "  Batch 30,000  of  46,094.    Elapsed: 8:00:53.\n",
      "  current average loss = 0.24676760771390593\n",
      "  Batch 30,500  of  46,094.    Elapsed: 8:08:57.\n",
      "  current average loss = 0.24466186454605127\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:17:03.\n",
      "  current average loss = 0.24243302461859534\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:25:07.\n",
      "  current average loss = 0.240244235642368\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:33:11.\n",
      "  current average loss = 0.23815877797979101\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:41:16.\n",
      "  current average loss = 0.2364353265423104\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:49:21.\n",
      "  current average loss = 0.23460038181126142\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:57:25.\n",
      "  current average loss = 0.2325801329176496\n",
      "  Batch 34,000  of  46,094.    Elapsed: 9:05:31.\n",
      "  current average loss = 0.23063942053421616\n",
      "  Batch 34,500  of  46,094.    Elapsed: 9:13:36.\n",
      "  current average loss = 0.2287901363862616\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:21:42.\n",
      "  current average loss = 0.22703539579341636\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:29:46.\n",
      "  current average loss = 0.2253008205118568\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:37:50.\n",
      "  current average loss = 0.22348192410400386\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:45:56.\n",
      "  current average loss = 0.2217739331451897\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:54:00.\n",
      "  current average loss = 0.22016926901047731\n",
      "  Batch 37,500  of  46,094.    Elapsed: 10:02:05.\n",
      "  current average loss = 0.21858089255345753\n",
      "  Batch 38,000  of  46,094.    Elapsed: 10:10:09.\n",
      "  current average loss = 0.21695726784528002\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:18:14.\n",
      "  current average loss = 0.21541917503737562\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:26:18.\n",
      "  current average loss = 0.2139470992422542\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:34:24.\n",
      "  current average loss = 0.21237006094882815\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:42:29.\n",
      "  current average loss = 0.2108120096277642\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:50:34.\n",
      "  current average loss = 0.2093501852445691\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:58:39.\n",
      "  current average loss = 0.20789773228598502\n",
      "  Batch 41,500  of  46,094.    Elapsed: 11:06:43.\n",
      "  current average loss = 0.2064045476245698\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:14:48.\n",
      "  current average loss = 0.2049654135116261\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:22:54.\n",
      "  current average loss = 0.20353512877150207\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:30:57.\n",
      "  current average loss = 0.20220805834929884\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:39:03.\n",
      "  current average loss = 0.20083316599930323\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:47:07.\n",
      "  current average loss = 0.1996433671489788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 44,500  of  46,094.    Elapsed: 11:55:10.\n",
      "  current average loss = 0.19834581152815373\n",
      "  Batch 45,000  of  46,094.    Elapsed: 12:03:14.\n",
      "  current average loss = 0.19703827457262021\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:11:18.\n",
      "  current average loss = 0.1958123529157064\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:19:22.\n",
      "  current average loss = 0.19453318259684285\n",
      "\n",
      "  Average training loss: 0.19427929408840847\n",
      "  Training epoch took: 12:20:52\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.06462574088643923\n",
      "  Accuracy: 0.9828192112456072\n",
      "  Validation took: 0:27:01\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:08:04.\n",
      "  current average loss = 0.0735813626376912\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:16:08.\n",
      "  current average loss = 0.07348795195561252\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:24:11.\n",
      "  current average loss = 0.07189535331759059\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:32:13.\n",
      "  current average loss = 0.07123592855608149\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:40:16.\n",
      "  current average loss = 0.07088015395549592\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:48:18.\n",
      "  current average loss = 0.06991504550880442\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:56:20.\n",
      "  current average loss = 0.06835703091432307\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:04:20.\n",
      "  current average loss = 0.06932758530267165\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:12:15.\n",
      "  current average loss = 0.0693370141519958\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:20:10.\n",
      "  current average loss = 0.06962934572037775\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:28:13.\n",
      "  current average loss = 0.06964096649727171\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:36:16.\n",
      "  current average loss = 0.06893141796257017\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:44:18.\n",
      "  current average loss = 0.06925871552231211\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:52:19.\n",
      "  current average loss = 0.06907714282123198\n",
      "  Batch 7,500  of  46,094.    Elapsed: 2:00:17.\n",
      "  current average loss = 0.06889716236796813\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:08:18.\n",
      "  current average loss = 0.06864627872448545\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:16:19.\n",
      "  current average loss = 0.06830495134813999\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:24:22.\n",
      "  current average loss = 0.06854027900963491\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:32:22.\n",
      "  current average loss = 0.06860581587745478\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:40:24.\n",
      "  current average loss = 0.0686116048077005\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:48:26.\n",
      "  current average loss = 0.06823173579820044\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:56:27.\n",
      "  current average loss = 0.06798532398378566\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:04:28.\n",
      "  current average loss = 0.06780395771890155\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:12:30.\n",
      "  current average loss = 0.06727861056112791\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:20:27.\n",
      "  current average loss = 0.06713790141385748\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:28:25.\n",
      "  current average loss = 0.06653216187185339\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:36:25.\n",
      "  current average loss = 0.06621692863853079\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:44:26.\n",
      "  current average loss = 0.06617345667616921\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:52:26.\n",
      "  current average loss = 0.06590219002124123\n",
      "  Batch 15,000  of  46,094.    Elapsed: 4:00:25.\n",
      "  current average loss = 0.06577603134263967\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:08:25.\n",
      "  current average loss = 0.06562609730427334\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:16:26.\n",
      "  current average loss = 0.06549416606250816\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:24:26.\n",
      "  current average loss = 0.0654822902995458\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:32:27.\n",
      "  current average loss = 0.06537281878990282\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:40:27.\n",
      "  current average loss = 0.06544328925809408\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:48:27.\n",
      "  current average loss = 0.06534648497310101\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:56:26.\n",
      "  current average loss = 0.06520975878143863\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:04:26.\n",
      "  current average loss = 0.06496987951539325\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:12:27.\n",
      "  current average loss = 0.06478387046626766\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:20:27.\n",
      "  current average loss = 0.06453014436219964\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:28:26.\n",
      "  current average loss = 0.06436636344462253\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:36:26.\n",
      "  current average loss = 0.06420729748205936\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:44:25.\n",
      "  current average loss = 0.06413765035220625\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:52:24.\n",
      "  current average loss = 0.0639600062477925\n",
      "  Batch 22,500  of  46,094.    Elapsed: 6:00:22.\n",
      "  current average loss = 0.06363239714445243\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:08:20.\n",
      "  current average loss = 0.06352201658534867\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:24:17.\n",
      "  current average loss = 0.06307740201514238\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:32:16.\n",
      "  current average loss = 0.06292462566138479\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:40:16.\n",
      "  current average loss = 0.06280906739375641\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:48:16.\n",
      "  current average loss = 0.06253482940348176\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:56:16.\n",
      "  current average loss = 0.06249451639747853\n",
      "  Batch 26,500  of  46,094.    Elapsed: 7:04:16.\n",
      "  current average loss = 0.06221628010312215\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:12:16.\n",
      "  current average loss = 0.06196314102344681\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:20:16.\n",
      "  current average loss = 0.06181309472082992\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:28:14.\n",
      "  current average loss = 0.06166901632444803\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:36:11.\n",
      "  current average loss = 0.06140046146885863\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:44:10.\n",
      "  current average loss = 0.06123612078801796\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:52:10.\n",
      "  current average loss = 0.06108811879610634\n",
      "  Batch 30,000  of  46,094.    Elapsed: 8:00:09.\n",
      "  current average loss = 0.06088178657183089\n",
      "  Batch 30,500  of  46,094.    Elapsed: 8:08:10.\n",
      "  current average loss = 0.06078521571656499\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:16:09.\n",
      "  current average loss = 0.06048578235105261\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:24:08.\n",
      "  current average loss = 0.06019306386931012\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:32:08.\n",
      "  current average loss = 0.06012820617773878\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:40:08.\n",
      "  current average loss = 0.06003795637933743\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:48:07.\n",
      "  current average loss = 0.05981188999153991\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:56:07.\n",
      "  current average loss = 0.059615615044840364\n",
      "  Batch 34,000  of  46,094.    Elapsed: 9:04:09.\n",
      "  current average loss = 0.05949784030225119\n",
      "  Batch 34,500  of  46,094.    Elapsed: 9:12:09.\n",
      "  current average loss = 0.05938313486264593\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:20:10.\n",
      "  current average loss = 0.059254685771606246\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:28:10.\n",
      "  current average loss = 0.05902133366932501\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:36:09.\n",
      "  current average loss = 0.05883571220650184\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:44:10.\n",
      "  current average loss = 0.05872356436455069\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:52:09.\n",
      "  current average loss = 0.058440507084909575\n",
      "  Batch 37,500  of  46,094.    Elapsed: 10:00:09.\n",
      "  current average loss = 0.05831431050110104\n",
      "  Batch 38,000  of  46,094.    Elapsed: 10:08:08.\n",
      "  current average loss = 0.058149233641015596\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:16:08.\n",
      "  current average loss = 0.058001790039674095\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:24:08.\n",
      "  current average loss = 0.057816631086037094\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:32:08.\n",
      "  current average loss = 0.05763272425234966\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:40:08.\n",
      "  current average loss = 0.05748799138003997\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:48:08.\n",
      "  current average loss = 0.05733878132835696\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:56:09.\n",
      "  current average loss = 0.057175707662446854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 41,500  of  46,094.    Elapsed: 11:04:08.\n",
      "  current average loss = 0.05708305598017185\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:12:08.\n",
      "  current average loss = 0.0568367514252251\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:20:08.\n",
      "  current average loss = 0.056669108492812065\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:28:09.\n",
      "  current average loss = 0.05659289677012397\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:36:08.\n",
      "  current average loss = 0.056490203324088994\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:44:07.\n",
      "  current average loss = 0.056349227775597685\n",
      "  Batch 44,500  of  46,094.    Elapsed: 11:52:07.\n",
      "  current average loss = 0.05622186168383747\n",
      "  Batch 45,000  of  46,094.    Elapsed: 12:00:08.\n",
      "  current average loss = 0.05605352304925165\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:08:07.\n",
      "  current average loss = 0.055918187949147616\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:16:09.\n",
      "  current average loss = 0.055825726943660295\n",
      "\n",
      "  Average training loss: 0.055811768940015896\n",
      "  Training epoch took: 12:17:38\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.036775721791844525\n",
      "  Accuracy: 0.9902991995314331\n",
      "  Validation took: 0:26:59\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:08:01.\n",
      "  current average loss = 0.031691232723387655\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:15:59.\n",
      "  current average loss = 0.03557972573077132\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:24:01.\n",
      "  current average loss = 0.034652553815874856\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:32:01.\n",
      "  current average loss = 0.03426146380314822\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:40:01.\n",
      "  current average loss = 0.03567165320390486\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:48:01.\n",
      "  current average loss = 0.03590481377621715\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:56:02.\n",
      "  current average loss = 0.03560435187646986\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:04:03.\n",
      "  current average loss = 0.03553008896474603\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:12:03.\n",
      "  current average loss = 0.035912933944275006\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:20:02.\n",
      "  current average loss = 0.0363588727761351\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:28:02.\n",
      "  current average loss = 0.03632105163164802\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:36:02.\n",
      "  current average loss = 0.03619421924918424\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:44:02.\n",
      "  current average loss = 0.036192776609558046\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:52:01.\n",
      "  current average loss = 0.036371453443069184\n",
      "  Batch 7,500  of  46,094.    Elapsed: 2:00:00.\n",
      "  current average loss = 0.036079635190728\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:08:00.\n",
      "  current average loss = 0.0361244224480306\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:16:00.\n",
      "  current average loss = 0.03635255197636434\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:24:03.\n",
      "  current average loss = 0.03639431775264691\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:32:04.\n",
      "  current average loss = 0.0362361205337756\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:40:03.\n",
      "  current average loss = 0.03645325370912324\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:48:02.\n",
      "  current average loss = 0.036371958729909414\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:56:02.\n",
      "  current average loss = 0.036333967292357223\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:04:01.\n",
      "  current average loss = 0.03619184809078781\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:12:00.\n",
      "  current average loss = 0.036334109724502926\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:20:00.\n",
      "  current average loss = 0.03632596308478329\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:28:00.\n",
      "  current average loss = 0.036336935328390746\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:36:00.\n",
      "  current average loss = 0.03619400540740645\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:43:59.\n",
      "  current average loss = 0.0362661031313832\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:51:59.\n",
      "  current average loss = 0.036191155065273053\n",
      "  Batch 15,000  of  46,094.    Elapsed: 3:59:59.\n",
      "  current average loss = 0.036049891700670314\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:07:59.\n",
      "  current average loss = 0.03593456634707775\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:15:58.\n",
      "  current average loss = 0.03570418033900569\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:23:58.\n",
      "  current average loss = 0.035622340021727726\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:31:58.\n",
      "  current average loss = 0.035531047990365015\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:39:58.\n",
      "  current average loss = 0.03561191372301359\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:47:57.\n",
      "  current average loss = 0.03557658152986369\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:55:57.\n",
      "  current average loss = 0.0355565406633189\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:03:56.\n",
      "  current average loss = 0.03546606426914667\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:11:55.\n",
      "  current average loss = 0.03536771884583682\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:19:55.\n",
      "  current average loss = 0.035403111992087\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:27:55.\n",
      "  current average loss = 0.035278727355068505\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:35:55.\n",
      "  current average loss = 0.03528627009298725\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:43:56.\n",
      "  current average loss = 0.035213932756177346\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:51:56.\n",
      "  current average loss = 0.035227293301199926\n",
      "  Batch 22,500  of  46,094.    Elapsed: 5:59:56.\n",
      "  current average loss = 0.03513823466636224\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:07:56.\n",
      "  current average loss = 0.03498078375305957\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:15:56.\n",
      "  current average loss = 0.03501716969570221\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:23:56.\n",
      "  current average loss = 0.03502095348454274\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:31:57.\n",
      "  current average loss = 0.03504295570930132\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:39:56.\n",
      "  current average loss = 0.03493447920815102\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:47:54.\n",
      "  current average loss = 0.03490928326104354\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:55:53.\n",
      "  current average loss = 0.034902826449214656\n",
      "  Batch 26,500  of  46,094.    Elapsed: 7:03:52.\n",
      "  current average loss = 0.03486076328940956\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:11:52.\n",
      "  current average loss = 0.03488346152227489\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:19:52.\n",
      "  current average loss = 0.03490677227416864\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:27:52.\n",
      "  current average loss = 0.03490085996051805\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:35:52.\n",
      "  current average loss = 0.034920526924038874\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:43:52.\n",
      "  current average loss = 0.034865710463466176\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:51:53.\n",
      "  current average loss = 0.034825164023611174\n",
      "  Batch 30,000  of  46,094.    Elapsed: 7:59:51.\n",
      "  current average loss = 0.03473035069526474\n",
      "  Batch 30,500  of  46,094.    Elapsed: 8:07:51.\n",
      "  current average loss = 0.034620421811964765\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:15:50.\n",
      "  current average loss = 0.0345551397285749\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:23:49.\n",
      "  current average loss = 0.03450813713653169\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:31:48.\n",
      "  current average loss = 0.03449436256052002\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:39:45.\n",
      "  current average loss = 0.034422305840053465\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:47:43.\n",
      "  current average loss = 0.03438608832542311\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:55:39.\n",
      "  current average loss = 0.03434404731849656\n",
      "  Batch 34,000  of  46,094.    Elapsed: 9:03:36.\n",
      "  current average loss = 0.034284749260389456\n",
      "  Batch 34,500  of  46,094.    Elapsed: 9:11:32.\n",
      "  current average loss = 0.03425175923861283\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:19:29.\n",
      "  current average loss = 0.03409651526858714\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:27:25.\n",
      "  current average loss = 0.034098611438237204\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:35:22.\n",
      "  current average loss = 0.03398175380363581\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:43:18.\n",
      "  current average loss = 0.03398620766760783\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:51:15.\n",
      "  current average loss = 0.03395363596116049\n",
      "  Batch 37,500  of  46,094.    Elapsed: 9:59:12.\n",
      "  current average loss = 0.03383961793481333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 38,000  of  46,094.    Elapsed: 10:07:08.\n",
      "  current average loss = 0.03372720736918717\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:15:04.\n",
      "  current average loss = 0.033669133375709884\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:23:00.\n",
      "  current average loss = 0.033653126131244604\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:30:56.\n",
      "  current average loss = 0.03358774067340756\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:38:53.\n",
      "  current average loss = 0.0335658948686314\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:46:49.\n",
      "  current average loss = 0.03349182855560898\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:54:46.\n",
      "  current average loss = 0.03344140454306336\n",
      "  Batch 41,500  of  46,094.    Elapsed: 11:02:43.\n",
      "  current average loss = 0.0334113194935791\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:10:40.\n",
      "  current average loss = 0.033370883509014244\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:18:37.\n",
      "  current average loss = 0.033306171617194284\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:26:33.\n",
      "  current average loss = 0.03325948407302103\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:34:30.\n",
      "  current average loss = 0.033221743205518246\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:42:26.\n",
      "  current average loss = 0.03314463060536623\n",
      "  Batch 44,500  of  46,094.    Elapsed: 11:50:23.\n",
      "  current average loss = 0.033098441743998334\n",
      "  Batch 45,000  of  46,094.    Elapsed: 11:58:20.\n",
      "  current average loss = 0.03309673679653828\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:06:17.\n",
      "  current average loss = 0.03300700484216157\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:14:14.\n",
      "  current average loss = 0.03287413954538858\n",
      "\n",
      "  Average training loss: 0.03287472293616099\n",
      "  Training epoch took: 12:15:42\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.021283598756308038\n",
      "  Accuracy: 0.9940696993361968\n",
      "  Validation took: 0:26:58\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:07:57.\n",
      "  current average loss = 0.020928556329781713\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:15:53.\n",
      "  current average loss = 0.0237437164169678\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:23:46.\n",
      "  current average loss = 0.022641331459800615\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:31:39.\n",
      "  current average loss = 0.022838910043112264\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:39:32.\n",
      "  current average loss = 0.023238135761910233\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:47:26.\n",
      "  current average loss = 0.023090351176272332\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:55:18.\n",
      "  current average loss = 0.022998078201043038\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:03:12.\n",
      "  current average loss = 0.022964963959762826\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:11:06.\n",
      "  current average loss = 0.023222091807610316\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:19:00.\n",
      "  current average loss = 0.023755368303804425\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:26:55.\n",
      "  current average loss = 0.023827422203188522\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:34:48.\n",
      "  current average loss = 0.02414966037025988\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:42:42.\n",
      "  current average loss = 0.02406041394631477\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:50:36.\n",
      "  current average loss = 0.02426268557939433\n",
      "  Batch 7,500  of  46,094.    Elapsed: 1:58:29.\n",
      "  current average loss = 0.02434679546327049\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:06:23.\n",
      "  current average loss = 0.024400445868264342\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:14:16.\n",
      "  current average loss = 0.024367267481773845\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:22:09.\n",
      "  current average loss = 0.024425999832952787\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:30:03.\n",
      "  current average loss = 0.024647472441270732\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:37:56.\n",
      "  current average loss = 0.024448545981147617\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:45:51.\n",
      "  current average loss = 0.024332443741200093\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:53:45.\n",
      "  current average loss = 0.024626451393436253\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:01:39.\n",
      "  current average loss = 0.024740559603237552\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:09:32.\n",
      "  current average loss = 0.024614855997609993\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:17:26.\n",
      "  current average loss = 0.02446003756193066\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:25:20.\n",
      "  current average loss = 0.024411357655778444\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:33:12.\n",
      "  current average loss = 0.02440405918964367\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:41:07.\n",
      "  current average loss = 0.02441577519817825\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:49:03.\n",
      "  current average loss = 0.024371792191521746\n",
      "  Batch 15,000  of  46,094.    Elapsed: 3:56:57.\n",
      "  current average loss = 0.024281284755367356\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:04:53.\n",
      "  current average loss = 0.024341958743547894\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:12:47.\n",
      "  current average loss = 0.024230540431402346\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:20:43.\n",
      "  current average loss = 0.024242837224086104\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:28:39.\n",
      "  current average loss = 0.024245278833460844\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:36:37.\n",
      "  current average loss = 0.024186236906337998\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:44:34.\n",
      "  current average loss = 0.024221377122573357\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:52:31.\n",
      "  current average loss = 0.024219774588511576\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:00:28.\n",
      "  current average loss = 0.024193434612744424\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:08:25.\n",
      "  current average loss = 0.02413407295784549\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:16:22.\n",
      "  current average loss = 0.02405973983515032\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:24:20.\n",
      "  current average loss = 0.02404382893593427\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:32:16.\n",
      "  current average loss = 0.024036701840498133\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:40:13.\n",
      "  current average loss = 0.023970813956545944\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:48:10.\n",
      "  current average loss = 0.024019922765627266\n",
      "  Batch 22,500  of  46,094.    Elapsed: 5:56:07.\n",
      "  current average loss = 0.023995430031962153\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:04:03.\n",
      "  current average loss = 0.02397738557806406\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:12:00.\n",
      "  current average loss = 0.02397525818092633\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:19:57.\n",
      "  current average loss = 0.02389145367561855\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:27:54.\n",
      "  current average loss = 0.023889254931350013\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:35:50.\n",
      "  current average loss = 0.02388765870479743\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:43:47.\n",
      "  current average loss = 0.023894119954214664\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:51:44.\n",
      "  current average loss = 0.02385664342864815\n",
      "  Batch 26,500  of  46,094.    Elapsed: 6:59:42.\n",
      "  current average loss = 0.0237989764671565\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:07:38.\n",
      "  current average loss = 0.023779193648504924\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:15:35.\n",
      "  current average loss = 0.023771506995381192\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:23:32.\n",
      "  current average loss = 0.023731663125920868\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:31:29.\n",
      "  current average loss = 0.023673903044549915\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:39:26.\n",
      "  current average loss = 0.023632373723882008\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:47:22.\n",
      "  current average loss = 0.023513214462321307\n",
      "  Batch 30,000  of  46,094.    Elapsed: 7:55:18.\n",
      "  current average loss = 0.02353377444803494\n",
      "  Batch 30,500  of  46,094.    Elapsed: 8:03:16.\n",
      "  current average loss = 0.023472192889616442\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:11:12.\n",
      "  current average loss = 0.02353072646965196\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:19:09.\n",
      "  current average loss = 0.023504584389949394\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:27:06.\n",
      "  current average loss = 0.023495638523355636\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:35:03.\n",
      "  current average loss = 0.02349133308016742\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:43:00.\n",
      "  current average loss = 0.02349706599255296\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:50:57.\n",
      "  current average loss = 0.02345150612296281\n",
      "  Batch 34,000  of  46,094.    Elapsed: 8:58:53.\n",
      "  current average loss = 0.023490471020546893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 34,500  of  46,094.    Elapsed: 9:06:49.\n",
      "  current average loss = 0.023453376737286792\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:14:45.\n",
      "  current average loss = 0.02341307201980027\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:22:40.\n",
      "  current average loss = 0.023384578650245548\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:30:36.\n",
      "  current average loss = 0.023348301164953327\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:38:32.\n",
      "  current average loss = 0.02331298953354028\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:46:28.\n",
      "  current average loss = 0.023272247537511812\n",
      "  Batch 37,500  of  46,094.    Elapsed: 9:54:24.\n",
      "  current average loss = 0.023213474455382334\n",
      "  Batch 38,000  of  46,094.    Elapsed: 10:02:19.\n",
      "  current average loss = 0.02321040679187215\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:10:16.\n",
      "  current average loss = 0.023141091609283966\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:18:11.\n",
      "  current average loss = 0.023136299335012107\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:26:08.\n",
      "  current average loss = 0.023071536900672672\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:34:03.\n",
      "  current average loss = 0.02300670786041578\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:42:00.\n",
      "  current average loss = 0.023021178987005387\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:49:55.\n",
      "  current average loss = 0.0229663975253844\n",
      "  Batch 41,500  of  46,094.    Elapsed: 10:57:51.\n",
      "  current average loss = 0.02295837335216839\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:05:47.\n",
      "  current average loss = 0.022898953667185414\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:13:42.\n",
      "  current average loss = 0.022860170683642784\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:21:38.\n",
      "  current average loss = 0.022827013846719087\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:29:34.\n",
      "  current average loss = 0.022835971207744866\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:37:30.\n",
      "  current average loss = 0.022792433829146044\n",
      "  Batch 44,500  of  46,094.    Elapsed: 11:45:26.\n",
      "  current average loss = 0.022756170432501062\n",
      "  Batch 45,000  of  46,094.    Elapsed: 11:53:21.\n",
      "  current average loss = 0.022734415561842938\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:01:17.\n",
      "  current average loss = 0.022687742821583673\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:09:13.\n",
      "  current average loss = 0.022675047582309045\n",
      "\n",
      "  Average training loss: 0.022645247202969634\n",
      "  Training epoch took: 12:10:42\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.017001590170147993\n",
      "  Accuracy: 0.9948262397500977\n",
      "  Validation took: 0:26:58\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:07:57.\n",
      "  current average loss = 0.017012212977286254\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:15:53.\n",
      "  current average loss = 0.017764428418129682\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:23:49.\n",
      "  current average loss = 0.018360905344804147\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:31:44.\n",
      "  current average loss = 0.01850028997112531\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:39:41.\n",
      "  current average loss = 0.01851187951992106\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:47:36.\n",
      "  current average loss = 0.019507843399519213\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:55:32.\n",
      "  current average loss = 0.01935444831035784\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:03:28.\n",
      "  current average loss = 0.019178089995892607\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:11:25.\n",
      "  current average loss = 0.018881251643019317\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:19:20.\n",
      "  current average loss = 0.018798626020634764\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:27:16.\n",
      "  current average loss = 0.018538597045653162\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:35:12.\n",
      "  current average loss = 0.01838767579880702\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:43:07.\n",
      "  current average loss = 0.018235275164975834\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:51:03.\n",
      "  current average loss = 0.018279114120866715\n",
      "  Batch 7,500  of  46,094.    Elapsed: 1:58:58.\n",
      "  current average loss = 0.018236932972694436\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:06:54.\n",
      "  current average loss = 0.018167554014160033\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:14:50.\n",
      "  current average loss = 0.01819208217433954\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:22:46.\n",
      "  current average loss = 0.018139675418525562\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:30:42.\n",
      "  current average loss = 0.017941275977924977\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:38:38.\n",
      "  current average loss = 0.0180280212737327\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:46:33.\n",
      "  current average loss = 0.018176634212314337\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:54:29.\n",
      "  current average loss = 0.018246913629973775\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:02:25.\n",
      "  current average loss = 0.018005868792794068\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:10:22.\n",
      "  current average loss = 0.017989558292341373\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:18:18.\n",
      "  current average loss = 0.017912765460963275\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:26:14.\n",
      "  current average loss = 0.017984295021235365\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:34:09.\n",
      "  current average loss = 0.018104138027884565\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:42:06.\n",
      "  current average loss = 0.018122630675885504\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:50:03.\n",
      "  current average loss = 0.01802530536630378\n",
      "  Batch 15,000  of  46,094.    Elapsed: 3:57:58.\n",
      "  current average loss = 0.01801064892401567\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:05:54.\n",
      "  current average loss = 0.01802620646285164\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:13:50.\n",
      "  current average loss = 0.018116367411331\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:21:46.\n",
      "  current average loss = 0.01800147216947197\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:29:41.\n",
      "  current average loss = 0.017876249514403003\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:37:36.\n",
      "  current average loss = 0.01783953002294823\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:45:32.\n",
      "  current average loss = 0.0178211989971762\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:53:27.\n",
      "  current average loss = 0.017829991642379528\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:01:23.\n",
      "  current average loss = 0.01781578434299598\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:09:19.\n",
      "  current average loss = 0.017771882783770055\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:17:15.\n",
      "  current average loss = 0.01781394613988041\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:25:11.\n",
      "  current average loss = 0.017811248915229953\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:33:07.\n",
      "  current average loss = 0.017820969565538948\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:41:03.\n",
      "  current average loss = 0.01785736226274398\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:48:58.\n",
      "  current average loss = 0.017835321673096503\n",
      "  Batch 22,500  of  46,094.    Elapsed: 5:56:53.\n",
      "  current average loss = 0.01779568760672458\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:04:50.\n",
      "  current average loss = 0.01774954109811496\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:12:46.\n",
      "  current average loss = 0.01771807693180768\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:20:42.\n",
      "  current average loss = 0.017761376229737835\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:28:38.\n",
      "  current average loss = 0.017797508955080383\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:36:33.\n",
      "  current average loss = 0.01773318316169738\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:44:29.\n",
      "  current average loss = 0.017672331342830576\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:52:24.\n",
      "  current average loss = 0.01759606886643143\n",
      "  Batch 26,500  of  46,094.    Elapsed: 7:00:19.\n",
      "  current average loss = 0.01759752966731719\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:08:15.\n",
      "  current average loss = 0.017610858081720464\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:16:10.\n",
      "  current average loss = 0.01758737856081425\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:24:07.\n",
      "  current average loss = 0.017541617763446215\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:32:02.\n",
      "  current average loss = 0.017518946906137884\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:39:58.\n",
      "  current average loss = 0.01749601792977297\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:47:54.\n",
      "  current average loss = 0.01746028258655688\n",
      "  Batch 30,000  of  46,094.    Elapsed: 7:55:50.\n",
      "  current average loss = 0.01748344171458557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 30,500  of  46,094.    Elapsed: 8:03:46.\n",
      "  current average loss = 0.0174807159249714\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:11:42.\n",
      "  current average loss = 0.01744277164569159\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:19:37.\n",
      "  current average loss = 0.017451268908946365\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:27:33.\n",
      "  current average loss = 0.017404676073835845\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:35:29.\n",
      "  current average loss = 0.017354537796498894\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:43:25.\n",
      "  current average loss = 0.017363794091607567\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:51:20.\n",
      "  current average loss = 0.017302216591050624\n",
      "  Batch 34,000  of  46,094.    Elapsed: 8:59:16.\n",
      "  current average loss = 0.017290502672276917\n",
      "  Batch 34,500  of  46,094.    Elapsed: 9:07:13.\n",
      "  current average loss = 0.0172806361475777\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:15:08.\n",
      "  current average loss = 0.01725040773587295\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:23:04.\n",
      "  current average loss = 0.01724014282750247\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:30:59.\n",
      "  current average loss = 0.017212541408058415\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:38:56.\n",
      "  current average loss = 0.017186689297063434\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:46:51.\n",
      "  current average loss = 0.017174014519693505\n",
      "  Batch 37,500  of  46,094.    Elapsed: 9:54:46.\n",
      "  current average loss = 0.017141989584265684\n",
      "  Batch 38,000  of  46,094.    Elapsed: 10:02:42.\n",
      "  current average loss = 0.017110127513457644\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:10:37.\n",
      "  current average loss = 0.017091795304029565\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:18:33.\n",
      "  current average loss = 0.017052062941998278\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:26:29.\n",
      "  current average loss = 0.017031565461740915\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:34:26.\n",
      "  current average loss = 0.016977593576809523\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:42:21.\n",
      "  current average loss = 0.016955987440698348\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:50:17.\n",
      "  current average loss = 0.0170054495671231\n",
      "  Batch 41,500  of  46,094.    Elapsed: 10:58:13.\n",
      "  current average loss = 0.01696383669392587\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:06:09.\n",
      "  current average loss = 0.016948648331061987\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:14:06.\n",
      "  current average loss = 0.016933244719408942\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:22:01.\n",
      "  current average loss = 0.016910324709858815\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:29:56.\n",
      "  current average loss = 0.016900469861671454\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:37:53.\n",
      "  current average loss = 0.016866024637562906\n",
      "  Batch 44,500  of  46,094.    Elapsed: 11:45:48.\n",
      "  current average loss = 0.01683611680979329\n",
      "  Batch 45,000  of  46,094.    Elapsed: 11:53:44.\n",
      "  current average loss = 0.016810274491472512\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:01:40.\n",
      "  current average loss = 0.016824958052682953\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:09:36.\n",
      "  current average loss = 0.016799093607541657\n",
      "\n",
      "  Average training loss: 0.01682530325353098\n",
      "  Training epoch took: 12:11:04\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.013024827441688234\n",
      "  Accuracy: 0.9954363529871144\n",
      "  Validation took: 0:26:57\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:07:57.\n",
      "  current average loss = 0.012149890563137887\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:15:53.\n",
      "  current average loss = 0.012104384575022777\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:23:48.\n",
      "  current average loss = 0.012757959153709331\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:31:43.\n",
      "  current average loss = 0.012663038757109462\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:39:38.\n",
      "  current average loss = 0.012970411481358315\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:47:34.\n",
      "  current average loss = 0.013214767874872147\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:55:29.\n",
      "  current average loss = 0.013053712394221033\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:03:24.\n",
      "  current average loss = 0.013186503426385115\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:11:19.\n",
      "  current average loss = 0.01362049584058372\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:19:14.\n",
      "  current average loss = 0.013540658932236693\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:27:09.\n",
      "  current average loss = 0.013721214681581015\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:35:04.\n",
      "  current average loss = 0.013819763297537065\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:42:59.\n",
      "  current average loss = 0.013747577988994286\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:50:54.\n",
      "  current average loss = 0.01375938931084212\n",
      "  Batch 7,500  of  46,094.    Elapsed: 1:58:49.\n",
      "  current average loss = 0.013585462481329401\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:06:44.\n",
      "  current average loss = 0.01364705175957056\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:14:39.\n",
      "  current average loss = 0.013622249313026474\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:22:34.\n",
      "  current average loss = 0.013548106272300882\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:30:29.\n",
      "  current average loss = 0.01378223203804507\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:38:24.\n",
      "  current average loss = 0.013722120505542806\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:46:19.\n",
      "  current average loss = 0.013731598590746996\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:54:13.\n",
      "  current average loss = 0.013792026683090378\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:02:06.\n",
      "  current average loss = 0.013784885791681597\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:09:59.\n",
      "  current average loss = 0.013910966386909725\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:17:53.\n",
      "  current average loss = 0.013740003842269944\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:25:47.\n",
      "  current average loss = 0.013737108452040467\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:33:41.\n",
      "  current average loss = 0.013708882114493733\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:41:34.\n",
      "  current average loss = 0.013635917888343231\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:49:28.\n",
      "  current average loss = 0.013653480692247505\n",
      "  Batch 15,000  of  46,094.    Elapsed: 3:57:22.\n",
      "  current average loss = 0.013610215077749671\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:05:16.\n",
      "  current average loss = 0.013582932095198034\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:13:10.\n",
      "  current average loss = 0.013430893240547108\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:21:03.\n",
      "  current average loss = 0.01346180111215863\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:28:57.\n",
      "  current average loss = 0.013442883966473506\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:36:50.\n",
      "  current average loss = 0.013439027591758142\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:44:44.\n",
      "  current average loss = 0.013368387676179636\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:52:40.\n",
      "  current average loss = 0.013338988721878048\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:00:35.\n",
      "  current average loss = 0.013323562153237043\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:08:31.\n",
      "  current average loss = 0.01327726241065629\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:16:25.\n",
      "  current average loss = 0.013341214138165514\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:24:20.\n",
      "  current average loss = 0.013303278936605083\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:32:13.\n",
      "  current average loss = 0.013255809261650028\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:40:06.\n",
      "  current average loss = 0.013254186168057057\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:48:00.\n",
      "  current average loss = 0.013283827330845724\n",
      "  Batch 22,500  of  46,094.    Elapsed: 5:55:53.\n",
      "  current average loss = 0.013299618006340906\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:03:47.\n",
      "  current average loss = 0.013264021632280042\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:11:40.\n",
      "  current average loss = 0.013279596511478729\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:19:34.\n",
      "  current average loss = 0.013271346627721678\n",
      "  Batch 24,500  of  46,094.    Elapsed: 6:27:29.\n",
      "  current average loss = 0.013215838272555184\n",
      "  Batch 25,000  of  46,094.    Elapsed: 6:35:22.\n",
      "  current average loss = 0.013203545254494275\n",
      "  Batch 25,500  of  46,094.    Elapsed: 6:43:15.\n",
      "  current average loss = 0.0131881667440495\n",
      "  Batch 26,000  of  46,094.    Elapsed: 6:51:09.\n",
      "  current average loss = 0.013182211604233404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 26,500  of  46,094.    Elapsed: 6:59:02.\n",
      "  current average loss = 0.013208998992952197\n",
      "  Batch 27,000  of  46,094.    Elapsed: 7:06:59.\n",
      "  current average loss = 0.013184609280249553\n",
      "  Batch 27,500  of  46,094.    Elapsed: 7:14:54.\n",
      "  current average loss = 0.013158093444159188\n",
      "  Batch 28,000  of  46,094.    Elapsed: 7:22:48.\n",
      "  current average loss = 0.01311977599105288\n",
      "  Batch 28,500  of  46,094.    Elapsed: 7:30:43.\n",
      "  current average loss = 0.013063515865803422\n",
      "  Batch 29,000  of  46,094.    Elapsed: 7:38:37.\n",
      "  current average loss = 0.013053384128782234\n",
      "  Batch 29,500  of  46,094.    Elapsed: 7:46:33.\n",
      "  current average loss = 0.0130552005806354\n",
      "  Batch 30,000  of  46,094.    Elapsed: 7:54:29.\n",
      "  current average loss = 0.013050165781059574\n",
      "  Batch 30,500  of  46,094.    Elapsed: 8:02:25.\n",
      "  current average loss = 0.013045083078542642\n",
      "  Batch 31,000  of  46,094.    Elapsed: 8:10:19.\n",
      "  current average loss = 0.013075703313337374\n",
      "  Batch 31,500  of  46,094.    Elapsed: 8:18:12.\n",
      "  current average loss = 0.013044664380360498\n",
      "  Batch 32,000  of  46,094.    Elapsed: 8:26:06.\n",
      "  current average loss = 0.013075852446986943\n",
      "  Batch 32,500  of  46,094.    Elapsed: 8:33:59.\n",
      "  current average loss = 0.013053520395719394\n",
      "  Batch 33,000  of  46,094.    Elapsed: 8:41:53.\n",
      "  current average loss = 0.01301260504484388\n",
      "  Batch 33,500  of  46,094.    Elapsed: 8:49:49.\n",
      "  current average loss = 0.013017462493835627\n",
      "  Batch 34,000  of  46,094.    Elapsed: 8:57:45.\n",
      "  current average loss = 0.01301610401162979\n",
      "  Batch 34,500  of  46,094.    Elapsed: 9:05:42.\n",
      "  current average loss = 0.012982185846108026\n",
      "  Batch 35,000  of  46,094.    Elapsed: 9:13:36.\n",
      "  current average loss = 0.012970358096573207\n",
      "  Batch 35,500  of  46,094.    Elapsed: 9:21:31.\n",
      "  current average loss = 0.013013294715148884\n",
      "  Batch 36,000  of  46,094.    Elapsed: 9:29:27.\n",
      "  current average loss = 0.01298686924906292\n",
      "  Batch 36,500  of  46,094.    Elapsed: 9:37:22.\n",
      "  current average loss = 0.012997718568197678\n",
      "  Batch 37,000  of  46,094.    Elapsed: 9:45:17.\n",
      "  current average loss = 0.013002381369296361\n",
      "  Batch 37,500  of  46,094.    Elapsed: 9:53:10.\n",
      "  current average loss = 0.013000330746010562\n",
      "  Batch 38,000  of  46,094.    Elapsed: 10:01:08.\n",
      "  current average loss = 0.012962056729508132\n",
      "  Batch 38,500  of  46,094.    Elapsed: 10:09:09.\n",
      "  current average loss = 0.01296092308475487\n",
      "  Batch 39,000  of  46,094.    Elapsed: 10:17:08.\n",
      "  current average loss = 0.012948968689380402\n",
      "  Batch 39,500  of  46,094.    Elapsed: 10:25:04.\n",
      "  current average loss = 0.012941261065262517\n",
      "  Batch 40,000  of  46,094.    Elapsed: 10:32:59.\n",
      "  current average loss = 0.01296929617079379\n",
      "  Batch 40,500  of  46,094.    Elapsed: 10:40:56.\n",
      "  current average loss = 0.012936360238880455\n",
      "  Batch 41,000  of  46,094.    Elapsed: 10:48:51.\n",
      "  current average loss = 0.012963047203738491\n",
      "  Batch 41,500  of  46,094.    Elapsed: 10:56:46.\n",
      "  current average loss = 0.012899351246266753\n",
      "  Batch 42,000  of  46,094.    Elapsed: 11:04:43.\n",
      "  current average loss = 0.012904830054113626\n",
      "  Batch 42,500  of  46,094.    Elapsed: 11:12:39.\n",
      "  current average loss = 0.012891327852469305\n",
      "  Batch 43,000  of  46,094.    Elapsed: 11:20:35.\n",
      "  current average loss = 0.012900456315846747\n",
      "  Batch 43,500  of  46,094.    Elapsed: 11:28:31.\n",
      "  current average loss = 0.012892395700085416\n",
      "  Batch 44,000  of  46,094.    Elapsed: 11:36:26.\n",
      "  current average loss = 0.012874110714385035\n",
      "  Batch 44,500  of  46,094.    Elapsed: 11:44:22.\n",
      "  current average loss = 0.012837083944593737\n",
      "  Batch 45,000  of  46,094.    Elapsed: 11:52:18.\n",
      "  current average loss = 0.012853971383811525\n",
      "  Batch 45,500  of  46,094.    Elapsed: 12:00:14.\n",
      "  current average loss = 0.012873294235841968\n",
      "  Batch 46,000  of  46,094.    Elapsed: 12:08:11.\n",
      "  current average loss = 0.012847666123259447\n",
      "\n",
      "  Average training loss: 0.01285388228781846\n",
      "  Training epoch took: 12:09:40\n",
      "\n",
      "Running Validation...\n",
      "  Average validation loss: 0.012452807851597998\n",
      "  Accuracy: 0.9958329265911753\n",
      "  Validation took: 0:26:57\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n",
      "  Batch   500  of  46,094.    Elapsed: 0:07:55.\n",
      "  current average loss = 0.012171473990973026\n",
      "  Batch 1,000  of  46,094.    Elapsed: 0:15:51.\n",
      "  current average loss = 0.010355457838535585\n",
      "  Batch 1,500  of  46,094.    Elapsed: 0:23:47.\n",
      "  current average loss = 0.010092166369227926\n",
      "  Batch 2,000  of  46,094.    Elapsed: 0:31:43.\n",
      "  current average loss = 0.010334635452370095\n",
      "  Batch 2,500  of  46,094.    Elapsed: 0:39:39.\n",
      "  current average loss = 0.01036931925996687\n",
      "  Batch 3,000  of  46,094.    Elapsed: 0:47:34.\n",
      "  current average loss = 0.010400575166021555\n",
      "  Batch 3,500  of  46,094.    Elapsed: 0:55:31.\n",
      "  current average loss = 0.010700310347554738\n",
      "  Batch 4,000  of  46,094.    Elapsed: 1:03:28.\n",
      "  current average loss = 0.010716106151721532\n",
      "  Batch 4,500  of  46,094.    Elapsed: 1:11:24.\n",
      "  current average loss = 0.010718532492970351\n",
      "  Batch 5,000  of  46,094.    Elapsed: 1:19:20.\n",
      "  current average loss = 0.010527491337547507\n",
      "  Batch 5,500  of  46,094.    Elapsed: 1:27:15.\n",
      "  current average loss = 0.01057871487481564\n",
      "  Batch 6,000  of  46,094.    Elapsed: 1:35:10.\n",
      "  current average loss = 0.010476227218252159\n",
      "  Batch 6,500  of  46,094.    Elapsed: 1:43:06.\n",
      "  current average loss = 0.010591289899533876\n",
      "  Batch 7,000  of  46,094.    Elapsed: 1:51:01.\n",
      "  current average loss = 0.01063825955125971\n",
      "  Batch 7,500  of  46,094.    Elapsed: 1:58:57.\n",
      "  current average loss = 0.010574370653707107\n",
      "  Batch 8,000  of  46,094.    Elapsed: 2:06:53.\n",
      "  current average loss = 0.010594981169529205\n",
      "  Batch 8,500  of  46,094.    Elapsed: 2:14:49.\n",
      "  current average loss = 0.010484091428202351\n",
      "  Batch 9,000  of  46,094.    Elapsed: 2:22:44.\n",
      "  current average loss = 0.010395284128687106\n",
      "  Batch 9,500  of  46,094.    Elapsed: 2:30:39.\n",
      "  current average loss = 0.01030522685763696\n",
      "  Batch 10,000  of  46,094.    Elapsed: 2:38:35.\n",
      "  current average loss = 0.010227915083882454\n",
      "  Batch 10,500  of  46,094.    Elapsed: 2:46:31.\n",
      "  current average loss = 0.010259339844639682\n",
      "  Batch 11,000  of  46,094.    Elapsed: 2:54:26.\n",
      "  current average loss = 0.010395414867574056\n",
      "  Batch 11,500  of  46,094.    Elapsed: 3:02:22.\n",
      "  current average loss = 0.010438646461984046\n",
      "  Batch 12,000  of  46,094.    Elapsed: 3:10:17.\n",
      "  current average loss = 0.01049018678422893\n",
      "  Batch 12,500  of  46,094.    Elapsed: 3:18:14.\n",
      "  current average loss = 0.010514025122081876\n",
      "  Batch 13,000  of  46,094.    Elapsed: 3:26:09.\n",
      "  current average loss = 0.01047803199707708\n",
      "  Batch 13,500  of  46,094.    Elapsed: 3:34:06.\n",
      "  current average loss = 0.01041611259159914\n",
      "  Batch 14,000  of  46,094.    Elapsed: 3:42:01.\n",
      "  current average loss = 0.010454101297793126\n",
      "  Batch 14,500  of  46,094.    Elapsed: 3:49:57.\n",
      "  current average loss = 0.010401994053690251\n",
      "  Batch 15,000  of  46,094.    Elapsed: 3:57:53.\n",
      "  current average loss = 0.010458861603817907\n",
      "  Batch 15,500  of  46,094.    Elapsed: 4:05:49.\n",
      "  current average loss = 0.010507247674037988\n",
      "  Batch 16,000  of  46,094.    Elapsed: 4:13:44.\n",
      "  current average loss = 0.01052750182846205\n",
      "  Batch 16,500  of  46,094.    Elapsed: 4:21:40.\n",
      "  current average loss = 0.010497123400863612\n",
      "  Batch 17,000  of  46,094.    Elapsed: 4:29:36.\n",
      "  current average loss = 0.010445762637529804\n",
      "  Batch 17,500  of  46,094.    Elapsed: 4:37:33.\n",
      "  current average loss = 0.01043892710436963\n",
      "  Batch 18,000  of  46,094.    Elapsed: 4:45:28.\n",
      "  current average loss = 0.010383192109751993\n",
      "  Batch 18,500  of  46,094.    Elapsed: 4:53:24.\n",
      "  current average loss = 0.010421778198174178\n",
      "  Batch 19,000  of  46,094.    Elapsed: 5:01:20.\n",
      "  current average loss = 0.010383961363785745\n",
      "  Batch 19,500  of  46,094.    Elapsed: 5:09:16.\n",
      "  current average loss = 0.010353882039996395\n",
      "  Batch 20,000  of  46,094.    Elapsed: 5:17:13.\n",
      "  current average loss = 0.010391163562474912\n",
      "  Batch 20,500  of  46,094.    Elapsed: 5:25:08.\n",
      "  current average loss = 0.010360160451008885\n",
      "  Batch 21,000  of  46,094.    Elapsed: 5:33:04.\n",
      "  current average loss = 0.010385496098942331\n",
      "  Batch 21,500  of  46,094.    Elapsed: 5:41:00.\n",
      "  current average loss = 0.010412344285854995\n",
      "  Batch 22,000  of  46,094.    Elapsed: 5:48:56.\n",
      "  current average loss = 0.010355790900839648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 22,500  of  46,094.    Elapsed: 5:56:53.\n",
      "  current average loss = 0.010378909692242005\n",
      "  Batch 23,000  of  46,094.    Elapsed: 6:04:48.\n",
      "  current average loss = 0.010405461286220089\n",
      "  Batch 23,500  of  46,094.    Elapsed: 6:12:44.\n",
      "  current average loss = 0.010457647192702388\n",
      "  Batch 24,000  of  46,094.    Elapsed: 6:20:40.\n",
      "  current average loss = 0.010487719408225\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0f38dd99f2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Backward 수행으로 그래디언트 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# 그래디언트 클리핑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "def flat_accuracy(preds, labels): \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "    labels_flat = labels.flatten() \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat) \n",
    "\n",
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "\n",
    "        # Forward 수행                \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=b_token_type_ids, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    #시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가모드로 변경\n",
    "    model.eval()\n",
    "\n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "        \n",
    "        # 그래디언트 계산 안함\n",
    "        with torch.no_grad():     \n",
    "            # Forward 수행\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=b_token_type_ids, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels = b_labels)\n",
    "    \n",
    "        \n",
    "        loss = outputs[0] \n",
    "        logits = outputs[1] \n",
    "        \n",
    "        # 로스 구함 \n",
    "        eval_loss += loss.item() \n",
    "        \n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    avg_val_loss = eval_loss / len(validation_dataloader)            \n",
    "    print(\"  Average validation loss: {}\".format(avg_val_loss))\n",
    "    print(\"  Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    torch.save(model.state_dict(), \"KoBERT_MORE_FEATURES_\" + str(epoch_i + 1)) \n",
    "    \n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best checkpoint model\n",
    "checkpoint = torch.load('') \n",
    "test_model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=46)\n",
    "test_model.load_state_dict(checkpoint) \n",
    "test_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as nnf\n",
    "\n",
    "test['사업명'].fillna('NAN',inplace=True) \n",
    "test['사업_부처명'].fillna('NAN',inplace=True) \n",
    "test['내역사업명'].fillna('NAN',inplace=True) \n",
    "test['과제명'].fillna('NAN',inplace=True) \n",
    "test['요약문_한글키워드'].fillna('NAN',inplace=True)\n",
    "\n",
    "\n",
    "test_contents = test['요약문_내용'].values \n",
    "test_feature1 = test['사업명'].values \n",
    "test_feature2 = test['사업_부처명'].values \n",
    "test_feature3 = test['내역사업명'].values \n",
    "test_feature4 = test['과제명'].values \n",
    "test_feature5 = test['요약문_한글키워드'].values \n",
    "\n",
    "predicted_classes = []\n",
    "\n",
    "# change to eval mode \n",
    "test_model.eval() \n",
    "\n",
    "for i in tqdm(range(test_contents.shape[0]), position=0, leave=True):\n",
    "    f1 = clean_text(str(test_feature1[i])) \n",
    "    f2 = clean_text(str(test_feature2[i])) \n",
    "    f3 = clean_text(str(test_feature3[i])) \n",
    "    f4 = clean_text(str(test_feature4[i])) \n",
    "    f5 = str(test_feature5[i]) \n",
    "    splitted = split_text(str(test_contents[i])) \n",
    "    # make predictions for each splitted text \n",
    "    probabilities = [] \n",
    "    for text in splitted: \n",
    "        test_text = f1 + \" \" + f2 + \" \" + f3 + \" \" + f4 + \" \" + f5 + \" \" + text \n",
    "        # tokenize test text \n",
    "        input_id, attention_mask, token_type_id = electra_tokenizer(test_text, MAX_LEN=MAX_LEN) \n",
    "        input_id = torch.tensor(input_id)\n",
    "        attention_mask = torch.tensor(attention_mask) \n",
    "        token_type_id = torch.tensor(token_type_id) \n",
    "        # reshape into (batch, MAX_LEN)\n",
    "        input_id = torch.reshape(input_id, (-1,MAX_LEN)) \n",
    "        attention_mask = torch.reshape(attention_mask, (-1,MAX_LEN)) \n",
    "        token_type_id = torch.reshape(token_type_id, (-1,MAX_LEN)) \n",
    "        # move tensor to cuda \n",
    "        input_id = input_id.to(device) \n",
    "        attention_mask = attention_mask.to(device) \n",
    "        token_type_id = token_type_id.to(device) \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = test_model(input_id, \n",
    "                            token_type_ids=token_type_id, \n",
    "                            attention_mask=attention_mask) \n",
    "        \n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # obtain softmax probabilities \n",
    "        prob = nnf.softmax(logits, dim=1).flatten()\n",
    "        probabilities.append(prob)\n",
    "    \n",
    "    # soft voting \n",
    "    prob_sum = np.zeros(46) \n",
    "    for i in range(len(probabilities)): \n",
    "        for j in range(46): \n",
    "            prob_sum[j] += probabilities[i][j] \n",
    "            \n",
    "    \n",
    "    prob_sum /= len(probabilities)  \n",
    "    \n",
    "    predicted_class = np.argmax(prob_sum)\n",
    "    \n",
    "    predicted_classes.append(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1] = predicted_classes \n",
    "\n",
    "\n",
    "submission.to_csv(\"KOBERT_voting.csv\",index=False)\n",
    "\n",
    "\n",
    "submission\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
